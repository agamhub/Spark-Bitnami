{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+----------+----------------------------------------------------------------------------------------------------------------------------------------------+-----+-------------------+----+\n",
      "|    |   BatchName   |  DqcId   |                                                                   Scripts                                                                    | Run |     RefTable      | SP |\n",
      "+----+---------------+----------+----------------------------------------------------------------------------------------------------------------------------------------------+-----+-------------------+----+\n",
      "| 0  | BATCH_ACT_VAL | DQ000003 |                                                      SELECT COUNT(1) CNT FROM PEOPLEPLF                                                      |  1  |        nan        | 0  |\n",
      "| 1  | BATCH_ACT_VAL | DQ000004 |                                                      SELECT COUNT(1) CNT FROM PEOPLEPF                                                       |  1  | [PEOPLEPF,AGNTPF] | 0  |\n",
      "| 2  | BATCH_ACT_VAL | DQ000005 |                        SELECT COUNT(1) CNT FROM PEOPLEPF WHERE YEAR(`Date of birth`) IN ('1992','1993','1995','2021')                        |  1  |        nan        | 0  |\n",
      "| 3  | BATCH_ACT_VAL | DQ000006 | SELECT COUNT(1) CNT FROM ACMVPF A LEFT JOIN GENLPF B ON A.Index = B.Index WHERE YEAR(A.`Subscription Date`) IN ('1992','1993','1995','2021') |  1  |        nan        | 0  |\n",
      "| 4  | BATCH_ACT_VAL | DQ000007 |                                                        /mnt/apps/Files/SP/USP_FCT.sql                                                        |  1  |        nan        | 1  |\n",
      "| 5  | BATCH_ACT_VAL | DQ000008 |                           SELECT CASE WHEN CNT > 0 THEN 0 ELSE 1 END CNT FROM (SELECT COUNT(1) CNT FROM ACMVPF) A                            |  1  |        nan        | 0  |\n",
      "| 9  | BATCH_ACT_VAL | DQ000001 |                                                       SELECT COUNT(1) CNT FROM GENLPF                                                        |  1  |        nan        | 0  |\n",
      "| 10 | BATCH_ACT_VAL | DQ000002 |                                                       SELECT COUNT(1) CNT FROM ACMVPF                                                        |  1  |        nan        | 0  |\n",
      "+----+---------------+----------+----------------------------------------------------------------------------------------------------------------------------------------------+-----+-------------------+----+\n",
      "       BatchName     DqcId                            Scripts  Run  RefTable  \\\n",
      "1  BATCH_ACT_VAL  DQ000004  SELECT COUNT(1) CNT FROM PEOPLEPF    1  PEOPLEPF   \n",
      "1  BATCH_ACT_VAL  DQ000004  SELECT COUNT(1) CNT FROM PEOPLEPF    1    AGNTPF   \n",
      "\n",
      "   SP  \n",
      "1   0  \n",
      "1   0  \n",
      "+----+---------------+---------------------------------+\n",
      "|    |   BatchName   |             JobName             |\n",
      "+----+---------------+---------------------------------+\n",
      "| 0  | BATCH_ACT_VAL |             ACMVPF              |\n",
      "| 1  | BATCH_ACT_VAL |             GENLPF              |\n",
      "| 2  | BATCH_ACT_VAL |             HPADPF              |\n",
      "| 3  | BATCH_ACT_VAL |             DESCPF              |\n",
      "| 4  | BATCH_ACT_VAL |             ITEMPF              |\n",
      "| 5  | BATCH_ACT_VAL |           SUNGL_IFRS4           |\n",
      "| 6  | BATCH_ACT_VAL |           COA_MAPPING           |\n",
      "| 7  | BATCH_ACT_VAL |          ETL4_PREMDUE           |\n",
      "| 8  | BATCH_ACT_VAL |            FCORE_FCT            |\n",
      "| 9  | BATCH_ACT_VAL |         FINANCE_CONFIG          |\n",
      "| 10 | BATCH_ACT_VAL |        ETL5_COA_INSCOPE         |\n",
      "| 11 | BATCH_ACT_VAL |           ETL5_DRIVER           |\n",
      "| 12 | BATCH_ACT_VAL | RENOVA_PREMIUM_TMP_BORDEUX_ETL5 |\n",
      "| 13 | BATCH_ACT_VAL |            PEOPLEPF             |\n",
      "| 14 | BATCH_ACT_VAL |             AGNTPF              |\n",
      "+----+---------------+---------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from random import random\n",
    "from operator import add\n",
    "from tabulate import tabulate\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import threading\n",
    "import csv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, length, regexp_replace, lit\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def pandas_read_csv(file_path,**options):\n",
    "    \"\"\"\n",
    "        Read small volume of data only using read.csv\n",
    "        Args:\n",
    "            **Options ----> Any\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path,**options)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:  # Catch other potential exceptions (e.g., parsing errors)\n",
    "        print(f\"An error occurred while reading the CSV: {e}\")\n",
    "        return None\n",
    "\n",
    "def loadTable(**kwargs):\n",
    "    pathCheck = kwargs[\"path\"].replace(\"/part*\",\"\")\n",
    "    if not os.path.exists(pathCheck):\n",
    "        return None\n",
    "    try:\n",
    "        sparkDqc.sql(f\"\"\"\n",
    "        CREATE EXTERNAL TABLE IF NOT EXISTS {kwargs[\"tableName\"]}\n",
    "        USING PARQUET LOCATION '{kwargs[\"path\"]}'\n",
    "        \"\"\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "def ListTable():\n",
    "    listCreatedTable = []\n",
    "    tables = sparkDqc.sql(\"SHOW TABLES\").collect()\n",
    "    for table in tables:\n",
    "       listCreatedTable.append(table.tableName)\n",
    "    return listCreatedTable\n",
    "\n",
    "def string_to_list(ref_table_str):\n",
    "    \"\"\"Safely converts a string representation of a list to a Python list.\"\"\"\n",
    "    if isinstance(ref_table_str, str):\n",
    "        # Remove the surrounding square brackets and split by comma\n",
    "        cleaned_str = ref_table_str.strip(\"[]\")\n",
    "        values = [val.strip() for val in cleaned_str.split(',')]\n",
    "        return values\n",
    "    elif isinstance(ref_table_str, list):\n",
    "        return ref_table_str\n",
    "    return []\n",
    "\n",
    "def getTables(**kwargs):\n",
    "    df = pandas_read_csv(kwargs[\"path\"], sep=\"|\")\n",
    "    df = df.query(f\"BatchName == '{batchname}'\")\n",
    "    print(tabulate(df.head(10), headers='keys', tablefmt='pretty'))\n",
    "    df_refTable = df.query(\"RefTable.notnull()\")\n",
    "    df_refTable.loc[:, 'RefTable'] = df_refTable['RefTable'].apply(string_to_list)\n",
    "    df_refTable_exploded = df_refTable.explode(\"RefTable\")\n",
    "    print(df_refTable_exploded)\n",
    "    distinct_reftable = df_refTable_exploded[[\"BatchName\",\"RefTable\"]].drop_duplicates().rename(columns={\"RefTable\": \"JobName\"})\n",
    "    df_job = pandas_read_csv(kwargs[\"LoadPath\"], sep=\"|\")\n",
    "    df_job = df_job[[\"BatchName\",\"JobName\"]].query(f\"BatchName == '{batchname}'\")\n",
    "    joined_df = pd.concat([df_job, distinct_reftable], ignore_index=True).drop_duplicates()\n",
    "    print(tabulate(joined_df.head(15), headers='keys', tablefmt='pretty'))\n",
    "    return joined_df, df\n",
    "\n",
    "def executeScripts(sparkDqc, **kwargs):\n",
    "    start_time = datetime.now()\n",
    "    try:\n",
    "        if kwargs[\"SP\"] == 1:\n",
    "            with open(kwargs[\"Scripts\"], 'r') as file:\n",
    "                sql_content = file.read()\n",
    "            sql_statements = sql_content.split(\";\")\n",
    "            sql_statements = [s.strip() for s in sql_statements if s.strip()]\n",
    "\n",
    "            for sql in sql_statements:\n",
    "                sql_upper = sql.upper()\n",
    "                if sql_upper.startswith(\"SELECT\"): #Check if the string STARTS with select.\n",
    "                    result_df = sparkDqc.sql(sql)\n",
    "                else:\n",
    "                    sparkDqc.sql(sql)\n",
    "        else:\n",
    "            result_df = sparkDqc.sql(kwargs[\"Scripts\"])\n",
    "\n",
    "        end_time = datetime.now()\n",
    "        duration_seconds = (end_time - start_time).total_seconds()\n",
    "        if duration_seconds < 60:\n",
    "            duration = f\"{duration_seconds:.2f} seconds\"\n",
    "        elif duration_seconds < 3600:\n",
    "            duration = f\"{duration_seconds / 60:.2f} minutes\"\n",
    "        else:\n",
    "            duration = f\"{duration_seconds / 3600:.2f} hours\"\n",
    "\n",
    "        count = result_df.collect()[0]\n",
    "        status = \"Failed\" if count[\"CNT\"] != 0 else \"Successful\"\n",
    "        kwargs[\"results\"].append((kwargs[\"BatchName\"], kwargs[\"DqcId\"], start_time, end_time, duration, kwargs[\"Scripts\"], kwargs[\"Run\"], str(count[\"CNT\"]), status))\n",
    "    except Exception as e:\n",
    "        end_time = datetime.now()\n",
    "        kwargs[\"results\"].append((kwargs[\"BatchName\"], kwargs[\"DqcId\"], start_time, end_time, 0, kwargs[\"Scripts\"], kwargs[\"Run\"], f\"Error : {str(e)}\", \"Failed\"))\n",
    "\n",
    "def executeScriptsParallel(sparkDqc, df):\n",
    "    results = []\n",
    "    threads = []\n",
    "\n",
    "    for row in df.itertuples(index=False):\n",
    "        kwargs = {\n",
    "            \"BatchName\": row.BatchName,\n",
    "            \"DqcId\": row.DqcId,\n",
    "            \"Scripts\": row.Scripts,\n",
    "            \"Run\": row.Run,\n",
    "            \"SP\": row.SP,\n",
    "            \"results\": results\n",
    "        }\n",
    "         \n",
    "        thread = threading.Thread(target=executeScripts, args=(sparkDqc,), kwargs=kwargs)\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    result_df = sparkDqc.createDataFrame(results, [\"BatchName\", \"DqcId\", \"StartTime\", \"EndTime\", \"Duration\", \"Scripts\", \"Run\", \"Result\", \"Status\"])\n",
    "    return result_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"/mnt/apps/Files/Config/DataQuality_Config.csv\"\n",
    "    LoadPath = \"/mnt/apps/Files/Config/master_job.csv\"\n",
    "    parquetOutput = \"/mnt/apps/Files/data-movement/Parquet/\"\n",
    "    batchname = \"BATCH_ACT_VAL\"\n",
    "    testWrite = \"/mnt/apps/Files/Test\"\n",
    "\n",
    "    getDf, df = getTables(path=path, LoadPath=LoadPath)\n",
    "\n",
    "    sparkDqc = (\n",
    "        SparkSession\n",
    "            .builder\n",
    "            .appName(f\"{batchname}_DQC\")\n",
    "            .master(\"local[*]\")\n",
    "            .config(\"spark.ui.port\", \"4222\")\n",
    "            .getOrCreate()\n",
    "    )\n",
    "\n",
    "    listJob = []\n",
    "    for row in getDf.itertuples():\n",
    "        listJob.append(row.JobName.lower())\n",
    "        loadTable(path=parquetOutput + '/' + row.JobName + '/part*', tableName=row.JobName)\n",
    "        \n",
    "    #print(ListTable())\n",
    "    #print(listJob)\n",
    "    \n",
    "    # if all(item in ListTable() for item in listJob):\n",
    "    #     result_df = executeScriptsParallel(sparkDqc, df)\n",
    "    #     print(result_df.show(truncate=False))\n",
    "    # else:\n",
    "    #     sys.exit(1)\n",
    "\n",
    "    sparkDqc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       BatchName     DqcId                            Scripts  Run  RefTable  \\\n",
      "0  BATCH_ACT_VAL  DQ000004  SELECT COUNT(1) CNT FROM PEOPLEPF    1  PEOPLEPF   \n",
      "0  BATCH_ACT_VAL  DQ000004  SELECT COUNT(1) CNT FROM PEOPLEPF    1    AGNTPF   \n",
      "\n",
      "   SP  \n",
      "0   0  \n",
      "0   0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# Your CSV data as a string\n",
    "csv_data = \"\"\"BatchName|DqcId|Scripts|Run|RefTable|SP\n",
    "BATCH_ACT_VAL|DQ000004|SELECT COUNT(1) CNT FROM PEOPLEPF|1|[PEOPLEPF,AGNTPF]|0\"\"\"\n",
    "\n",
    "# Read the CSV data from the string\n",
    "df = pd.read_csv(io.StringIO(csv_data), sep='|')\n",
    "\n",
    "def string_to_list(ref_table_str):\n",
    "    \"\"\"Safely converts a string representation of a list to a Python list.\"\"\"\n",
    "    if isinstance(ref_table_str, str):\n",
    "        # Remove the surrounding square brackets and split by comma\n",
    "        cleaned_str = ref_table_str.strip(\"[]\")\n",
    "        values = [val.strip() for val in cleaned_str.split(',')]\n",
    "        return values\n",
    "    elif isinstance(ref_table_str, list):\n",
    "        return ref_table_str\n",
    "    return []\n",
    "\n",
    "# Apply the custom function to the 'RefTable' column\n",
    "df['RefTable'] = df['RefTable'].apply(string_to_list)\n",
    "\n",
    "# Explode the 'RefTable' column\n",
    "df_exploded = df.explode('RefTable')\n",
    "\n",
    "print(df_exploded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sparkDqc.sql(\"\"\"\n",
    "        SELECT `Job title`, CNT, DENSE_RANK() OVER(ORDER BY CNT DESC) RANKED FROM temp_jobs_3\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4baa344918d4:4222\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>BATCH_ACT_VAL_DQC</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8cbc4af4d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkDqc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"Locally\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.ui.port\", \"4222\")\n",
    "        .config(\"spark.executor.instances\", \"2\")  # Initial number of executors (if dynamicAllocation is false)\n",
    "        .config(\"spark.executor.cores\", \"2\")  # Cores per executor\n",
    "        .config(\"spark.cores.max\", \"3\") # Maximum total cores for the application\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"true\") #Enable dynamic allocation\n",
    "        .config(\"spark.dynamicAllocation.minExecutors\", \"1\") #minimum executors\n",
    "        .config(\"spark.dynamicAllocation.maxExecutors\", \"5\") #maximum executors\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "def dict_to_row(data_dict):\n",
    "    \"\"\"Converts a dictionary to a Spark Row.\"\"\"\n",
    "    return Row(**data_dict)\n",
    "\n",
    "def create_dataframe_from_dict(spark, data_dict):\n",
    "    \"\"\"Creates a Spark DataFrame from a dictionary.\"\"\"\n",
    "    row = dict_to_row(data_dict)\n",
    "    return spark.createDataFrame([row])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName(\"DictToDataFrame\").getOrCreate()\n",
    "\n",
    "    data = {\n",
    "        'JobName': 'RTRNPF',\n",
    "        'Path': '/mnt/apps/gcs/ETL4/RTRNPF/RTRNPF*.csv',\n",
    "        'dqID': 'DQ000001',\n",
    "        'CountRecords': -1,\n",
    "        'Message': ['Invalid Index values (containing only non-numeric characters). Total count: 5', \"Invalid Subscription Date values (not in '%Y-%m-%d' format). Total count: 1\"],\n",
    "        'Status': 'Failed'\n",
    "    }\n",
    "\n",
    "    df = create_dataframe_from_dict(spark, data)\n",
    "\n",
    "    # Explode the 'Message' array into rows\n",
    "    exploded_df = df.select(\"JobName\", \"Path\", \"dqID\", \"CountRecords\", explode(\"Message\").alias(\"Message\"), \"Status\")\n",
    "\n",
    "    exploded_df.show(truncate=False)\n",
    "    exploded_df.printSchema()\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"CREATE TEMPORARY VIEW temp_jobs AS SELECT `Job Title`,COUNT(1) CNT FROM PEOPLEPF WHERE YEAR(`Date of birth`) IN ('1992','1993','1995','2021') GROUP BY `Job Title`\", ' SELECT COUNT(1) CNT FROM temp_jobs']\n",
      "CREATE TEMPORARY VIEW temp_jobs AS SELECT `Job Title`,COUNT(1) CNT FROM PEOPLEPF WHERE YEAR(`Date of birth`) IN ('1992','1993','1995','2021') GROUP BY `Job Title`\n",
      "SELECT COUNT(1) CNT FROM temp_jobs\n"
     ]
    }
   ],
   "source": [
    "test_Split = (\"CREATE TEMPORARY VIEW temp_jobs AS SELECT `Job Title`,COUNT(1) CNT FROM PEOPLEPF WHERE YEAR(`Date of birth`) IN ('1992','1993','1995','2021') GROUP BY `Job Title`; SELECT COUNT(1) CNT FROM temp_jobs\").split(\";\")\n",
    "print(test_Split)\n",
    "\n",
    "for scripts in test_Split:\n",
    "    scripts = scripts.strip()\n",
    "    print(scripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import csv\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CSVMultiLine\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame (replace with your actual data source)\n",
    "data = [(\"John\", \"Engineer\", \"1992-05-15\"), (\"Jane\", \"Manager\", \"1993-10-20\"), (\"Mike\", \"Engineer\", \"1995-03-01\"), (\"Sarah\", \"Analyst\", \"2021-12-05\")]\n",
    "columns = [\"Name\", \"Job Title\", \"Date of birth\"]\n",
    "people_df = spark.createDataFrame(data, columns)\n",
    "people_df.createOrReplaceTempView(\"PEOPLEPF\")\n",
    "\n",
    "def execute_sql_from_csv(csv_file_path):\n",
    "    with open(csv_file_path, 'r') as file:\n",
    "        sql_content = file.read()\n",
    "    sql_statements = sql_content.split(\";\")\n",
    "    sql_statements = [s.strip() for s in sql_statements if s.strip()]\n",
    "    for sql in sql_statements:\n",
    "        sql_upper = sql.upper()\n",
    "        if sql_upper.startswith(\"SELECT\"):\n",
    "            result_df = spark.sql(sql)\n",
    "            result_df.show()\n",
    "        else:\n",
    "            spark.sql(sql)\n",
    "\n",
    "csv_file_path = '/mnt/apps/Files/SP/USP_FCT.sql'\n",
    "\n",
    "execute_sql_from_csv(csv_file_path=csv_file_path)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
