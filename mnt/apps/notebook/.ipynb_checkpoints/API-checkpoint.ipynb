{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+---------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+-----------------+--------------------+--------------------+\n",
      "|Index|    Customer Id|First Name|Last Name|             Company|            City|             Country|             Phone 1|             Phone 2|               Email|Subscription Date|             Website|       error_details|\n",
      "+-----+---------------+----------+---------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+-----------------+--------------------+--------------------+\n",
      "| NULL|4962fdbE6Bfee6D|       Pam|   Sparks|        Patel-Deleon|      Blakemouth|British Indian Oc...|    267-243-9490x035|    480-078-0535x889|nicolas00@faulkne...|       2020-11-29| https://nelson.com/|DDL Error please ...|\n",
      "|    2|9b12Ae76fdBc9bE|      Gina|    Rocha|Acosta, Paul and ...|East Lynnchester|          Costa Rica|        027.142.0940|+1-752-593-4777x0...|  yfarley@morgan.com|             NULL|https://pineda-ro...|DDL Error please ...|\n",
      "| NULL|Fc2c8D2BE1AEfDb|  Kristina|  Andrade|            Mann Ltd|    Port Taraton|    Pitcairn Islands| (640)067-7023x66846|001-367-405-8096x592|ivillarreal@fowle...|       2020-09-11|  https://foley.com/|DDL Error please ...|\n",
      "| NULL|9468BBc926AaAB3|       Zoe|   Hansen|          Tanner PLC|    Kimberlyfort|               Benin|   638-798-9796x0247|  (265)475-2386x9812|duransheena@hughe...|       2021-10-09|https://franco-ga...|DDL Error please ...|\n",
      "| NULL|A1505BF376CC5Ed|     Aimee|   Brooks|          Walker Ltd|    Mitchellview|            Malaysia|  112.920.9961x77753|  471.896.6847x82788|chambersdanielle@...|       2022-03-28|   http://solis.org/|DDL Error please ...|\n",
      "| NULL|a24eB840950dac7| Mackenzie|  Leonard|          Abbott Inc|       Bauerfort|             Ukraine|+1-010-716-9313x7...|        315.423.2995|bwheeler@hickman-...|       2022-05-02|http://www.pachec...|DDL Error please ...|\n",
      "+-----+---------------+----------+---------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+-----------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+---------+-----------------------+----------------+---------------------------------------------------+---------------------+---------------------+--------------------------------+-----------------+----------------------------+------------------------------------+\n",
      "|Index|Customer Id    |First Name|Last Name|Company                |City            |Country                                            |Phone 1              |Phone 2              |Email                           |Subscription Date|Website                     |error_details                       |\n",
      "+-----+---------------+----------+---------+-----------------------+----------------+---------------------------------------------------+---------------------+---------------------+--------------------------------+-----------------+----------------------------+------------------------------------+\n",
      "|NULL |4962fdbE6Bfee6D|Pam       |Sparks   |Patel-Deleon           |Blakemouth      |British Indian Ocean Territory (Chagos Archipelago)|267-243-9490x035     |480-078-0535x889     |nicolas00@faulkner-kramer.com   |2020-11-29       |https://nelson.com/         |DDL Error please check data contents|\n",
      "|2    |9b12Ae76fdBc9bE|Gina      |Rocha    |Acosta, Paul and Barber|East Lynnchester|Costa Rica                                         |027.142.0940         |+1-752-593-4777x07171|yfarley@morgan.com              |NULL             |https://pineda-rogers.biz/  |DDL Error please check data contents|\n",
      "|NULL |Fc2c8D2BE1AEfDb|Kristina  |Andrade  |Mann Ltd               |Port Taraton    |Pitcairn Islands                                   |(640)067-7023x66846  |001-367-405-8096x592 |ivillarreal@fowler.biz          |2020-09-11       |https://foley.com/          |DDL Error please check data contents|\n",
      "|NULL |9468BBc926AaAB3|Zoe       |Hansen   |Tanner PLC             |Kimberlyfort    |Benin                                              |638-798-9796x0247    |(265)475-2386x9812   |duransheena@hughes.com          |2021-10-09       |https://franco-galloway.com/|DDL Error please check data contents|\n",
      "|NULL |A1505BF376CC5Ed|Aimee     |Brooks   |Walker Ltd             |Mitchellview    |Malaysia                                           |112.920.9961x77753   |471.896.6847x82788   |chambersdanielle@good-cannon.com|2022-03-28       |http://solis.org/           |DDL Error please check data contents|\n",
      "|NULL |a24eB840950dac7|Mackenzie |Leonard  |Abbott Inc             |Bauerfort       |Ukraine                                            |+1-010-716-9313x74577|315.423.2995         |bwheeler@hickman-acevedo.com    |2022-05-02       |http://www.pacheco.net/     |DDL Error please check data contents|\n",
      "+-----+---------------+----------+---------+-----------------------+----------------+---------------------------------------------------+---------------------+---------------------+--------------------------------+-----------------+----------------------------+------------------------------------+\n",
      "\n",
      "+------------------------------------+-----+\n",
      "|error_details                       |total|\n",
      "+------------------------------------+-----+\n",
      "|DDL Error please check data contents|6    |\n",
      "+------------------------------------+-----+\n",
      "\n",
      "[{'JobName': 'RTRNPF', 'Path': '/mnt/apps/gcs/ETL4/RTRNPF', 'dqID': 'DQ000001', 'CountRecords': 6, 'Message': 'DDL Error please check data contents', 'Status': 'Failed'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/01 09:48:01 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/spark-a54f9980-7041-4384-92c9-2845a307b762/pyspark-fd0ef3d4-5c74-43dc-a464-829976b693ea. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/spark-a54f9980-7041-4384-92c9-2845a307b762/pyspark-fd0ef3d4-5c74-43dc-a464-829976b693ea\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, length, regexp_replace, lit, udf, count, trim\n",
    "from datetime import datetime\n",
    "\n",
    "def spark_read_csv_from_os(spark, file_path, schema, **kwargs):\n",
    "    \"\"\"\n",
    "    Reads a CSV file from the operating system into a Spark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        spark: The SparkSession object.\n",
    "        file_path: The path to the CSV file.  Can be a local path or a path\n",
    "                   that your Spark environment can access (e.g., if you're\n",
    "                   using a distributed file system like HDFS).\n",
    "        header (bool, optional): Whether the CSV file has a header row. Defaults to True.\n",
    "        inferSchema (bool, optional): Whether to infer the schema from the data. Defaults to True.\n",
    "        **options: Additional options to pass to the Spark CSV reader.  See\n",
    "                   the Spark documentation for available options like `delimiter`,\n",
    "                   `quote`, `escape`, etc.\n",
    "\n",
    "    Returns:\n",
    "        A Spark DataFrame representing the CSV data, or None if there's an error.\n",
    "\n",
    "    Raises:\n",
    "       FileNotFoundError: If the file path doesn't exist.\n",
    "    \"\"\"\n",
    "    base_options = {\n",
    "        \"inferSchema\": \"False\",\n",
    "        \"header\": \"True\",\n",
    "        \"quote\": '\"',\n",
    "        \"columnNameOfCorruptRecord\": \"rejected_records\",\n",
    "        \"mode\": \"PERMISSIVE\"\n",
    "    }\n",
    "    base_options.update(kwargs)\n",
    "\n",
    "    try:\n",
    "        schema = StructType(schema.fields + [StructField(\"rejected_records\", StringType(), True)])\n",
    "        df = spark.read.options(**base_options).schema(schema).csv(file_path)\n",
    "    \n",
    "        def parse_and_identify_errors(rejected_str, delimiter=\",\"):\n",
    "            if rejected_str:\n",
    "                try:\n",
    "                    parts = rejected_str.split(delimiter)\n",
    "                    error_columns = []\n",
    "                    error_reasons = []\n",
    "\n",
    "                    for i, field in enumerate(schema.fields):\n",
    "                        try:\n",
    "                            if field.dataType == IntegerType():\n",
    "                                int(parts[i])\n",
    "                            elif field.dataType == DateType():\n",
    "                                datetime.strptime(parts[i], \"%Y-%m-%d\").date() #change date format\n",
    "                            # Add more data type checks as needed\n",
    "                        except (ValueError, IndexError):\n",
    "                            error_columns.append(field.name)\n",
    "                            error_reasons.append(f\"Invalid {field.dataType.typeName()}\")\n",
    "\n",
    "                    if error_columns:\n",
    "                        error_detail_string = \", \".join([f\"{col}: {reason}\" for col, reason in zip(error_columns, error_reasons)])\n",
    "                        #print(f\"Rejected String: {rejected_str}, Error Details: {error_detail_string}\") #debugging print\n",
    "                        return error_detail_string\n",
    "                    else:\n",
    "                        print(f\"Error parsing record: {e}, No errors found\") #debugging print\n",
    "                        return None\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing record: {e}, Rejected String: {rejected_str}\") #debugging print\n",
    "                    return f\"Error parsing record: {e}\"\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        parse_errors_udf = udf(parse_and_identify_errors, StringType())\n",
    "\n",
    "        rejected_df = df.filter(col(\"rejected_records\").isNotNull()).withColumn(\n",
    "            \"error_details\", parse_errors_udf(col(\"rejected_records\"))\n",
    "        )\n",
    "\n",
    "        df = df.drop(\"rejected_records\")\n",
    "        rejected_df = rejected_df.drop(\"rejected_records\")\n",
    "\n",
    "        return df, rejected_df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:  # Catch other potential exceptions (e.g., parsing errors)\n",
    "        print(f\"An error occurred while reading the CSV: {e}\")\n",
    "        return None\n",
    "    \n",
    "def pandas_read_csv(file_path,**options):\n",
    "    \"\"\"\n",
    "        Read small volume of data only using read.csv\n",
    "        Args:\n",
    "            **Options ----> Any\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path,**options)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:  # Catch other potential exceptions (e.g., parsing errors)\n",
    "        print(f\"An error occurred while reading the CSV: {e}\")\n",
    "        return None\n",
    "    \n",
    "def construct_sql_schema(**kwargs):\n",
    "    \"\"\"\n",
    "        Args: kwargs path and sep -->>> Any\n",
    "        this function is best practice to compute large amount of data to not reading schema metadata\n",
    "        recommendation : \n",
    "    \"\"\"\n",
    "\n",
    "    fields = []\n",
    "    type_mapping = {\n",
    "        \"varchar\": StringType(),\n",
    "        \"nvarchar\": StringType(),\n",
    "        \"int\": IntegerType(),\n",
    "        \"bigint\": LongType(),\n",
    "        \"date\": DateType(),\n",
    "        \"decimal\": DecimalType\n",
    "    }\n",
    "\n",
    "    df = pandas_read_csv(kwargs[\"path\"],sep=kwargs[\"sep\"])\n",
    "    #print(df)\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        try:\n",
    "            name, data_type_str = row.DataType.split(\"(\", 1) if \"(\" in row.DataType else (row.DataType,\"\")\n",
    "            name = name.strip()\n",
    "            data_type_str = data_type_str[:-1].strip()\n",
    "            parts = data_type_str.split(\",\")\n",
    "            name_lower = name.lower()\n",
    "\n",
    "            for keyword,spark_type in type_mapping.items():\n",
    "                if keyword in name_lower:\n",
    "                    if spark_type == DecimalType:\n",
    "                        data_type = DecimalType() if not data_type_str else DecimalType(int(parts[0]),int(parts[1]))\n",
    "                        fields.append(StructField(row.ColumnName, data_type, True))\n",
    "                    else:\n",
    "                        data_type = spark_type\n",
    "                        fields.append(StructField(row.ColumnName, data_type, True))\n",
    "                    break\n",
    "        except Exception as e:  # Catch other potential errors\n",
    "            print(f\"Error processing file in construct schema {kwargs[\"path\"]}: {e}\")\n",
    "            return None\n",
    "    return StructType(fields)\n",
    "\n",
    "def validateDecimal(**kwargs):\n",
    "    df_contents = kwargs[\"df_contents\"]\n",
    "    is_valid = False\n",
    "    errors = []\n",
    "    dqcId = \"DQ000001\"\n",
    "    for field in kwargs[\"dtypes\"]:\n",
    "        colName = field.name\n",
    "        dType = str(field.dataType)\n",
    "        if \"decimal\" in dType.lower() or \"int\" in dType.lower():\n",
    "            #print(colName)\n",
    "            df_cleaned = df_contents.withColumn(\n",
    "                f\"{colName}_cleaned\",\n",
    "                regexp_replace(col(colName), \"[^0-9.]\", \"\")\n",
    "            )\n",
    "            df_empty = df_cleaned.filter((col(f\"{colName}_cleaned\")).isNull()) # is null due to schema defined as decimal if we use inferSchema its a string\n",
    "            #print(\"test disini\")\n",
    "            #df_empty.show()\n",
    "            empty_count = df_empty.count()\n",
    "\n",
    "            if empty_count > 0:\n",
    "                is_valid = True\n",
    "                error_msg = (f\"Invalid {colName} values (containing only non-numeric characters). Total count: {empty_count}\")\n",
    "                errors.append(error_msg)\n",
    "            df_contents = df_contents.drop(f\"{colName}_cleaned\") \n",
    "            \n",
    "    if is_valid == False:        \n",
    "        error_msg = \"DDL Decimal/Int Data Type Structure Checks Passed.\"\n",
    "        errors.append(error_msg)\n",
    "    msg = \"\\n\".join(errors) if errors else \"Data Quality Checks Passed.\" # this is for breakdown into rows from array\n",
    "    return is_valid, errors, df_contents, dqcId\n",
    "\n",
    "def writeOptions(df, dataMovement, **kwargs):\n",
    "    base_options = {  # Keep this separate\n",
    "        \"header\": \"true\",\n",
    "        \"delimiter\": \"|\",\n",
    "        \"quote\": '\"'\n",
    "    }\n",
    "    base_options.update(kwargs)\n",
    "    df.coalesce(1).write.format(\"csv\").mode(\"overwrite\").options(**base_options).save(dataMovement)\n",
    "\n",
    "def writeToParquet(df, path):\n",
    "    df = df.coalesce(50)\n",
    "    df.write.parquet(path, mode=\"overwrite\", compression=\"snappy\")\n",
    "\n",
    "def loadTable(**kwargs):\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS spark_catalog.default.{kwargs['tableName']}\") \n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE EXTERNAL TABLE {kwargs[\"tableName\"]}\n",
    "    USING CSV\n",
    "    OPTIONS (\n",
    "        header 'true',  -- If your CSV has a header row\n",
    "        inferSchema 'false', -- Important: Set to false since we provide schema\n",
    "        delimiter '|' -- Specify the delimiter if it's not a comma\n",
    "    )\n",
    "        LOCATION '{kwargs[\"path\"]}'\n",
    "    \"\"\")\n",
    "    df = spark.sql(f\"SELECT * FROM {kwargs[\"tableName\"]}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"/mnt/apps/Files/Config/master_job.csv\"\n",
    "    pathSchema = \"/mnt/apps/Files/Schema/\"\n",
    "    outputFile = \"/mnt/apps/Files/data-movement/\"\n",
    "    parquetOutput = \"/mnt/apps/Files/data-movement/Parquet/\"\n",
    "    dqcOutput = []\n",
    "    dqcId = \"DQ000001\"\n",
    "\n",
    "    spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        appName(\"Testing\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.ui.port\", \"4222\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    df = pandas_read_csv(path,sep=\"|\")\n",
    "    df = df.query(f\"JobName == 'RTRNPF' | JobName == 'ACMVPF'\")\n",
    "    #print(df)\n",
    "\n",
    "    for row in df.itertuples():  # Collects all data to the driver - NOT recommended for large datasets\n",
    "        filePath = row.SourceDirectory + '/' + row.FileName + '*.' + row.FileType\n",
    "        filePath = filePath.replace(\"/gcs\", \"/Files\")\n",
    "        dataMovement = outputFile + row.JobName + '/'\n",
    "        dataMovementParquet = parquetOutput + row.JobName\n",
    "        #print(row.JobName)\n",
    "        FullPathSchema = pathSchema + row.FileName + '.' + row.FileType\n",
    "        #print(FullPathSchema)\n",
    "        #spark.stop()\n",
    "        df_dtype = construct_sql_schema(path=FullPathSchema, sep=\"|\")\n",
    "        #print(df_dtype)\n",
    "        df, rejected_df = spark_read_csv_from_os(spark, filePath, schema=df_dtype, sep=row.Delimiter)\n",
    "        rejected_df.cache()\n",
    "        rejected_df.take(1)\n",
    "        is_empty = True if rejected_df.count() > 0 else False\n",
    "        if is_empty:\n",
    "            # need to catch malforme\n",
    "            # Group by error_details \n",
    "            rejected_df.show(truncate=False)\n",
    "            rejCnt = rejected_df.groupBy(\"error_details\").agg(count(\"*\").alias(\"total\"))\n",
    "            rejCnt.show(truncate=False)\n",
    "            collected_rejCnt = rejCnt.collect()\n",
    "            if collected_rejCnt:\n",
    "                err_msg = collected_rejCnt[0]\n",
    "                dqcOutput.append({\"JobName\":row.JobName, \"Path\":row.SourceDirectory, \"dqID\":dqcId, \"CountRecords\":err_msg.total, \"Message\":err_msg.error_details, \"Status\":\"Failed\"})\n",
    "            else:\n",
    "                print(\"No rejected errors found.\")\n",
    "            rejected_df.unpersist()\n",
    "        else:\n",
    "            df_count = df.count()\n",
    "            #writeOptions(df, dataMovement)\n",
    "            dqcOutput.append({\"JobName\":row.JobName, \"Path\":row.SourceDirectory, \"dqID\":dqcId, \"CountRecords\":df_count, \"Message\":\"DDL Data Quality Check Passed !!!!\", \"Status\":\"Successful\"})\n",
    "        rejected_df.unpersist()\n",
    "    print(dqcOutput)\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "#help(spark.read.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Index: int, Customer Id: string, First Name: string, Last Name: string, Company: string, City: string, Country: string, Phone 1: string, Phone 2: string, Email: string, Subscription Date: date, Website: string, error_details: string]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rejected_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(error_details='invalid', total=6)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark1 = SparkSession.builder.appName(\"CreateDataFrameExample\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"invalid\",6)\n",
    "]\n",
    "\n",
    "schema = [\"error_details\", \"total\"]\n",
    "\n",
    "df = spark1.createDataFrame(data, schema=schema)\n",
    "\n",
    "a = df.collect()[0]\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/01 05:42:49 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------+-----+\n",
      "|error_details                                          |count|\n",
      "+-------------------------------------------------------+-----+\n",
      "|Index: Invalid integer, Subscription Date: Invalid date|6    |\n",
      "+-------------------------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"GroupByErrorDetails\").getOrCreate()\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"Index\", StringType(), True),\n",
    "    StructField(\"Customer Id\", StringType(), True),\n",
    "    StructField(\"First Name\", StringType(), True),\n",
    "    StructField(\"Last Name\", StringType(), True),\n",
    "    StructField(\"Company\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "    StructField(\"Phone 1\", StringType(), True),\n",
    "    StructField(\"Phone 2\", StringType(), True),\n",
    "    StructField(\"Email\", StringType(), True),\n",
    "    StructField(\"Subscription Date\", StringType(), True),\n",
    "    StructField(\"Website\", StringType(), True),\n",
    "    StructField(\"error_details\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Sample data (replace with your actual data source)\n",
    "data = [\n",
    "    (None, \"4962fdbE6Bfee6D\", \"Pam\", \"Sparks\", \"Patel-Deleon\", \"Blakemouth\", \"British Indian Ocean Territory (Chagos Archipelago)\", \"267-243-9490x035\", \"480-078-0535x889\", \"nicolas00@faulkner-kramer.com\", \"2020-11-29\", \"https://nelson.com/\", \"Index: Invalid integer, Subscription Date: Invalid date\"),\n",
    "    (\"2\", \"9b12Ae76fdBc9bE\", \"Gina\", \"Rocha\", \"Acosta, Paul and Barber\", \"East Lynnchester\", \"Costa Rica\", \"027.142.0940\", \"+1-752-593-4777x07171\", \"yfarley@morgan.com\", None, \"https://pineda-rogers.biz/\", \"Index: Invalid integer, Subscription Date: Invalid date\"),\n",
    "    (None, \"Fc2c8D2BE1AEfDb\", \"Kristina\", \"Andrade\", \"Mann Ltd\", \"Port Taraton\", \"Pitcairn Islands\", \"(640)067-7023x66846\", \"001-367-405-8096x592\", \"ivillarreal@fowler.biz\", \"2020-09-11\", \"https://foley.com/\", \"Index: Invalid integer, Subscription Date: Invalid date\"),\n",
    "    (None, \"9468BBc926AaAB3\", \"Zoe\", \"Hansen\", \"Tanner PLC\", \"Kimberlyfort\", \"Benin\", \"638-798-9796x0247\", \"(265)475-2386x9812\", \"duransheena@hughes.com\", \"2021-10-09\", \"https://franco-galloway.com/\", \"Index: Invalid integer, Subscription Date: Invalid date\"),\n",
    "    (None, \"A1505BF376CC5Ed\", \"Aimee\", \"Brooks\", \"Walker Ltd\", \"Mitchellview\", \"Malaysia\", \"112.920.9961x77753\", \"471.896.6847x82788\", \"chambersdanielle@good-cannon.com\", \"2022-03-28\", \"http://solis.org/\", \"Index: Invalid integer, Subscription Date: Invalid date\"),\n",
    "    (None, \"a24eB840950dac7\", \"Mackenzie\", \"Leonard\", \"Abbott Inc\", \"Bauerfort\", \"Ukraine\", \"+1-010-716-9313x74577\", \"315.423.2995\", \"bwheeler@hickman-acevedo.com\", \"2022-05-02\", \"http://www.pacheco.net/\", \"Index: Invalid integer, Subscription Date: Invalid date\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Group by 'error_details' and count occurrences\n",
    "result_df = df.groupBy(\"error_details\").agg(F.count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Show the result\n",
    "result_df.show(truncate=False)\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       BatchName JobName            SourceDirectory FileName FileType  Flag  \\\n",
      "1  BATCH_ACT_VAL  ACMVPF  /mnt/apps/gcs/ETL4/ACMVPF   ACMVPF      csv     1   \n",
      "2  BATCH_ACT_VAL  RTRNPF  /mnt/apps/gcs/ETL4/RTRNPF   RTRNPF      csv     1   \n",
      "\n",
      "  Delimiter  \n",
      "1         |  \n",
      "2         |  \n",
      "['/mnt/apps/Files/ETL4/ACMVPF/ACMVPF.csv', '/mnt/apps/Files/ETL4/RTRNPF/RTRNPF.csv']\n"
     ]
    }
   ],
   "source": [
    "path = \"/mnt/apps/Files/Config/master_job.csv\"\n",
    "df = pandas_read_csv(path,sep=\"|\")\n",
    "df = df.query(f\"BatchName == 'BATCH_ACT_VAL'\")\n",
    "print(df)\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        appName(\"Thread\"). \\\n",
    "        getOrCreate()\n",
    "\n",
    "tables = []\n",
    "for row in df.itertuples():\n",
    "    filePath = row.SourceDirectory + '/' + row.FileName + '.' + row.FileType\n",
    "    filePath = filePath.replace(\"/gcs\", \"/Files\")\n",
    "    tables.append(filePath)\n",
    "\n",
    "def loadTable(path):\n",
    "    sc = path.split(\"/\")[5]\n",
    "    pathParquet = f\"/mnt/apps/Files/data-movement/Parquet/{sc}\"\n",
    "    print(pathParquet)\n",
    "    df_dtype = construct_sql_schema(path=FullPathSchema, sep=\"|\")\n",
    "    df = spark.read.csv(path, header=True, inferSchema=False, schema=df_dtype, sep=\"|\")\n",
    "    result, dqc_msg, df_final, dqcId = validateDecimal(dtypes=df_dtype, df_contents=df)\n",
    "    df_count = df_final.count()\n",
    "    if result:\n",
    "        print(dqc_msg)\n",
    "        dqcOutput.append({\"JobName\":row.JobName, \"Path\":row.SourceDirectory, \"dqID\":dqcId, \"CountRecords\":df_count, \"Message\":dqc_msg, \"Status\":\"Failed\"})\n",
    "    else:\n",
    "        writeToParquet(df_final, pathParquet)\n",
    "        print(dqc_msg)\n",
    "        dqcOutput.append({\"JobName\":row.JobName, \"Path\":row.SourceDirectory, \"dqID\":dqcId, \"CountRecords\":df_count, \"Message\":dqc_msg, \"Status\":\"Successful\"})\n",
    "    \n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from queue import Queue\n",
    "\n",
    "q = Queue()\n",
    "\n",
    "workerCount = 2\n",
    "\n",
    "def run_task(function, q):\n",
    "    while not q.empty():\n",
    "        value = q.get()\n",
    "        function(value)\n",
    "        q.task_done()\n",
    "\n",
    "for table in tables:\n",
    "    q.put(table)\n",
    "\n",
    "for i in range(workerCount):\n",
    "    t=Thread(target=run_task, args=(loadTable, q))\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "\n",
    "print(\"running load\")\n",
    "q.join()\n",
    "spark.stop()\n",
    "print(\"running completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name\n",
    "import dask.dataframe as dd\n",
    "\n",
    "#outputFile = \"/mnt/apps/Files/ETL4/LAS/customers.csv\"\n",
    "outputFile = \"/mnt/apps/Files/data-movement/Renova/part-00000*\"\n",
    "FullPathSchema = \"/mnt/apps/Files/Schema/customers.csv\"\n",
    "ParquetPath = \"/mnt/apps/Files/data-movement/Parquet/Renova/part-000*\"\n",
    "PdoutputFile = \"/mnt/apps/Files/ETL4/LAS/customers.csv\"\n",
    "\n",
    "#df_dtype = construct_sql_schema(path=FullPathSchema, sep=\"|\")\n",
    "#print(df_dtype)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TestReadCSV\").getOrCreate()\n",
    "\n",
    "#CSV reader\n",
    "#df = spark.read.csv(outputFile, sep=\"|\", header=True, schema=df_dtype, inferSchema=False).repartition(10)\n",
    "\n",
    "# df_with_filename = df.withColumn(\"filename\", input_file_name())\n",
    "# null_count = df_with_filename.filter(col(\"Index\").isNotNull())\n",
    "# df_count = null_count.count()\n",
    "# print(df_count)\n",
    "# #null_count.limit(10).show(truncate=False)\n",
    "# null_count.createOrReplaceTempView(\"readCSV\")\n",
    "# #df_parquet_count = null_count_parquet.count()\n",
    "# #print(df_parquet_count)\n",
    "# null_count = spark.sql(\n",
    "#     \"\"\"\n",
    "#     SELECT country, cnt, sum(cnt) over () total_all\n",
    "#     FROM (\n",
    "#     SELECT Country, COUNT(1) AS cnt\n",
    "#         FROM readCSV\n",
    "#         GROUP BY Country\n",
    "#     ) Z\n",
    "# \"\"\")\n",
    "# print(null_count.count())\n",
    "#df.show(n=5, truncate=False) #default 20\n",
    "\n",
    "#parquet part\n",
    "# df_read_parquet = spark.read.parquet(outputFile, schema=df_dtype)\n",
    "# null_count_parquet = df_read_parquet.filter(col(\"Index\").isNotNull())\n",
    "# null_count_parquet.createOrReplaceTempView(\"my_parquet_table\")\n",
    "# #df_parquet_count = null_count_parquet.count()\n",
    "# #print(df_parquet_count)\n",
    "# null_count_parquet = spark.sql(\n",
    "#     \"\"\"\n",
    "#     SELECT country, cnt, sum(cnt) over () total_all\n",
    "#     FROM (\n",
    "#     SELECT Country, COUNT(1) AS cnt\n",
    "#         FROM my_parquet_table\n",
    "#         GROUP BY Country\n",
    "#     ) Z\n",
    "# \"\"\")\n",
    "# print(null_count_parquet.count())\n",
    "# null_count_parquet.show(n=5, truncate=False) #default 20\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of = []\n",
    "for i in range(240):\n",
    "    list_of.append(f\"Cloned_{i + 1}|Varchar(100)\")\n",
    "   \n",
    "df = pd.DataFrame(list_of, columns=[\"ColumnName\"])\n",
    "df.head(n=250)\n",
    "\n",
    "df.to_csv(\"/mnt/apps/Files/NewSChema/new_schema_cust.csv\",header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/mnt/apps/Files/Config/master_job.csv\"\n",
    "pathSchema = \"/mnt/apps/Files/Schema/\"\n",
    "\n",
    "df = pandas_read_csv(path,sep=\"|\")\n",
    "df = df.query(\"JobName == 'PAT'\")\n",
    "\n",
    "for row in df.itertuples():  # Collects all data to the driver - NOT recommended for large datasets\n",
    "    filePath = row.SourceDirectory + '/' + row.FileName + '.' + row.FileType\n",
    "    FullPathSchema = pathSchema + row.FileName + '.' + row.FileType\n",
    "    spark = SparkSession.builder.appName(f\"{row.JobName}\").getOrCreate()\n",
    "    df_dtype = construct_sql_schema(path=FullPathSchema, sep=\"|\")\n",
    "    df = spark_read_csv_from_os(spark, filePath, schema=df_dtype, quote='\"', sep=\"|\")\n",
    "    # Handle the error, e.g., skip the file, log the error, etc.\n",
    "    df.show()\n",
    "\"\"\"     result, dqc_msg, df_final = validateDecimal(dtypes=df_dtype, df_contents=df)\n",
    "    df_final.show()\n",
    "    if result:\n",
    "        print(dqc_msg)\n",
    "    else:\n",
    "        print(dqc_msg) \"\"\"\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index             int64\n",
      "User Id          object\n",
      "First Name       object\n",
      "Last Name        object\n",
      "Sex              object\n",
      "Email            object\n",
      "Phone            object\n",
      "Date of birth    object\n",
      "Job Title        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "path = \"/mnt/apps/Files/ETL4/PEOPLEPF/people.csv\"\n",
    "\n",
    "df = pandas_read_csv(path, sep=\",\")\n",
    "#df.head(1)\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "path = \"/mnt/apps/Files/Schema/etl4pat.csv\"\n",
    "fields = []\n",
    "type_mapping = {\n",
    "    \"varchar\": VarcharType,\n",
    "    \"nvarchar\": VarcharType,\n",
    "    \"int\": IntegerType(),\n",
    "    \"bigint\": LongType(),\n",
    "    \"date\": DateType(),\n",
    "    \"decimal\": DecimalType\n",
    "}\n",
    "\n",
    "df = pandas_read_csv(path,sep=\"|\")\n",
    "print(df)\n",
    "\n",
    "for row in df.itertuples():\n",
    "    name, data_type_str = row.DataType.split(\"(\", 1) if \"(\" in row.DataType else (row.DataType,\"\")\n",
    "    name = name.strip()\n",
    "    data_type_str = data_type_str[:-1].strip()\n",
    "    parts = data_type_str.split(\",\")\n",
    "    name_lower = name.lower()\n",
    "    print(data_type_str)\n",
    "\n",
    "    for keyword,spark_type in type_mapping.items():\n",
    "        if keyword in name_lower:\n",
    "            if spark_type == VarcharType:\n",
    "                data_type = VarcharType(4000) if data_type_str == \"MAX\" else VarcharType(int(data_type_str))\n",
    "                fields.append(StructField(row.ColumnName, data_type, True))\n",
    "            elif spark_type == DecimalType:\n",
    "                data_type = DecimalType() if not data_type_str else DecimalType(int(parts[0]),int(parts[1]))\n",
    "                fields.append(StructField(row.ColumnName, data_type, True))\n",
    "            else:\n",
    "                data_type = spark_type\n",
    "                fields.append(StructField(row.ColumnName, data_type, True))\n",
    "            break\n",
    "\n",
    "print(fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type_str = \"Decimal(2,0)\"\n",
    "data_type, *args = data_type_str.split(\")\")\n",
    "\n",
    "print(f\"data_type: {data_type}\")  # Output: data_type: Decimal(2,0\n",
    "print(f\"args: {args}\")          # Output: args: ['']\n",
    "\n",
    "data_type_str = \"VARCHAR\"\n",
    "data_type, *args = data_type_str.split(\")\")\n",
    "\n",
    "print(f\"data_type: {data_type}\")  # Output: data_type: VARCHAR\n",
    "print(f\"args: {args}\")          # Output: args: []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"A,2\"\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/mnt/apps/Files/Schema/etl4pat.csv\"\n",
    "test = construct_sql_schema(path=path,sep=\"|\")\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *  # Import data types for clarity\n",
    "\n",
    "# Create a SparkSession (if you don't have one already)\n",
    "spark = SparkSession.builder.appName(\"DataTypeExample\").getOrCreate()\n",
    "\n",
    "# Sample data (replace with your actual data)\n",
    "data = [(\"Alice\", 25, 2000.00), (\"Bob\", 30, 2000.00), (\"Charlie\", 22, 2000.00)]\n",
    "\n",
    "# Define the schema explicitly (best practice)\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"height\", DecimalType(2,0), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Display the schema\n",
    "df.printSchema()\n",
    "\n",
    "# Stop the SparkSession (good practice)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"/mnt/apps/gcs/ETL4/CONFIG/etl4pat*.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def writeOptions(df, path, **kwargs):\n",
    "    base_options = {  # Keep this separate\n",
    "        \"header\": \"true\",\n",
    "        \"delimiter\": \"|\",\n",
    "        \"quote\": '\"',\n",
    "        \"mode\":\"overwrite\",\n",
    "        \"format\":\"csv\"\n",
    "    }\n",
    "\n",
    "    # Correct way to merge options:\n",
    "    all_options = base_options.copy()  # Create a copy to avoid modifying base_options\n",
    "    all_options.update(kwargs)       # Add or overwrite kwargs\n",
    "    print(all_options)\n",
    "\n",
    "    df.write.options(**all_options).save(path)\n",
    "\n",
    "# Example usage (important: complete example):\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
    "\n",
    "dataMovement = \"path/to/save.csv\" # Or your actual path\n",
    "\n",
    "writeOptions(df, dataMovement)  # Now works correctly\n",
    "\n",
    "spark.stop()  # Don't forget to stop the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "path = \"/mnt/apps/Files/data-movement/ACMVPF/part*.csv.gz\"\n",
    "\n",
    "spark = SparkSession.builder.appName(\"READCSVGZ\").getOrCreate()\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "            CREATE EXTERNAL TABLE IF NOT EXISTS ACMVPF\n",
    "            USING CSV \n",
    "            OPTIONS (\n",
    "                    path '{path}',\n",
    "                    delimiter '|',\n",
    "                    header 'true',\n",
    "                    compression 'gzip'\n",
    "            )\n",
    "            \"\"\")\n",
    "\n",
    "spark.sql(\"DESCRIBE EXTENDED ACMVPF\").show(truncate=False)\n",
    "spark.sql(\"SELECT COUNT(*) FROM ACMVPF\").show()\n",
    "spark.sql(\"SELECT * FROM ACMVPF LIMIT 10\").show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-3.1.0-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting google-auth<3.0dev,>=2.26.1 (from google-cloud-storage)\n",
      "  Downloading google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting google-api-core<3.0.0dev,>=2.15.0 (from google-cloud-storage)\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.4.2 (from google-cloud-storage)\n",
      "  Downloading google_cloud_core-2.4.3-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media>=2.7.2 (from google-cloud-storage)\n",
      "  Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/bitnami/python/lib/python3.12/site-packages (from google-cloud-storage) (2.32.3)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage)\n",
      "  Downloading google_crc32c-1.7.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage)\n",
      "  Downloading googleapis_common_protos-1.69.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage)\n",
      "  Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0dev,>=2.26.1->google-cloud-storage)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0dev,>=2.26.1->google-cloud-storage)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0dev,>=2.26.1->google-cloud-storage)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/bitnami/python/lib/python3.12/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/bitnami/python/lib/python3.12/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/bitnami/python/lib/python3.12/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/bitnami/python/lib/python3.12/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2025.1.31)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading google_cloud_storage-3.1.0-py2.py3-none-any.whl (174 kB)\n",
      "Downloading google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Downloading google_cloud_core-2.4.3-py2.py3-none-any.whl (29 kB)\n",
      "Downloading google_crc32c-1.7.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading googleapis_common_protos-1.69.2-py3-none-any.whl (293 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: pyasn1, protobuf, google-crc32c, cachetools, rsa, pyasn1-modules, proto-plus, googleapis-common-protos, google-resumable-media, google-auth, google-api-core, google-cloud-core, google-cloud-storage\n",
      "\u001b[33m  WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in '/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed cachetools-5.5.2 google-api-core-2.24.2 google-auth-2.38.0 google-cloud-core-2.4.3 google-cloud-storage-3.1.0 google-crc32c-1.7.1 google-resumable-media-2.7.2 googleapis-common-protos-1.69.2 proto-plus-1.26.1 protobuf-6.30.2 pyasn1-0.6.1 pyasn1-modules-0.4.2 rsa-4.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mPackage                   Version\n",
      "------------------------- --------------\n",
      "anyio                     4.9.0\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 3.0.0\n",
      "async-lru                 2.0.5\n",
      "attrs                     25.3.0\n",
      "babel                     2.17.0\n",
      "beautifulsoup4            4.13.3\n",
      "bleach                    6.2.0\n",
      "cachetools                5.5.2\n",
      "certifi                   2025.1.31\n",
      "cffi                      1.17.1\n",
      "charset-normalizer        3.4.1\n",
      "comm                      0.2.2\n",
      "debugpy                   1.8.13\n",
      "decorator                 5.2.1\n",
      "defusedxml                0.7.1\n",
      "distlib                   0.3.9\n",
      "executing                 2.2.0\n",
      "fastjsonschema            2.21.1\n",
      "filelock                  3.17.0\n",
      "fqdn                      1.5.1\n",
      "google-api-core           2.24.2\n",
      "google-auth               2.38.0\n",
      "google-cloud-core         2.4.3\n",
      "google-cloud-storage      3.1.0\n",
      "google-crc32c             1.7.1\n",
      "google-resumable-media    2.7.2\n",
      "googleapis-common-protos  1.69.2\n",
      "h11                       0.14.0\n",
      "httpcore                  1.0.7\n",
      "httpx                     0.28.1\n",
      "idna                      3.10\n",
      "ipykernel                 6.29.5\n",
      "ipython                   9.0.2\n",
      "ipython_pygments_lexers   1.1.1\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.2\n",
      "Jinja2                    3.1.6\n",
      "json5                     0.12.0\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.23.0\n",
      "jsonschema-specifications 2024.10.1\n",
      "jupyter_client            8.6.3\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.12.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter_server            2.15.0\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.4.0\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.3\n",
      "MarkupSafe                3.0.2\n",
      "matplotlib-inline         0.1.7\n",
      "mistune                   3.1.3\n",
      "nbclient                  0.10.2\n",
      "nbconvert                 7.16.6\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "notebook_shim             0.2.4\n",
      "numpy                     2.2.4\n",
      "overrides                 7.7.0\n",
      "packaging                 24.2\n",
      "pandas                    2.2.3\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "pexpect                   4.9.0\n",
      "pip                       25.0.1\n",
      "platformdirs              4.3.6\n",
      "prometheus_client         0.21.1\n",
      "prompt_toolkit            3.0.50\n",
      "proto-plus                1.26.1\n",
      "protobuf                  6.30.2\n",
      "psutil                    7.0.0\n",
      "ptyprocess                0.7.0\n",
      "pure_eval                 0.2.3\n",
      "pyasn1                    0.6.1\n",
      "pyasn1_modules            0.4.2\n",
      "pycparser                 2.22\n",
      "Pygments                  2.19.1\n",
      "pyspark                   3.5.4\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        3.3.0\n",
      "pytz                      2025.2\n",
      "PyYAML                    6.0.2\n",
      "pyzmq                     26.4.0\n",
      "referencing               0.36.2\n",
      "requests                  2.32.3\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rpds-py                   0.24.0\n",
      "rsa                       4.9\n",
      "Send2Trash                1.8.3\n",
      "setuptools                75.8.0\n",
      "six                       1.17.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.6\n",
      "stack-data                0.6.3\n",
      "tabulate                  0.9.0\n",
      "terminado                 0.18.1\n",
      "tinycss2                  1.4.0\n",
      "tornado                   6.4.2\n",
      "traitlets                 5.14.3\n",
      "types-python-dateutil     2.9.0.20241206\n",
      "typing_extensions         4.13.1\n",
      "tzdata                    2025.2\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.3.0\n",
      "virtualenv                20.29.2\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 24.11.1\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
