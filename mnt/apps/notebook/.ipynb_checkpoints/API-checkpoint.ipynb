{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/20 17:48:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renova\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DDL Decimal/Int Data Type Structure Checks Passed.']\n",
      "[{'JobName': 'Renova', 'Path': '/mnt/apps/gcs/ETL4/LAS', 'dqID': 'DQ000001', 'CountRecords': 2000000, 'Message': ['DDL Decimal/Int Data Type Structure Checks Passed.'], 'Status': 'Successful'}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, length, regexp_replace, lit\n",
    "\n",
    "def spark_read_csv_from_os(spark, file_path, schema, header=True, **options):\n",
    "    \"\"\"\n",
    "    Reads a CSV file from the operating system into a Spark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        spark: The SparkSession object.\n",
    "        file_path: The path to the CSV file.  Can be a local path or a path\n",
    "                   that your Spark environment can access (e.g., if you're\n",
    "                   using a distributed file system like HDFS).\n",
    "        header (bool, optional): Whether the CSV file has a header row. Defaults to True.\n",
    "        inferSchema (bool, optional): Whether to infer the schema from the data. Defaults to True.\n",
    "        **options: Additional options to pass to the Spark CSV reader.  See\n",
    "                   the Spark documentation for available options like `delimiter`,\n",
    "                   `quote`, `escape`, etc.\n",
    "\n",
    "    Returns:\n",
    "        A Spark DataFrame representing the CSV data, or None if there's an error.\n",
    "\n",
    "    Raises:\n",
    "       FileNotFoundError: If the file path doesn't exist.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.read.csv(file_path, header=header, inferSchema=False, schema=schema, **options).coalesce(30)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:  # Catch other potential exceptions (e.g., parsing errors)\n",
    "        print(f\"An error occurred while reading the CSV: {e}\")\n",
    "        return None\n",
    "    \n",
    "def pandas_read_csv(file_path,**options):\n",
    "    \"\"\"\n",
    "        Read small volume of data only using read.csv\n",
    "        Args:\n",
    "            **Options ----> Any\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path,**options)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:  # Catch other potential exceptions (e.g., parsing errors)\n",
    "        print(f\"An error occurred while reading the CSV: {e}\")\n",
    "        return None\n",
    "    \n",
    "def construct_sql_schema(**kwargs):\n",
    "    \"\"\"\n",
    "        Args: kwargs path and sep -->>> Any\n",
    "        this function is best practice to compute large amount of data to not reading schema metadata\n",
    "        recommendation : \n",
    "    \"\"\"\n",
    "\n",
    "    fields = []\n",
    "    type_mapping = {\n",
    "        \"varchar\": StringType(),\n",
    "        \"nvarchar\": StringType(),\n",
    "        \"int\": IntegerType(),\n",
    "        \"bigint\": LongType(),\n",
    "        \"date\": DateType(),\n",
    "        \"decimal\": DecimalType\n",
    "    }\n",
    "\n",
    "    df = pandas_read_csv(kwargs[\"path\"],sep=kwargs[\"sep\"])\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        try:\n",
    "            name, data_type_str = row.DataType.split(\"(\", 1) if \"(\" in row.DataType else (row.DataType,\"\")\n",
    "            name = name.strip()\n",
    "            data_type_str = data_type_str[:-1].strip()\n",
    "            parts = data_type_str.split(\",\")\n",
    "            name_lower = name.lower()\n",
    "\n",
    "            for keyword,spark_type in type_mapping.items():\n",
    "                if keyword in name_lower:\n",
    "                    if spark_type == DecimalType:\n",
    "                        data_type = DecimalType() if not data_type_str else DecimalType(int(parts[0]),int(parts[1]))\n",
    "                        fields.append(StructField(row.ColumnName, data_type, True))\n",
    "                    else:\n",
    "                        data_type = spark_type\n",
    "                        fields.append(StructField(row.ColumnName, data_type, True))\n",
    "                    break\n",
    "        except Exception as e:  # Catch other potential errors\n",
    "            print(f\"Error processing file in construct schema {kwargs[\"path\"]}: {e}\")\n",
    "            return None\n",
    "    return StructType(fields)\n",
    "\n",
    "def validateDecimal(**kwargs):\n",
    "    df_contents = kwargs[\"df_contents\"]\n",
    "    is_valid = False\n",
    "    errors = []\n",
    "    dqcId = \"DQ000001\"\n",
    "    for field in kwargs[\"dtypes\"]:\n",
    "        colName = field.name\n",
    "        dType = str(field.dataType)\n",
    "        if \"decimal\" in dType.lower():\n",
    "            #print(colName)\n",
    "            df_cleaned = df_contents.withColumn(\n",
    "                f\"{colName}_cleaned\",\n",
    "                regexp_replace(col(colName), \"[^0-9.]\", \"\")\n",
    "            )\n",
    "            df_empty = df_cleaned.filter((col(f\"{colName}_cleaned\")).isNull()) # is null due to schema defined as decimal if we use inferSchema its a string\n",
    "            empty_count = df_empty.count()\n",
    "\n",
    "            if empty_count > 0:\n",
    "                is_valid = True\n",
    "                error_msg = (f\"Invalid {colName} values (containing only non-numeric characters). Total count: {empty_count}\")\n",
    "                errors.append(error_msg)\n",
    "            df_contents = df_contents.drop(f\"{colName}_cleaned\") \n",
    "            \n",
    "    if is_valid == False:        \n",
    "        error_msg = \"DDL Decimal/Int Data Type Structure Checks Passed.\"\n",
    "        errors.append(error_msg)\n",
    "    msg = \"\\n\".join(errors) if errors else \"Data Quality Checks Passed.\" # this is for breakdown into rows from array\n",
    "    return is_valid, errors, df_contents, dqcId\n",
    "\n",
    "def writeOptions(df, dataMovement, **kwargs):\n",
    "    base_options = {  # Keep this separate\n",
    "        \"header\": \"true\",\n",
    "        \"delimiter\": \"|\",\n",
    "        \"quote\": '\"'\n",
    "    }\n",
    "    base_options.update(kwargs)\n",
    "\n",
    "    # existing_columns = df.columns\n",
    "    # columns_to_clone = len(existing_columns)\n",
    "\n",
    "    # # for i in range(20):\n",
    "    # #     if i < columns_to_clone:\n",
    "    # #         original_col_name = existing_columns[i]\n",
    "    # #         new_col_name = f\"Cloned_{original_col_name}\"\n",
    "    # #         df = df.withColumn(new_col_name, col(original_col_name))\n",
    "    # #     else:\n",
    "    # #         print()\n",
    "    # #         new_col_name = f\"Cloned_{i + 1}\"\n",
    "    #         df = df.withColumn(new_col_name, lit(\"hardcoded contents for 250 columns\"))\n",
    "\n",
    "    #df.coalesce(50).write.parquet(dataMovement, mode=\"overwrite\", compression=\"snappy\")\n",
    "    df.coalesce(1).write.format(\"csv\").mode(\"overwrite\").options(**base_options).save(dataMovement)\n",
    "\n",
    "def writeToParquet(df, path):\n",
    "    df = df.coalesce(50)\n",
    "    df.write.parquet(path, mode=\"overwrite\", compression=\"snappy\")\n",
    "\n",
    "def loadTable(**kwargs):\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS spark_catalog.default.{kwargs['tableName']}\") \n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE EXTERNAL TABLE {kwargs[\"tableName\"]}\n",
    "    USING CSV\n",
    "    OPTIONS (\n",
    "        header 'true',  -- If your CSV has a header row\n",
    "        inferSchema 'false', -- Important: Set to false since we provide schema\n",
    "        delimiter '|' -- Specify the delimiter if it's not a comma\n",
    "    )\n",
    "        LOCATION '{kwargs[\"path\"]}'\n",
    "    \"\"\")\n",
    "    df = spark.sql(f\"SELECT * FROM {kwargs[\"tableName\"]}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"/mnt/apps/Files/Config/master_job.csv\"\n",
    "    pathSchema = \"/mnt/apps/Files/Schema/\"\n",
    "    outputFile = \"/mnt/apps/Files/data-movement/\"\n",
    "    parquetOutput = \"/mnt/apps/Files/data-movement/Parquet/\"\n",
    "    dqcOutput = []\n",
    "\n",
    "    spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        appName(\"Testing\"). \\\n",
    "        getOrCreate()\n",
    "\n",
    "    df = pandas_read_csv(path,sep=\"|\")\n",
    "    df = df.query(f\"JobName == 'Renova'\")\n",
    "\n",
    "    for row in df.itertuples():  # Collects all data to the driver - NOT recommended for large datasets\n",
    "        filePath = row.SourceDirectory + '/' + row.FileName + '.' + row.FileType\n",
    "        filePath = filePath.replace(\"/gcs\", \"/Files\")\n",
    "        dataMovement = outputFile + row.JobName + '/'\n",
    "        dataMovementParquet = parquetOutput + row.JobName\n",
    "        print(row.JobName)\n",
    "        FullPathSchema = pathSchema + row.FileName + '.' + row.FileType\n",
    "        #spark.stop()\n",
    "        df_dtype = construct_sql_schema(path=FullPathSchema, sep=\"|\")\n",
    "        df = spark_read_csv_from_os(spark, filePath, schema=df_dtype, quote='\"', sep=row.Delimiter)\n",
    "        #df_load = loadTable(tableName=row.JobName, path=filePath, sep=row.Delimiter)\n",
    "        result, dqc_msg, df_final, dqcId = validateDecimal(dtypes=df_dtype, df_contents=df)\n",
    "        #df_final.show()\n",
    "        #sql = spark.sql(\"SELECT * FROM PAT\")\n",
    "        #sql.show()\n",
    "        df_count = df_final.count()\n",
    "        #print(df.rdd.getNumPartitions()) # rdd create while read csv\n",
    "        #df_count = df_final.count()\n",
    "        #print(df.columns)\n",
    "        #df.limit(10)\n",
    "        #num_cores = spark.sparkContext.defaultParallelism\n",
    "        #print(f\"Type of num_cores: {type(num_cores)}\") # Add this line!\n",
    "        #num_partitions = num_cores * 1\n",
    "        #print(f\"Number of partitions: {num_partitions}\")\n",
    "        #partitioned_df = df.repartition(num_partitions)\n",
    "        if result:\n",
    "            print(dqc_msg)\n",
    "            dqcOutput.append({\"JobName\":row.JobName, \"Path\":row.SourceDirectory, \"dqID\":dqcId, \"CountRecords\":df_count, \"Message\":dqc_msg, \"Status\":\"Failed\"})\n",
    "        else:\n",
    "            #writeOptions(df_final, dataMovement)\n",
    "            #writeToParquet(df, dataMovementParquet)\n",
    "            #df_load = loadTable(tableName=row.JobName, path=filePath, sep=row.Delimiter)\n",
    "            #df_load.show()\n",
    "            print(dqc_msg)\n",
    "            dqcOutput.append({\"JobName\":row.JobName, \"Path\":row.SourceDirectory, \"dqID\":dqcId, \"CountRecords\":df_count, \"Message\":dqc_msg, \"Status\":\"Successful\"})\n",
    "    print(dqcOutput)\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  JobName            SourceDirectory FileName FileType  Flag Delimiter\n",
      "1  ACMVPF  /mnt/apps/gcs/ETL4/ACMVPF   ACMVPF      csv     1         |\n",
      "2  RTRNPF  /mnt/apps/gcs/ETL4/RTRNPF   RTRNPF      csv     1         |\n",
      "['/mnt/apps/Files/ETL4/ACMVPF/ACMVPF.csv', '/mnt/apps/Files/ETL4/RTRNPF/RTRNPF.csv']\n"
     ]
    }
   ],
   "source": [
    "path = \"/mnt/apps/Files/Config/master_job.csv\"\n",
    "df = pandas_read_csv(path,sep=\"|\")\n",
    "df = df.query(f\"JobName == 'ACMVPF' | JobName =='RTRNPF'\")\n",
    "print(df)\n",
    "\n",
    "sparkNew = SparkSession. \\\n",
    "        builder. \\\n",
    "        appName(\"Thread\"). \\\n",
    "        getOrCreate()\n",
    "\n",
    "tables = []\n",
    "for row in df.itertuples():\n",
    "    filePath = row.SourceDirectory + '/' + row.FileName + '.' + row.FileType\n",
    "    filePath = filePath.replace(\"/gcs\", \"/Files\")\n",
    "    tables.append(filePath)\n",
    "\n",
    "def loadTable(path):\n",
    "    sc = path.split(\"/\")[5]\n",
    "    pathParquet = f\"/mnt/apps/Files/data-movement/Parquet/{sc}\"\n",
    "    print(pathParquet)\n",
    "    df_dtype = construct_sql_schema(path=FullPathSchema, sep=\"|\")\n",
    "    df = sparkNew.read.csv(path, header=True, inferSchema=False, schema=df_dtype, sep=\"|\")\n",
    "    writeToParquet(df, pathParquet)\n",
    "\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/apps/Files/data-movement/Parquet/ACMVPF\n",
      "/mnt/apps/Files/data-movement/Parquet/RTRNPF\n",
      "running load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/20 17:54:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/20 17:54:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/02/20 17:54:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/02/20 17:54:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/02/20 17:54:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "25/02/20 17:54:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "25/02/20 17:54:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 54.29% for 14 writers\n",
      "25/02/20 17:54:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 50.67% for 15 writers\n",
      "25/02/20 17:54:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 47.50% for 16 writers\n",
      "25/02/20 17:54:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 50.67% for 15 writers\n",
      "25/02/20 17:54:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 47.50% for 16 writers\n",
      "25/02/20 17:54:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 50.67% for 15 writers\n",
      "25/02/20 17:54:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 54.29% for 14 writers\n",
      "25/02/20 17:54:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "25/02/20 17:54:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "25/02/20 17:54:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/02/20 17:54:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/02/20 17:54:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/02/20 17:54:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/20 17:54:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/02/20 17:54:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/02/20 17:54:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/02/20 17:54:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/02/20 17:54:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "25/02/20 17:54:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "25/02/20 17:54:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 54.29% for 14 writers\n",
      "25/02/20 17:54:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 50.67% for 15 writers\n",
      "25/02/20 17:54:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 47.50% for 16 writers\n",
      "25/02/20 17:54:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 50.67% for 15 writers\n",
      "25/02/20 17:54:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 54.29% for 14 writers\n",
      "25/02/20 17:54:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "25/02/20 17:54:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "25/02/20 17:54:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "25/02/20 17:54:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/02/20 17:54:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/02/20 17:54:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running completed\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "from queue import Queue\n",
    "\n",
    "q = Queue()\n",
    "\n",
    "workerCount = 2\n",
    "\n",
    "def run_task(function, q):\n",
    "    while not q.empty():\n",
    "        value = q.get()\n",
    "        function(value)\n",
    "        q.task_done()\n",
    "\n",
    "for table in tables:\n",
    "    q.put(table)\n",
    "\n",
    "for i in range(workerCount):\n",
    "    t=Thread(target=run_task, args=(loadTable, q))\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "\n",
    "print(\"running load\")\n",
    "q.join()\n",
    "print(\"running completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name\n",
    "import dask.dataframe as dd\n",
    "\n",
    "#outputFile = \"/mnt/apps/Files/ETL4/LAS/customers.csv\"\n",
    "outputFile = \"/mnt/apps/Files/data-movement/Renova/part-00000*\"\n",
    "FullPathSchema = \"/mnt/apps/Files/Schema/customers.csv\"\n",
    "ParquetPath = \"/mnt/apps/Files/data-movement/Parquet/Renova/part-000*\"\n",
    "PdoutputFile = \"/mnt/apps/Files/ETL4/LAS/customers.csv\"\n",
    "\n",
    "#df_dtype = construct_sql_schema(path=FullPathSchema, sep=\"|\")\n",
    "#print(df_dtype)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TestReadCSV\").getOrCreate()\n",
    "\n",
    "#CSV reader\n",
    "#df = spark.read.csv(outputFile, sep=\"|\", header=True, schema=df_dtype, inferSchema=False).repartition(10)\n",
    "\n",
    "# df_with_filename = df.withColumn(\"filename\", input_file_name())\n",
    "# null_count = df_with_filename.filter(col(\"Index\").isNotNull())\n",
    "# df_count = null_count.count()\n",
    "# print(df_count)\n",
    "# #null_count.limit(10).show(truncate=False)\n",
    "# null_count.createOrReplaceTempView(\"readCSV\")\n",
    "# #df_parquet_count = null_count_parquet.count()\n",
    "# #print(df_parquet_count)\n",
    "# null_count = spark.sql(\n",
    "#     \"\"\"\n",
    "#     SELECT country, cnt, sum(cnt) over () total_all\n",
    "#     FROM (\n",
    "#     SELECT Country, COUNT(1) AS cnt\n",
    "#         FROM readCSV\n",
    "#         GROUP BY Country\n",
    "#     ) Z\n",
    "# \"\"\")\n",
    "# print(null_count.count())\n",
    "#df.show(n=5, truncate=False) #default 20\n",
    "\n",
    "#parquet part\n",
    "# df_read_parquet = spark.read.parquet(outputFile, schema=df_dtype)\n",
    "# null_count_parquet = df_read_parquet.filter(col(\"Index\").isNotNull())\n",
    "# null_count_parquet.createOrReplaceTempView(\"my_parquet_table\")\n",
    "# #df_parquet_count = null_count_parquet.count()\n",
    "# #print(df_parquet_count)\n",
    "# null_count_parquet = spark.sql(\n",
    "#     \"\"\"\n",
    "#     SELECT country, cnt, sum(cnt) over () total_all\n",
    "#     FROM (\n",
    "#     SELECT Country, COUNT(1) AS cnt\n",
    "#         FROM my_parquet_table\n",
    "#         GROUP BY Country\n",
    "#     ) Z\n",
    "# \"\"\")\n",
    "# print(null_count_parquet.count())\n",
    "# null_count_parquet.show(n=5, truncate=False) #default 20\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of = []\n",
    "for i in range(240):\n",
    "    list_of.append(f\"Cloned_{i + 1}|Varchar(100)\")\n",
    "   \n",
    "df = pd.DataFrame(list_of, columns=[\"ColumnName\"])\n",
    "df.head(n=250)\n",
    "\n",
    "df.to_csv(\"/mnt/apps/Files/NewSChema/new_schema_cust.csv\",header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/mnt/apps/Files/Config/master_job.csv\"\n",
    "pathSchema = \"/mnt/apps/Files/Schema/\"\n",
    "\n",
    "df = pandas_read_csv(path,sep=\"|\")\n",
    "df = df.query(\"JobName == 'PAT'\")\n",
    "\n",
    "for row in df.itertuples():  # Collects all data to the driver - NOT recommended for large datasets\n",
    "    filePath = row.SourceDirectory + '/' + row.FileName + '.' + row.FileType\n",
    "    FullPathSchema = pathSchema + row.FileName + '.' + row.FileType\n",
    "    spark = SparkSession.builder.appName(f\"{row.JobName}\").getOrCreate()\n",
    "    df_dtype = construct_sql_schema(path=FullPathSchema, sep=\"|\")\n",
    "    df = spark_read_csv_from_os(spark, filePath, schema=df_dtype, quote='\"', sep=\"|\")\n",
    "    # Handle the error, e.g., skip the file, log the error, etc.\n",
    "    df.show()\n",
    "\"\"\"     result, dqc_msg, df_final = validateDecimal(dtypes=df_dtype, df_contents=df)\n",
    "    df_final.show()\n",
    "    if result:\n",
    "        print(dqc_msg)\n",
    "    else:\n",
    "        print(dqc_msg) \"\"\"\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/mnt/apps/Files/Config/master_job.csv\"\n",
    "\n",
    "df = pandas_read_csv(path,sep=\"|\")\n",
    "df = df.query(\"Flag == 1\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "path = \"/mnt/apps/Files/Schema/etl4pat.csv\"\n",
    "fields = []\n",
    "type_mapping = {\n",
    "    \"varchar\": VarcharType,\n",
    "    \"nvarchar\": VarcharType,\n",
    "    \"int\": IntegerType(),\n",
    "    \"bigint\": LongType(),\n",
    "    \"date\": DateType(),\n",
    "    \"decimal\": DecimalType\n",
    "}\n",
    "\n",
    "df = pandas_read_csv(path,sep=\"|\")\n",
    "print(df)\n",
    "\n",
    "for row in df.itertuples():\n",
    "    name, data_type_str = row.DataType.split(\"(\", 1) if \"(\" in row.DataType else (row.DataType,\"\")\n",
    "    name = name.strip()\n",
    "    data_type_str = data_type_str[:-1].strip()\n",
    "    parts = data_type_str.split(\",\")\n",
    "    name_lower = name.lower()\n",
    "    print(data_type_str)\n",
    "\n",
    "    for keyword,spark_type in type_mapping.items():\n",
    "        if keyword in name_lower:\n",
    "            if spark_type == VarcharType:\n",
    "                data_type = VarcharType(4000) if data_type_str == \"MAX\" else VarcharType(int(data_type_str))\n",
    "                fields.append(StructField(row.ColumnName, data_type, True))\n",
    "            elif spark_type == DecimalType:\n",
    "                data_type = DecimalType() if not data_type_str else DecimalType(int(parts[0]),int(parts[1]))\n",
    "                fields.append(StructField(row.ColumnName, data_type, True))\n",
    "            else:\n",
    "                data_type = spark_type\n",
    "                fields.append(StructField(row.ColumnName, data_type, True))\n",
    "            break\n",
    "\n",
    "print(fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type_str = \"Decimal(2,0)\"\n",
    "data_type, *args = data_type_str.split(\")\")\n",
    "\n",
    "print(f\"data_type: {data_type}\")  # Output: data_type: Decimal(2,0\n",
    "print(f\"args: {args}\")          # Output: args: ['']\n",
    "\n",
    "data_type_str = \"VARCHAR\"\n",
    "data_type, *args = data_type_str.split(\")\")\n",
    "\n",
    "print(f\"data_type: {data_type}\")  # Output: data_type: VARCHAR\n",
    "print(f\"args: {args}\")          # Output: args: []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"A,2\"\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/mnt/apps/Files/Schema/etl4pat.csv\"\n",
    "test = construct_sql_schema(path=path,sep=\"|\")\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *  # Import data types for clarity\n",
    "\n",
    "# Create a SparkSession (if you don't have one already)\n",
    "spark = SparkSession.builder.appName(\"DataTypeExample\").getOrCreate()\n",
    "\n",
    "# Sample data (replace with your actual data)\n",
    "data = [(\"Alice\", 25, 2000.00), (\"Bob\", 30, 2000.00), (\"Charlie\", 22, 2000.00)]\n",
    "\n",
    "# Define the schema explicitly (best practice)\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"height\", DecimalType(2,0), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Display the schema\n",
    "df.printSchema()\n",
    "\n",
    "# Stop the SparkSession (good practice)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"/mnt/apps/gcs/ETL4/CONFIG/etl4pat*.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def writeOptions(df, path, **kwargs):\n",
    "    base_options = {  # Keep this separate\n",
    "        \"header\": \"true\",\n",
    "        \"delimiter\": \"|\",\n",
    "        \"quote\": '\"',\n",
    "        \"mode\":\"overwrite\",\n",
    "        \"format\":\"csv\"\n",
    "    }\n",
    "\n",
    "    # Correct way to merge options:\n",
    "    all_options = base_options.copy()  # Create a copy to avoid modifying base_options\n",
    "    all_options.update(kwargs)       # Add or overwrite kwargs\n",
    "    print(all_options)\n",
    "\n",
    "    df.write.options(**all_options).save(path)\n",
    "\n",
    "# Example usage (important: complete example):\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
    "\n",
    "dataMovement = \"path/to/save.csv\" # Or your actual path\n",
    "\n",
    "writeOptions(df, dataMovement)  # Now works correctly\n",
    "\n",
    "spark.stop()  # Don't forget to stop the SparkSession"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
