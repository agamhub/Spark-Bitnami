{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aee6c1b-f8aa-4065-9ad7-bf27b1107ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Int: long (nullable = true)\n",
      " |-- Decimal: decimal(18,2) (nullable = true)\n",
      " |-- Float: decimal(18,2) (nullable = true)\n",
      " |-- Money: string (nullable = true)\n",
      " |-- Bigint: long (nullable = true)\n",
      " |-- DateTime: timestamp (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      "\n",
      "+---+-------+------+-------+------+-------------------+----------+\n",
      "|Int|Decimal| Float|  Money|Bigint|           DateTime|      Date|\n",
      "+---+-------+------+-------+------+-------------------+----------+\n",
      "|  1| 141.23|141.23|4141.32|     0|2025-03-22 10:00:00|2025-03-22|\n",
      "+---+-------+------+-------+------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, regexp_replace\n",
    "\n",
    "def spark_read_csv_from_os(spark, file_path, schema, **kwargs):\n",
    "    base_options = {\n",
    "        \"inferSchema\": \"False\",\n",
    "        \"header\": \"True\",\n",
    "        \"quote\": '\"',\n",
    "        \"columnNameOfCorruptRecord\": \"rejected_records\",\n",
    "        \"mode\": \"PERMISSIVE\"\n",
    "    }\n",
    "    base_options.update(kwargs)\n",
    "    \n",
    "    try:\n",
    "        #schema = StructType(schema.fields + [StructField(\"rejected_records\", StringType(), True)])\n",
    "        df = spark.read.options(**base_options).schema(schema).csv(file_path)\n",
    "    \n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:  # Catch other potential exceptions (e.g., parsing errors)\n",
    "        print(f\"An error occurred while reading the CSV: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"/mnt/apps/Files/ETL4/TMP/test.csv\"\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"Int\", LongType(), True),\n",
    "        StructField(\"Decimal\", DecimalType(18, 2), True),\n",
    "        StructField(\"Float\", DecimalType(18, 2), True),\n",
    "        StructField(\"Money\", StringType(), True),\n",
    "        StructField(\"Bigint\", LongType(), True),\n",
    "        StructField(\"DateTime\", TimestampType(), True),\n",
    "        StructField(\"Date\", DateType(), True)\n",
    "    ])\n",
    "    \n",
    "    spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        appName(\"Testing\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.ui.port\", \"4222\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    df = spark_read_csv_from_os(spark, path, schema, sep=\"|\")\n",
    "    df = df.withColumn(\"Money\", regexp_replace(col(\"Money\"), \",\", \".\"))\n",
    "    \n",
    "    df.printSchema()\n",
    "    df.show()\n",
    "    \n",
    "    spark.stop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f96543d2-ea4d-4423-bcd1-950b4fcb9e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def loadTable(**kwargs):\n",
    "    pathCheck = kwargs[\"path\"].replace(\"/part*\",\"\")\n",
    "    if not os.path.exists(pathCheck):\n",
    "        return None\n",
    "    try:\n",
    "        if kwargs[\"loadType\"] == \"Parquet\":\n",
    "            sparkDqc.sql(f\"\"\"\n",
    "            CREATE EXTERNAL TABLE IF NOT EXISTS {kwargs[\"tableName\"]}\n",
    "            USING PARQUET LOCATION '{kwargs[\"path\"]}'\n",
    "            \"\"\")\n",
    "            return True\n",
    "        else:\n",
    "            sparkDqc.sql(f\"\"\"\n",
    "            CREATE EXTERNAL TABLE IF NOT EXISTS {kwargs[\"tableName\"]}\n",
    "            USING CSV\n",
    "            OPTIONS (\n",
    "                'path' '{kwargs[\"path\"]}',\n",
    "                'delimiter' '|',\n",
    "                'compression' 'gzip',\n",
    "                'header' 'true'\n",
    "            )\n",
    "            \"\"\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    path = \"/mnt/apps/Files/data-movement/Parquet/RTRNPF\"\n",
    "    \n",
    "    sparkDqc =  SparkSession. \\\n",
    "            builder. \\\n",
    "            appName(\"parquet\") \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .config(\"spark.ui.port\", \"4222\") \\\n",
    "            .getOrCreate()\n",
    "            \n",
    "    df_table = loadTable(path=path, loadType=\"Parquet\", tableName=\"RTRNPF\")\n",
    "    df_sql = sparkDqc.sql(\"SELECT * FROM RTRNPF LIMIT 100\")\n",
    "\n",
    "    html = df_sql.toPandas().to_html()  # Convert to HTML\n",
    "    styled_html = f\"\"\"\n",
    "    <style>\n",
    "      table {{width: 100%; border-collapse: collapse;}}\n",
    "      th, td {{border: 1px solid black; padding: 8px; text-align: left;}}\n",
    "    </style>\n",
    "    {html}\n",
    "    \"\"\"\n",
    "    HTML(styled_html)\n",
    "\n",
    "    sparkDqc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e6d1651-a55f-4442-9675-d571cd0e5fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-numeric rows: 3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, isnull, count, length\n",
    "\n",
    "def count_non_numeric(df, colName):\n",
    "    df_cleaned = df.withColumn(\n",
    "        f\"{colName}_cleaned\",\n",
    "        regexp_replace(col(colName), \"[^0-9.]\", \"\")\n",
    "    )\n",
    "\n",
    "    df_non_numeric = df_cleaned.filter(\n",
    "        (length(col(f\"{colName}_cleaned\")) == 0) & col(colName).isNotNull()\n",
    "    )\n",
    "\n",
    "    non_numeric_count = df_non_numeric.count()\n",
    "\n",
    "    return non_numeric_count\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CountNonNumeric\").getOrCreate()\n",
    "\n",
    "data = [(\"1000.31\",), (None,), (\"AGAM\",), (\"AGAM\",), (\"AGAM\",)]\n",
    "df = spark.createDataFrame(data, [\"AMT\"])\n",
    "\n",
    "non_numeric_count = count_non_numeric(df, \"AMT\")\n",
    "print(f\"Number of non-numeric rows: {non_numeric_count}\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee009aaf-db48-4004-ba88-c4caf0bf51bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/25 12:46:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PrettySparkOutput\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1, \"4962fdbE6Bfee6D\", \"Pam\", \"Sparks\", \"Patel-Deleon\", \"Blakemouth\", \"British Indian Ocean Territory (Chagos Archipelago)\", \"267-243-9490x035\", \"480-078-0535x889\", \"nicolas00@faulkner-kramer.com\", \"2020-11-29\", \"https://nelson.com/\", \"1000232,32\"),\n",
    "    (2, \"9b12Ae76fdBc9bE\", \"Gina\", \"Rocha\", \"Acosta, Paul and Barber\", \"East Lynnchester\", \"Costa Rica\", \"027.142.0940\", \"+1-752-593-4777x07171\", \"yfarley@morgan.com\", \"2021-01-03\", \"https://pineda-rogers.biz/\", \"1000232,32\"),\n",
    "    (3, \"39edFd2F60C85BC\", \"Kristie\", \"Greer\", \"Ochoa PLC\", \"West Pamela\", \"Ecuador\", \"+1-049-168-7497x5053\", \"+1-311-216-7855\", \"jennyhayden@p\", None, None, None)\n",
    "]\n",
    "\n",
    "columns = [\"Index\", \"Customer Id\", \"First Name\", \"Last Name\", \"Company\", \"City\", \"Country\", \"Phone 1\", \"Phone 2\", \"Email\", \"Subscription Date\", \"Website\", \"Budget\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "pandas_df = df.toPandas()\n",
    "#print(pandas_df)\n",
    "html = pandas_df.to_html(index=False) # index=false to remove index column from html\n",
    "styled_html = f\"\"\"\n",
    "<style>\n",
    "  table {{width: 100%; border-collapse: collapse;}}\n",
    "  th, td {{border: 1px solid black; padding: 8px; text-align: left; word-wrap: break-word;}}\n",
    "</style>\n",
    "{html}\n",
    "\"\"\"\n",
    "HTML(styled_html)\n",
    "HTML(\"<h1>Hello, World!</h1>\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec65c7bf-7b42-4d24-bd08-8d9c235ddbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PrettySparkOutput\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1, \"4962fdbE6Bfee6D\", \"Pam\", \"Sparks\", \"Patel-Deleon\", \"Blakemouth\", \"British Indian Ocean Territory (Chagos Archipelago)\", \"267-243-9490x035\", \"480-078-0535x889\", \"nicolas00@faulkner-kramer.com\", \"2020-11-29\", \"https://nelson.com/\", \"1000232,32\"),\n",
    "    (2, \"9b12Ae76fdBc9bE\", \"Gina\", \"Rocha\", \"Acosta, Paul and Barber\", \"East Lynnchester\", \"Costa Rica\", \"027.142.0940\", \"+1-752-593-4777x07171\", \"yfarley@morgan.com\", \"2021-01-03\", \"https://pineda-rogers.biz/\", \"1000232,32\"),\n",
    "    (3, \"39edFd2F60C85BC\", \"Kristie\", \"Greer\", \"Ochoa PLC\", \"West Pamela\", \"Ecuador\", \"+1-049-168-7497x5053\", \"+1-311-216-7855\", \"jennyhayden@p\", None, None, None)\n",
    "]\n",
    "\n",
    "columns = [\"Index\", \"Customer Id\", \"First Name\", \"Last Name\", \"Company\", \"City\", \"Country\", \"Phone 1\", \"Phone 2\", \"Email\", \"Subscription Date\", \"Website\", \"Budget\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "pandas_df = df.toPandas()\n",
    "pandas_df = pandas_df.fillna(\"\")\n",
    "\n",
    "print(pandas_df)  # Check Pandas DataFrame content\n",
    "\n",
    "HTML(\"<h1>Hello, World!</h1>\")  # Basic HTML test\n",
    "\n",
    "html = pandas_df.to_html(index=False)\n",
    "print(html) # check the html string.\n",
    "\n",
    "HTML(html)  # Try displaying basic HTML\n",
    "\n",
    "display(pandas_df) # test the display method.\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
