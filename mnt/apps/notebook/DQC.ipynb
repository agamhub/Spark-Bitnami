{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+----------+------------------------------------+-----+----------+\n",
      "|   |   BatchName   |  DqcId   |              Scripts               | Run | RefTable |\n",
      "+---+---------------+----------+------------------------------------+-----+----------+\n",
      "| 0 | BATCH_ACT_VAL | DQ000001 |  SELECT COUNT(1) CNT FROM ACMVPF   |  1  |   nan    |\n",
      "| 1 | BATCH_ACT_VAL | DQ000002 |  SELECT COUNT(1) CNT FROM GENLPF   |  1  |   nan    |\n",
      "| 2 | BATCH_ACT_VAL | DQ000003 | SELECT COUNT(1) CNT FROM PEOPLEPLF |  1  | PEOPLEPF |\n",
      "+---+---------------+----------+------------------------------------+-----+----------+\n",
      "+---+---------------+----------+\n",
      "|   |   BatchName   | JobName  |\n",
      "+---+---------------+----------+\n",
      "| 0 | BATCH_ACT_VAL |  ACMVPF  |\n",
      "| 1 | BATCH_ACT_VAL |  GENLPF  |\n",
      "| 2 | BATCH_ACT_VAL | PEOPLEPF |\n",
      "+---+---------------+----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/28 17:14:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acmvpf', 'genlpf', 'peoplepf']\n",
      "['acmvpf', 'genlpf', 'peoplepf']\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/mnt/apps/Files/Test already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 94\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(item \u001b[38;5;129;01min\u001b[39;00m ListTable() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m listJob):\n\u001b[1;32m     90\u001b[0m     df \u001b[38;5;241m=\u001b[39m sparkDqc\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124m                    SELECT COUNT(1) JML FROM PEOPLEPF\u001b[39m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124m                    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepartition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestWrite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mshow())\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/mnt/apps/Files/Test already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from random import random\n",
    "from operator import add\n",
    "from tabulate import tabulate\n",
    "import logging\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from threading import Thread\n",
    "from queue import Queue\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, length, regexp_replace, lit\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def pandas_read_csv(file_path,**options):\n",
    "    \"\"\"\n",
    "        Read small volume of data only using read.csv\n",
    "        Args:\n",
    "            **Options ----> Any\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path,**options)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:  # Catch other potential exceptions (e.g., parsing errors)\n",
    "        print(f\"An error occurred while reading the CSV: {e}\")\n",
    "        return None\n",
    "\n",
    "def loadTable(**kwargs):\n",
    "    pathCheck = kwargs[\"path\"].replace(\"/part*\",\"\")\n",
    "    if not os.path.exists(pathCheck):\n",
    "        return None\n",
    "    try:\n",
    "        sparkDqc.sql(f\"\"\"\n",
    "        CREATE EXTERNAL TABLE IF NOT EXISTS {kwargs[\"tableName\"]}\n",
    "        USING PARQUET LOCATION '{kwargs[\"path\"]}'\n",
    "        \"\"\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "def ListTable():\n",
    "    listCreatedTable = []\n",
    "    tables = sparkDqc.sql(\"SHOW TABLES\").collect()\n",
    "    for table in tables:\n",
    "       listCreatedTable.append(table.tableName)\n",
    "    return listCreatedTable\n",
    "\n",
    "def getTables(**kwargs):\n",
    "    df = pandas_read_csv(kwargs[\"path\"], sep=\"|\")\n",
    "    df = df.query(f\"BatchName == '{batchname}'\")\n",
    "    print(tabulate(df.head(), headers='keys', tablefmt='pretty'))\n",
    "    df_refTable = df.query(\"RefTable.notnull()\")\n",
    "    distinct_reftable = df_refTable[[\"BatchName\",\"RefTable\"]].drop_duplicates().rename(columns={\"RefTable\": \"JobName\"})\n",
    "    df_job = pandas_read_csv(kwargs[\"LoadPath\"], sep=\"|\")\n",
    "    df_job = df_job[[\"BatchName\",\"JobName\"]].query(f\"BatchName == '{batchname}'\")\n",
    "    joined_df = pd.concat([df_job, distinct_reftable], ignore_index=True).drop_duplicates()\n",
    "    print(tabulate(joined_df.head(), headers='keys', tablefmt='pretty'))\n",
    "    return joined_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"/mnt/apps/Files/Config/DataQuality_Config.csv\"\n",
    "    LoadPath = \"/mnt/apps/Files/Config/master_job.csv\"\n",
    "    parquetOutput = \"/mnt/apps/Files/data-movement/Parquet/\"\n",
    "    batchname = \"BATCH_ACT_VAL\"\n",
    "    testWrite = \"/mnt/apps/Files/Test\"\n",
    "\n",
    "    getDf = getTables(path=path, LoadPath=LoadPath)\n",
    "\n",
    "    sparkDqc = (\n",
    "        SparkSession\n",
    "            .builder\n",
    "            .appName(f\"{batchname}_DQC\")\n",
    "            .master(\"local[*]\")\n",
    "            .config(\"spark.ui.port\", \"4222\")\n",
    "            .getOrCreate()\n",
    "    )\n",
    "\n",
    "    listJob = []\n",
    "    for row in getDf.itertuples():\n",
    "        listJob.append(row.JobName.lower())\n",
    "        df_output = loadTable(path=parquetOutput + '/' + row.JobName + '/part*', tableName=row.JobName)\n",
    "        \n",
    "    print(ListTable())\n",
    "    print(listJob)\n",
    "\n",
    "    if all(item in ListTable() for item in listJob):\n",
    "        df = sparkDqc.sql(\"\"\"\n",
    "                        SELECT COUNT(1) JML FROM PEOPLEPF\n",
    "                        \"\"\")\n",
    "        \n",
    "        df.repartition(1).write.parquet(testWrite)\n",
    "        print(df.show())\n",
    "    else:\n",
    "        sys.exit(1)\n",
    "\n",
    "    #sparkDqc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sparkDqc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msparkDqc\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sparkDqc' is not defined"
     ]
    }
   ],
   "source": [
    "sparkDqc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://74083a35e7ef:4222\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Locally</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2b7a39a930>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"Locally\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.ui.port\", \"4222\")\n",
    "        .config(\"spark.executor.instances\", \"2\")  # Initial number of executors (if dynamicAllocation is false)\n",
    "        .config(\"spark.executor.cores\", \"2\")  # Cores per executor\n",
    "        .config(\"spark.cores.max\", \"3\") # Maximum total cores for the application\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"true\") #Enable dynamic allocation\n",
    "        .config(\"spark.dynamicAllocation.minExecutors\", \"1\") #minimum executors\n",
    "        .config(\"spark.dynamicAllocation.maxExecutors\", \"5\") #maximum executors\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
