{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from random import random\n",
    "from operator import add\n",
    "from tabulate import tabulate\n",
    "import logging\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import threading\n",
    "import csv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, length, regexp_replace, lit\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def pandas_read_csv(file_path,**options):\n",
    "    \"\"\"\n",
    "        Read small volume of data only using read.csv\n",
    "        Args:\n",
    "            **Options ----> Any\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path,**options)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:  # Catch other potential exceptions (e.g., parsing errors)\n",
    "        print(f\"An error occurred while reading the CSV: {e}\")\n",
    "        return None\n",
    "\n",
    "def loadTable(**kwargs):\n",
    "    pathCheck = kwargs[\"path\"].replace(\"/part*\",\"\")\n",
    "    if not os.path.exists(pathCheck):\n",
    "        return None\n",
    "    try:\n",
    "        sparkDqc.sql(f\"\"\"\n",
    "        CREATE EXTERNAL TABLE IF NOT EXISTS {kwargs[\"tableName\"]}\n",
    "        USING PARQUET LOCATION '{kwargs[\"path\"]}'\n",
    "        \"\"\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "def ListTable():\n",
    "    listCreatedTable = []\n",
    "    tables = sparkDqc.sql(\"SHOW TABLES\").collect()\n",
    "    for table in tables:\n",
    "       listCreatedTable.append(table.tableName)\n",
    "    return listCreatedTable\n",
    "\n",
    "def getTables(**kwargs):\n",
    "    df = pandas_read_csv(kwargs[\"path\"], sep=\"|\")\n",
    "    df = df.query(f\"BatchName == '{batchname}' & Run == 1\")\n",
    "    print(tabulate(df.head(), headers='keys', tablefmt='pretty'))\n",
    "    df_refTable = df.query(\"RefTable.notnull()\")\n",
    "    distinct_reftable = df_refTable[[\"BatchName\",\"RefTable\"]].drop_duplicates().rename(columns={\"RefTable\": \"JobName\"})\n",
    "    df_job = pandas_read_csv(kwargs[\"LoadPath\"], sep=\"|\")\n",
    "    df_job = df_job[[\"BatchName\",\"JobName\"]].query(f\"BatchName == '{batchname}'\")\n",
    "    joined_df = pd.concat([df_job, distinct_reftable], ignore_index=True).drop_duplicates()\n",
    "    print(tabulate(joined_df.head(), headers='keys', tablefmt='pretty'))\n",
    "    return joined_df, df\n",
    "\n",
    "def executeScripts(sparkDqc, **kwargs):\n",
    "    try:\n",
    "        if kwargs[\"SP\"] == 1:\n",
    "            with open(kwargs[\"Scripts\"], 'r') as file:\n",
    "                sql_content = file.read()\n",
    "            sql_statements = sql_content.split(\";\")\n",
    "            sql_statements = [s.strip() for s in sql_statements if s.strip()]\n",
    "\n",
    "            for sql in sql_statements:\n",
    "                sql_upper = sql.upper()\n",
    "                if sql_upper.startswith(\"SELECT\"): #Check if the string STARTS with select.\n",
    "                    result_df = sparkDqc.sql(sql)\n",
    "                else:\n",
    "                    sparkDqc.sql(sql)\n",
    "        else:\n",
    "            result_df = sparkDqc.sql(kwargs[\"Scripts\"])\n",
    "\n",
    "        count = result_df.collect()[0]\n",
    "        kwargs[\"results\"].append((kwargs[\"BatchName\"], kwargs[\"DqcId\"], kwargs[\"Scripts\"], kwargs[\"Run\"], str(count[\"CNT\"])))\n",
    "    except Exception as e:\n",
    "        kwargs[\"results\"].append((kwargs[\"BatchName\"], kwargs[\"DqcId\"], kwargs[\"Scripts\"], kwargs[\"Run\"], f\"Error : {str(e)}\"))\n",
    "\n",
    "def executeScriptsParallel(sparkDqc, df):\n",
    "    results = []\n",
    "    threads = []\n",
    "\n",
    "    for row in df.itertuples(index=False):\n",
    "        kwargs = {\n",
    "            \"BatchName\": row.BatchName,\n",
    "            \"DqcId\": row.DqcId,\n",
    "            \"Scripts\": row.Scripts,\n",
    "            \"Run\": row.Run,\n",
    "            \"SP\": row.SP,\n",
    "            \"results\": results\n",
    "        }\n",
    "         \n",
    "        thread = threading.Thread(target=executeScripts, args=(sparkDqc,), kwargs=kwargs)\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    result_df = sparkDqc.createDataFrame(results, [\"BatchName\", \"DqcId\", \"Scripts\", \"Run\", \"Result\"])\n",
    "    return result_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"/mnt/apps/Files/Config/DataQuality_Config.csv\"\n",
    "    LoadPath = \"/mnt/apps/Files/Config/master_job.csv\"\n",
    "    parquetOutput = \"/mnt/apps/Files/data-movement/Parquet/\"\n",
    "    batchname = \"BATCH_ACT_VAL\"\n",
    "    testWrite = \"/mnt/apps/Files/Test\"\n",
    "\n",
    "    getDf, df = getTables(path=path, LoadPath=LoadPath)\n",
    "\n",
    "    sparkDqc = (\n",
    "        SparkSession\n",
    "            .builder\n",
    "            .appName(f\"{batchname}_DQC\")\n",
    "            .master(\"local[*]\")\n",
    "            .config(\"spark.ui.port\", \"4222\")\n",
    "            .getOrCreate()\n",
    "    )\n",
    "\n",
    "    listJob = []\n",
    "    for row in getDf.itertuples():\n",
    "        listJob.append(row.JobName.lower())\n",
    "        loadTable(path=parquetOutput + '/' + row.JobName + '/part*', tableName=row.JobName)\n",
    "        \n",
    "    print(ListTable())\n",
    "    print(listJob)\n",
    "    \n",
    "    if all(item in ListTable() for item in listJob):\n",
    "        result_df = executeScriptsParallel(sparkDqc, df)\n",
    "        print(result_df.show(truncate=False))\n",
    "    else:\n",
    "        sys.exit(1)\n",
    "\n",
    "    sparkDqc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"Locally\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.ui.port\", \"4222\")\n",
    "        .config(\"spark.executor.instances\", \"2\")  # Initial number of executors (if dynamicAllocation is false)\n",
    "        .config(\"spark.executor.cores\", \"2\")  # Cores per executor\n",
    "        .config(\"spark.cores.max\", \"3\") # Maximum total cores for the application\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"true\") #Enable dynamic allocation\n",
    "        .config(\"spark.dynamicAllocation.minExecutors\", \"1\") #minimum executors\n",
    "        .config(\"spark.dynamicAllocation.maxExecutors\", \"5\") #maximum executors\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "def dict_to_row(data_dict):\n",
    "    \"\"\"Converts a dictionary to a Spark Row.\"\"\"\n",
    "    return Row(**data_dict)\n",
    "\n",
    "def create_dataframe_from_dict(spark, data_dict):\n",
    "    \"\"\"Creates a Spark DataFrame from a dictionary.\"\"\"\n",
    "    row = dict_to_row(data_dict)\n",
    "    return spark.createDataFrame([row])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName(\"DictToDataFrame\").getOrCreate()\n",
    "\n",
    "    data = {\n",
    "        'JobName': 'RTRNPF',\n",
    "        'Path': '/mnt/apps/gcs/ETL4/RTRNPF/RTRNPF*.csv',\n",
    "        'dqID': 'DQ000001',\n",
    "        'CountRecords': -1,\n",
    "        'Message': ['Invalid Index values (containing only non-numeric characters). Total count: 5', \"Invalid Subscription Date values (not in '%Y-%m-%d' format). Total count: 1\"],\n",
    "        'Status': 'Failed'\n",
    "    }\n",
    "\n",
    "    df = create_dataframe_from_dict(spark, data)\n",
    "\n",
    "    # Explode the 'Message' array into rows\n",
    "    exploded_df = df.select(\"JobName\", \"Path\", \"dqID\", \"CountRecords\", explode(\"Message\").alias(\"Message\"), \"Status\")\n",
    "\n",
    "    exploded_df.show(truncate=False)\n",
    "    exploded_df.printSchema()\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Split = (\"CREATE TEMPORARY VIEW temp_jobs AS SELECT `Job Title`,COUNT(1) CNT FROM PEOPLEPF WHERE YEAR(`Date of birth`) IN ('1992','1993','1995','2021') GROUP BY `Job Title`; SELECT COUNT(1) CNT FROM temp_jobs\").split(\";\")\n",
    "print(test_Split)\n",
    "\n",
    "for scripts in test_Split:\n",
    "    scripts = scripts.strip()\n",
    "    print(scripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import csv\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CSVMultiLine\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame (replace with your actual data source)\n",
    "data = [(\"John\", \"Engineer\", \"1992-05-15\"), (\"Jane\", \"Manager\", \"1993-10-20\"), (\"Mike\", \"Engineer\", \"1995-03-01\"), (\"Sarah\", \"Analyst\", \"2021-12-05\")]\n",
    "columns = [\"Name\", \"Job Title\", \"Date of birth\"]\n",
    "people_df = spark.createDataFrame(data, columns)\n",
    "people_df.createOrReplaceTempView(\"PEOPLEPF\")\n",
    "\n",
    "def execute_sql_from_csv(csv_file_path):\n",
    "    with open(csv_file_path, 'r') as file:\n",
    "        sql_content = file.read()\n",
    "    sql_statements = sql_content.split(\";\")\n",
    "    sql_statements = [s.strip() for s in sql_statements if s.strip()]\n",
    "    for sql in sql_statements:\n",
    "        sql_upper = sql.upper()\n",
    "        if sql_upper.startswith(\"SELECT\"):\n",
    "            result_df = spark.sql(sql)\n",
    "            result_df.show()\n",
    "        else:\n",
    "            spark.sql(sql)\n",
    "\n",
    "csv_file_path = '/mnt/apps/Files/SP/USP_FCT.sql'\n",
    "\n",
    "execute_sql_from_csv(csv_file_path=csv_file_path)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
