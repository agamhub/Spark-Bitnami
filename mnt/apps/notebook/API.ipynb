{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/15 01:53:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/15 01:53:38 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 13, schema size: 12\n",
      "CSV file: file:///mnt/apps/Files/ETL4/RTRNPF/RTRNPF.csv\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+---------+-----------------------------+-----------------+---------------------------------------------------+----------------------+----------------------+-----------------------------+-----------------+--------------------------------+-------------------------------------------------------+\n",
      "|Index|Customer Id    |First Name|Last Name|Company                      |City             |Country                                            |Phone 1               |Phone 2               |Email                        |Subscription Date|Website                         |error_details                                          |\n",
      "+-----+---------------+----------+---------+-----------------------------+-----------------+---------------------------------------------------+----------------------+----------------------+-----------------------------+-----------------+--------------------------------+-------------------------------------------------------+\n",
      "|1    |4962fdbE6Bfee6D|Pam       |Sparks   |Patel-Deleon                 |Blakemouth       |British Indian Ocean Territory (Chagos Archipelago)|267-243-9490x035      |480-078-0535x889      |nicolas00@faulkner-kramer.com|2020-11-29       |https://nelson.com/             |Index: Invalid integer, Subscription Date: Invalid date|\n",
      "|2    |9b12Ae76fdBc9bE|Gina      |Rocha    |Acosta, Paul and Barber      |East Lynnchester |Costa Rica                                         |027.142.0940          |+1-752-593-4777x07171 |yfarley@morgan.com           |2021-01-03       |https://pineda-rogers.biz/      |Index: Invalid integer, Subscription Date: Invalid date|\n",
      "|3    |39edFd2F60C85BC|Kristie   |Greer    |Ochoa PLC                    |West Pamela      |Ecuador                                            |+1-049-168-7497x5053  |+1-311-216-7855       |jennyhayden@petty.org        |2021-06-20       |https://mckinney.com/           |Index: Invalid integer, Subscription Date: Invalid date|\n",
      "|4    |Fa42AE6a9aD39cE|Arthur    |Fields   |Moyer-Wang                   |East Belinda     |Afghanistan                                        |001-653-754-7486x65787|521-630-3858x953      |igrimes@ruiz-todd.org        |2020-02-13       |https://dominguez.biz/          |Index: Invalid integer, Subscription Date: Invalid date|\n",
      "|5    |F5702Edae925F1D|Michelle  |Blevins  |Shah and Sons                |West Jared       |Marshall Islands                                   |8735278329            |(633)283-6034x500     |diamondcarter@jordan.com     |2020-10-20       |http://murillo-ryan.com/        |Index: Invalid integer, Subscription Date: Invalid date|\n",
      "|6    |9FBfbd34c3f30d9|Greg      |Rivers   |Hendrix, Arias and Holmes    |New Pedrohaven   |Senegal                                            |2382697028            |822-449-7638          |mannkiara@bowman.com         |2020-02-19       |https://www.richard.biz/        |Index: Invalid integer, Subscription Date: Invalid date|\n",
      "|8    |3C3B8bee16BcC5F|Dennis    |Pacheco  |Contreras-Brock              |South Eugenehaven|Sri Lanka                                          |+1-391-505-2358       |076.454.1128          |joy45@conner.org             |2021-03-20       |http://www.armstrong-golden.com/|Index: Invalid integer, Subscription Date: Invalid date|\n",
      "|9    |7C1C93522Bd5e4B|Malik     |Rivers   |Best, Washington and Jacobson|Mejiastad        |Wallis and Futuna                                  |001-366-285-9819x5924 |852.896.9192x3681     |stacydavis@spencer.com       |2020-01-16       |https://watson.com/             |Index: Invalid integer, Subscription Date: Invalid date|\n",
      "|10   |bABC8cBffe68792|Hayden    |Riddle   |Rogers-Dean                  |Kaitlynmouth     |Cameroon                                           |951.359.7028x94651    |931.227.4541          |danhouston@moon.net          |2022-05-18       |https://www.farley.net/         |Index: Invalid integer, Subscription Date: Invalid date|\n",
      "|11   |534a2B5FE8Aba5d|Edward    |Wood     |Mckee, Schneider and Orozco  |Kelseystad       |Croatia                                            |(932)069-3090x0566    |037-375-9588x561      |batesbill@jensen.net         |2020-05-22       |http://alvarez.com              |Index: Invalid integer, Subscription Date: Invalid date|\n",
      "|12   |4baa94d4FaEbe9d|Julie     |Stein    |Fry Group                    |South Frankside  |Macao                                              |+1-918-017-3392x459   |500-005-1315          |sherry62@robinson.org        |2020-01-17       |https://carlson-johnston.com/   |Index: Invalid integer, Subscription Date: Invalid date|\n",
      "|13   |Fc2c8D2BE1AEfDb|Kristina  |Andrade  |Mann Ltd                     |Port Taraton     |Pitcairn Islands                                   |(640)067-7023x66846   |001-367-405-8096x592  |ivillarreal@fowler.biz       |2020-09-11       |https://foley.com/              |Index: Invalid integer, Subscription Date: Invalid date|\n",
      "|14   |9468BBc926AaAB3|Zoe       |Hansen   |Tanner PLC                   |Kimberlyfort     |Benin                                              |638-798-9796x0247     |(265)475-2386x9812    |duransheena@hughes.com       |2021-10-09       |https://franco-galloway.com/    |Index: Invalid integer, Subscription Date: Invalid date|\n",
      "|15   |BE96AcDDCc77da0|Joseph    |Bennett  |Stafford Group               |Port Mathew      |Peru                                               |+1-597-363-0416x6579  |900.196.1169x767      |albertparsons@shea.com       |2020-04-29       |https://howard-boyer.net/       |Index: Invalid integer, Subscription Date: Invalid date|\n",
      "|16   |A24E162EBb6D97B|Vickie    |Ferguson |Ward-Ferrell                 |Carlyport        |Tunisia                                            |(351)349-8170         |+1-943-399-6212x1474  |rita82@carey.com             |2021-04-16       |https://ashley-oneill.info/     |Index: Invalid integer, Subscription Date: Invalid date|\n",
      "|17   |f1a979d0Ed2dB14|Dillon    |Frye     |Short-Novak                  |Lake Kendraview  |Colombia                                           |+1-003-466-7005       |001-253-461-6631x21150|housegreg@dillon.com         |2021-11-29       |http://www.young.com/           |Index: Invalid integer, Subscription Date: Invalid date|\n",
      "|18   |dCBC761B59e4EFB|Grace     |Logan    |Ellis-Williamson             |Rochaborough     |Kenya                                              |1744624651            |166-747-2389x97375    |jodyeverett@potter.com       |2020-02-11       |https://www.knapp.net/          |Index: Invalid integer, Subscription Date: Invalid date|\n",
      "|19   |d0e6bb354BAeEEd|Sara      |Dudley   |Coffey Ltd                   |Lake Davestad    |Guernsey                                           |921.993.4487          |(812)799-2828x896     |beth75@noble.com             |2021-10-16       |https://padilla.info/           |Index: Invalid integer, Subscription Date: Invalid date|\n",
      "+-----+---------------+----------+---------+-----------------------------+-----------------+---------------------------------------------------+----------------------+----------------------+-----------------------------+-----------------+--------------------------------+-------------------------------------------------------+\n",
      "\n",
      "+-------------------------------------------------------+-----+\n",
      "|error_details                                          |total|\n",
      "+-------------------------------------------------------+-----+\n",
      "|Index: Invalid integer, Subscription Date: Invalid date|18   |\n",
      "+-------------------------------------------------------+-----+\n",
      "\n",
      "[{'JobName': 'RTRNPF', 'Path': '/mnt/apps/gcs/ETL4/RTRNPF', 'dqID': 'DQ000001', 'CountRecords': 18, 'Message': 'Index: Invalid integer, Subscription Date: Invalid date', 'Status': 'Failed'}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, length, regexp_replace, lit, udf, count, trim\n",
    "from datetime import datetime\n",
    "\n",
    "def spark_read_csv_from_os(spark, file_path, schema, **kwargs):\n",
    "    \"\"\"\n",
    "    Reads a CSV file from the operating system into a Spark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        spark: The SparkSession object.\n",
    "        file_path: The path to the CSV file.  Can be a local path or a path\n",
    "                   that your Spark environment can access (e.g., if you're\n",
    "                   using a distributed file system like HDFS).\n",
    "        header (bool, optional): Whether the CSV file has a header row. Defaults to True.\n",
    "        inferSchema (bool, optional): Whether to infer the schema from the data. Defaults to True.\n",
    "        **options: Additional options to pass to the Spark CSV reader.  See\n",
    "                   the Spark documentation for available options like `delimiter`,\n",
    "                   `quote`, `escape`, etc.\n",
    "\n",
    "    Returns:\n",
    "        A Spark DataFrame representing the CSV data, or None if there's an error.\n",
    "\n",
    "    Raises:\n",
    "       FileNotFoundError: If the file path doesn't exist.\n",
    "    \"\"\"\n",
    "    base_options = {\n",
    "        \"inferSchema\": \"False\",\n",
    "        \"header\": \"True\",\n",
    "        \"quote\": '\"',\n",
    "        \"columnNameOfCorruptRecord\": \"rejected_records\",\n",
    "        \"mode\": \"PERMISSIVE\"\n",
    "    }\n",
    "    base_options.update(kwargs)\n",
    "\n",
    "    try:\n",
    "        schema = StructType(schema.fields + [StructField(\"rejected_records\", StringType(), True)])\n",
    "        df = spark.read.options(**base_options).schema(schema).csv(file_path)\n",
    "    \n",
    "        def parse_and_identify_errors(rejected_str, delimiter=\",\"):\n",
    "            if rejected_str:\n",
    "                try:\n",
    "                    parts = rejected_str.split(delimiter)\n",
    "                    error_columns = []\n",
    "                    error_reasons = []\n",
    "\n",
    "                    for i, field in enumerate(schema.fields):\n",
    "                        try:\n",
    "                            if field.dataType == IntegerType():\n",
    "                                int(parts[i])\n",
    "                            elif field.dataType == DateType():\n",
    "                                datetime.strptime(parts[i], \"%Y-%m-%d\").date() #change date format\n",
    "                            # Add more data type checks as needed\n",
    "                        except (ValueError, IndexError):\n",
    "                            error_columns.append(field.name)\n",
    "                            error_reasons.append(f\"Invalid {field.dataType.typeName()}\")\n",
    "\n",
    "                    if error_columns:\n",
    "                        error_detail_string = \", \".join([f\"{col}: {reason}\" for col, reason in zip(error_columns, error_reasons)])\n",
    "                        #print(f\"Rejected String: {rejected_str}, Error Details: {error_detail_string}\") #debugging print\n",
    "                        return error_detail_string\n",
    "                    else:\n",
    "                        print(f\"Error parsing record: {e}, No errors found\") #debugging print\n",
    "                        return None\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing record: {e}, Rejected String: {rejected_str}\") #debugging print\n",
    "                    return f\"Error parsing record: {e}\"\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        parse_errors_udf = udf(parse_and_identify_errors, StringType())\n",
    "\n",
    "        rejected_df = df.filter(col(\"rejected_records\").isNotNull()).withColumn(\n",
    "            \"error_details\", parse_errors_udf(col(\"rejected_records\"))\n",
    "        )\n",
    "\n",
    "        df = df.drop(\"rejected_records\")\n",
    "        rejected_df = rejected_df.drop(\"rejected_records\")\n",
    "\n",
    "        return df, rejected_df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:  # Catch other potential exceptions (e.g., parsing errors)\n",
    "        print(f\"An error occurred while reading the CSV: {e}\")\n",
    "        return None\n",
    "    \n",
    "def pandas_read_csv(file_path,**options):\n",
    "    \"\"\"\n",
    "        Read small volume of data only using read.csv\n",
    "        Args:\n",
    "            **Options ----> Any\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path,**options)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:  # Catch other potential exceptions (e.g., parsing errors)\n",
    "        print(f\"An error occurred while reading the CSV: {e}\")\n",
    "        return None\n",
    "    \n",
    "def construct_sql_schema(**kwargs):\n",
    "    \"\"\"\n",
    "        Args: kwargs path and sep -->>> Any\n",
    "        this function is best practice to compute large amount of data to not reading schema metadata\n",
    "        recommendation : \n",
    "    \"\"\"\n",
    "\n",
    "    fields = []\n",
    "    type_mapping = {\n",
    "        \"varchar\": StringType(),\n",
    "        \"nvarchar\": StringType(),\n",
    "        \"int\": IntegerType(),\n",
    "        \"bigint\": LongType(),\n",
    "        \"date\": DateType(),\n",
    "        \"decimal\": DecimalType\n",
    "    }\n",
    "\n",
    "    df = pandas_read_csv(kwargs[\"path\"],sep=kwargs[\"sep\"])\n",
    "    #print(df)\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        try:\n",
    "            name, data_type_str = row.DataType.split(\"(\", 1) if \"(\" in row.DataType else (row.DataType,\"\")\n",
    "            name = name.strip()\n",
    "            data_type_str = data_type_str[:-1].strip()\n",
    "            parts = data_type_str.split(\",\")\n",
    "            name_lower = name.lower()\n",
    "\n",
    "            for keyword,spark_type in type_mapping.items():\n",
    "                if keyword in name_lower:\n",
    "                    if spark_type == DecimalType:\n",
    "                        data_type = DecimalType() if not data_type_str else DecimalType(int(parts[0]),int(parts[1]))\n",
    "                        fields.append(StructField(row.ColumnName, data_type, True))\n",
    "                    else:\n",
    "                        data_type = spark_type\n",
    "                        fields.append(StructField(row.ColumnName, data_type, True))\n",
    "                    break\n",
    "        except Exception as e:  # Catch other potential errors\n",
    "            print(f\"Error processing file in construct schema {kwargs[\"path\"]}: {e}\")\n",
    "            return None\n",
    "    return StructType(fields)\n",
    "\n",
    "def validateDecimal(**kwargs):\n",
    "    df_contents = kwargs[\"df_contents\"]\n",
    "    is_valid = False\n",
    "    errors = []\n",
    "    dqcId = \"DQ000001\"\n",
    "    for field in kwargs[\"dtypes\"]:\n",
    "        colName = field.name\n",
    "        dType = str(field.dataType)\n",
    "        if \"decimal\" in dType.lower() or \"int\" in dType.lower():\n",
    "            #print(colName)\n",
    "            df_cleaned = df_contents.withColumn(\n",
    "                f\"{colName}_cleaned\",\n",
    "                regexp_replace(col(colName), \"[^0-9.]\", \"\")\n",
    "            )\n",
    "            df_empty = df_cleaned.filter((col(f\"{colName}_cleaned\")).isNull()) # is null due to schema defined as decimal if we use inferSchema its a string\n",
    "            #print(\"test disini\")\n",
    "            #df_empty.show()\n",
    "            empty_count = df_empty.count()\n",
    "\n",
    "            if empty_count > 0:\n",
    "                is_valid = True\n",
    "                error_msg = (f\"Invalid {colName} values (containing only non-numeric characters). Total count: {empty_count}\")\n",
    "                errors.append(error_msg)\n",
    "            df_contents = df_contents.drop(f\"{colName}_cleaned\") \n",
    "            \n",
    "    if is_valid == False:        \n",
    "        error_msg = \"DDL Decimal/Int Data Type Structure Checks Passed.\"\n",
    "        errors.append(error_msg)\n",
    "    msg = \"\\n\".join(errors) if errors else \"Data Quality Checks Passed.\" # this is for breakdown into rows from array\n",
    "    return is_valid, errors, df_contents, dqcId\n",
    "\n",
    "def writeOptions(df, dataMovement, **kwargs):\n",
    "    base_options = {  # Keep this separate\n",
    "        \"header\": \"true\",\n",
    "        \"delimiter\": \"|\",\n",
    "        \"quote\": '\"'\n",
    "    }\n",
    "    base_options.update(kwargs)\n",
    "    df.coalesce(1).write.format(\"csv\").mode(\"overwrite\").options(**base_options).save(dataMovement)\n",
    "\n",
    "def writeToParquet(df, path):\n",
    "    df = df.coalesce(50)\n",
    "    df.write.parquet(path, mode=\"overwrite\", compression=\"snappy\")\n",
    "\n",
    "def loadTable(**kwargs):\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS spark_catalog.default.{kwargs['tableName']}\") \n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE EXTERNAL TABLE {kwargs[\"tableName\"]}\n",
    "    USING CSV\n",
    "    OPTIONS (\n",
    "        header 'true',  -- If your CSV has a header row\n",
    "        inferSchema 'false', -- Important: Set to false since we provide schema\n",
    "        delimiter '|' -- Specify the delimiter if it's not a comma\n",
    "    )\n",
    "        LOCATION '{kwargs[\"path\"]}'\n",
    "    \"\"\")\n",
    "    df = spark.sql(f\"SELECT * FROM {kwargs[\"tableName\"]}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"/mnt/apps/Files/Config/master_job.csv\"\n",
    "    pathSchema = \"/mnt/apps/Files/Schema/\"\n",
    "    outputFile = \"/mnt/apps/Files/data-movement/\"\n",
    "    parquetOutput = \"/mnt/apps/Files/data-movement/Parquet/\"\n",
    "    dqcOutput = []\n",
    "    dqcId = \"DQ000001\"\n",
    "\n",
    "    spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        appName(\"Testing\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.ui.port\", \"4222\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    df = pandas_read_csv(path,sep=\"|\")\n",
    "    df = df.query(f\"JobName == 'RTRNPF'\")\n",
    "    #print(df)\n",
    "\n",
    "    for row in df.itertuples():  # Collects all data to the driver - NOT recommended for large datasets\n",
    "        filePath = row.SourceDirectory + '/' + row.FileName + '*.' + row.FileType\n",
    "        filePath = filePath.replace(\"/gcs\", \"/Files\")\n",
    "        dataMovement = outputFile + row.JobName + '/'\n",
    "        dataMovementParquet = parquetOutput + row.JobName\n",
    "        #print(row.JobName)\n",
    "        FullPathSchema = pathSchema + row.FileName + '.' + row.FileType\n",
    "        #print(FullPathSchema)\n",
    "        #spark.stop()\n",
    "        df_dtype = construct_sql_schema(path=FullPathSchema, sep=\"|\")\n",
    "        #print(df_dtype)\n",
    "        df, rejected_df = spark_read_csv_from_os(spark, filePath, schema=df_dtype, sep=row.Delimiter)\n",
    "        rejected_df.cache()\n",
    "        rejected_df.take(1)\n",
    "        is_empty = True if rejected_df.count() > 0 else False\n",
    "        if is_empty:\n",
    "            # need to catch malforme\n",
    "            # Group by error_details \n",
    "            rejected_df.show(truncate=False)\n",
    "            rejCnt = rejected_df.groupBy(\"error_details\").agg(count(\"*\").alias(\"total\"))\n",
    "            rejCnt.show(truncate=False)\n",
    "            collected_rejCnt = rejCnt.collect()\n",
    "            if collected_rejCnt:\n",
    "                err_msg = collected_rejCnt[0]\n",
    "                dqcOutput.append({\"JobName\":row.JobName, \"Path\":row.SourceDirectory, \"dqID\":dqcId, \"CountRecords\":err_msg.total, \"Message\":err_msg.error_details, \"Status\":\"Failed\"})\n",
    "            else:\n",
    "                print(\"No rejected errors found.\")\n",
    "            rejected_df.unpersist()\n",
    "        else:\n",
    "            df_count = df.count()\n",
    "            #writeOptions(df, dataMovement)\n",
    "            dqcOutput.append({\"JobName\":row.JobName, \"Path\":row.SourceDirectory, \"dqID\":dqcId, \"CountRecords\":df_count, \"Message\":\"DDL Data Quality Check Passed !!!!\", \"Status\":\"Successful\"})\n",
    "        rejected_df.unpersist()\n",
    "    print(dqcOutput)\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /opt/bitnami/python/lib/python3.12/site-packages/pip-23.3.2-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Package(s) not found: py4j\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "#help(spark.read.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Index: int, Customer Id: string, First Name: string, Last Name: string, Company: string, City: string, Country: string, Phone 1: string, Phone 2: string, Email: string, Subscription Date: date, Website: string, error_details: string]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rejected_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                        (0 + 16) / 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(error_details='invalid', total=6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark1 = SparkSession.builder.appName(\"CreateDataFrameExample\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"invalid\",6)\n",
    "]\n",
    "\n",
    "schema = [\"error_details\", \"total\"]\n",
    "\n",
    "df = spark1.createDataFrame(data, schema=schema)\n",
    "\n",
    "a = df.collect()[0]\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/14 15:19:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------+-----+\n",
      "|error_details                                          |count|\n",
      "+-------------------------------------------------------+-----+\n",
      "|Index: Invalid integer, Subscription Date: Invalid date|6    |\n",
      "+-------------------------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"GroupByErrorDetails\").getOrCreate()\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"Index\", StringType(), True),\n",
    "    StructField(\"Customer Id\", StringType(), True),\n",
    "    StructField(\"First Name\", StringType(), True),\n",
    "    StructField(\"Last Name\", StringType(), True),\n",
    "    StructField(\"Company\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "    StructField(\"Phone 1\", StringType(), True),\n",
    "    StructField(\"Phone 2\", StringType(), True),\n",
    "    StructField(\"Email\", StringType(), True),\n",
    "    StructField(\"Subscription Date\", StringType(), True),\n",
    "    StructField(\"Website\", StringType(), True),\n",
    "    StructField(\"error_details\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Sample data (replace with your actual data source)\n",
    "data = [\n",
    "    (None, \"4962fdbE6Bfee6D\", \"Pam\", \"Sparks\", \"Patel-Deleon\", \"Blakemouth\", \"British Indian Ocean Territory (Chagos Archipelago)\", \"267-243-9490x035\", \"480-078-0535x889\", \"nicolas00@faulkner-kramer.com\", \"2020-11-29\", \"https://nelson.com/\", \"Index: Invalid integer, Subscription Date: Invalid date\"),\n",
    "    (\"2\", \"9b12Ae76fdBc9bE\", \"Gina\", \"Rocha\", \"Acosta, Paul and Barber\", \"East Lynnchester\", \"Costa Rica\", \"027.142.0940\", \"+1-752-593-4777x07171\", \"yfarley@morgan.com\", None, \"https://pineda-rogers.biz/\", \"Index: Invalid integer, Subscription Date: Invalid date\"),\n",
    "    (None, \"Fc2c8D2BE1AEfDb\", \"Kristina\", \"Andrade\", \"Mann Ltd\", \"Port Taraton\", \"Pitcairn Islands\", \"(640)067-7023x66846\", \"001-367-405-8096x592\", \"ivillarreal@fowler.biz\", \"2020-09-11\", \"https://foley.com/\", \"Index: Invalid integer, Subscription Date: Invalid date\"),\n",
    "    (None, \"9468BBc926AaAB3\", \"Zoe\", \"Hansen\", \"Tanner PLC\", \"Kimberlyfort\", \"Benin\", \"638-798-9796x0247\", \"(265)475-2386x9812\", \"duransheena@hughes.com\", \"2021-10-09\", \"https://franco-galloway.com/\", \"Index: Invalid integer, Subscription Date: Invalid date\"),\n",
    "    (None, \"A1505BF376CC5Ed\", \"Aimee\", \"Brooks\", \"Walker Ltd\", \"Mitchellview\", \"Malaysia\", \"112.920.9961x77753\", \"471.896.6847x82788\", \"chambersdanielle@good-cannon.com\", \"2022-03-28\", \"http://solis.org/\", \"Index: Invalid integer, Subscription Date: Invalid date\"),\n",
    "    (None, \"a24eB840950dac7\", \"Mackenzie\", \"Leonard\", \"Abbott Inc\", \"Bauerfort\", \"Ukraine\", \"+1-010-716-9313x74577\", \"315.423.2995\", \"bwheeler@hickman-acevedo.com\", \"2022-05-02\", \"http://www.pacheco.net/\", \"Index: Invalid integer, Subscription Date: Invalid date\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Group by 'error_details' and count occurrences\n",
    "result_df = df.groupBy(\"error_details\").agg(F.count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Show the result\n",
    "result_df.show(truncate=False)\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       BatchName JobName            SourceDirectory FileName FileType  Flag  \\\n",
      "1  BATCH_ACT_VAL  ACMVPF  /mnt/apps/gcs/ETL4/ACMVPF   ACMVPF      csv     1   \n",
      "2  BATCH_ACT_VAL  RTRNPF  /mnt/apps/gcs/ETL4/RTRNPF   RTRNPF      csv     1   \n",
      "\n",
      "  Delimiter  \n",
      "1         |  \n",
      "2         |  \n",
      "['/mnt/apps/Files/ETL4/ACMVPF/ACMVPF.csv', '/mnt/apps/Files/ETL4/RTRNPF/RTRNPF.csv']\n"
     ]
    }
   ],
   "source": [
    "path = \"/mnt/apps/Files/Config/master_job.csv\"\n",
    "df = pandas_read_csv(path,sep=\"|\")\n",
    "df = df.query(f\"BatchName == 'BATCH_ACT_VAL'\")\n",
    "print(df)\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        appName(\"Thread\"). \\\n",
    "        getOrCreate()\n",
    "\n",
    "tables = []\n",
    "for row in df.itertuples():\n",
    "    filePath = row.SourceDirectory + '/' + row.FileName + '.' + row.FileType\n",
    "    filePath = filePath.replace(\"/gcs\", \"/Files\")\n",
    "    tables.append(filePath)\n",
    "\n",
    "def loadTable(path):\n",
    "    sc = path.split(\"/\")[5]\n",
    "    pathParquet = f\"/mnt/apps/Files/data-movement/Parquet/{sc}\"\n",
    "    print(pathParquet)\n",
    "    df_dtype = construct_sql_schema(path=FullPathSchema, sep=\"|\")\n",
    "    df = spark.read.csv(path, header=True, inferSchema=False, schema=df_dtype, sep=\"|\")\n",
    "    result, dqc_msg, df_final, dqcId = validateDecimal(dtypes=df_dtype, df_contents=df)\n",
    "    df_count = df_final.count()\n",
    "    if result:\n",
    "        print(dqc_msg)\n",
    "        dqcOutput.append({\"JobName\":row.JobName, \"Path\":row.SourceDirectory, \"dqID\":dqcId, \"CountRecords\":df_count, \"Message\":dqc_msg, \"Status\":\"Failed\"})\n",
    "    else:\n",
    "        writeToParquet(df_final, pathParquet)\n",
    "        print(dqc_msg)\n",
    "        dqcOutput.append({\"JobName\":row.JobName, \"Path\":row.SourceDirectory, \"dqID\":dqcId, \"CountRecords\":df_count, \"Message\":dqc_msg, \"Status\":\"Successful\"})\n",
    "    \n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from queue import Queue\n",
    "\n",
    "q = Queue()\n",
    "\n",
    "workerCount = 2\n",
    "\n",
    "def run_task(function, q):\n",
    "    while not q.empty():\n",
    "        value = q.get()\n",
    "        function(value)\n",
    "        q.task_done()\n",
    "\n",
    "for table in tables:\n",
    "    q.put(table)\n",
    "\n",
    "for i in range(workerCount):\n",
    "    t=Thread(target=run_task, args=(loadTable, q))\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "\n",
    "print(\"running load\")\n",
    "q.join()\n",
    "spark.stop()\n",
    "print(\"running completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name\n",
    "import dask.dataframe as dd\n",
    "\n",
    "#outputFile = \"/mnt/apps/Files/ETL4/LAS/customers.csv\"\n",
    "outputFile = \"/mnt/apps/Files/data-movement/Renova/part-00000*\"\n",
    "FullPathSchema = \"/mnt/apps/Files/Schema/customers.csv\"\n",
    "ParquetPath = \"/mnt/apps/Files/data-movement/Parquet/Renova/part-000*\"\n",
    "PdoutputFile = \"/mnt/apps/Files/ETL4/LAS/customers.csv\"\n",
    "\n",
    "#df_dtype = construct_sql_schema(path=FullPathSchema, sep=\"|\")\n",
    "#print(df_dtype)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TestReadCSV\").getOrCreate()\n",
    "\n",
    "#CSV reader\n",
    "#df = spark.read.csv(outputFile, sep=\"|\", header=True, schema=df_dtype, inferSchema=False).repartition(10)\n",
    "\n",
    "# df_with_filename = df.withColumn(\"filename\", input_file_name())\n",
    "# null_count = df_with_filename.filter(col(\"Index\").isNotNull())\n",
    "# df_count = null_count.count()\n",
    "# print(df_count)\n",
    "# #null_count.limit(10).show(truncate=False)\n",
    "# null_count.createOrReplaceTempView(\"readCSV\")\n",
    "# #df_parquet_count = null_count_parquet.count()\n",
    "# #print(df_parquet_count)\n",
    "# null_count = spark.sql(\n",
    "#     \"\"\"\n",
    "#     SELECT country, cnt, sum(cnt) over () total_all\n",
    "#     FROM (\n",
    "#     SELECT Country, COUNT(1) AS cnt\n",
    "#         FROM readCSV\n",
    "#         GROUP BY Country\n",
    "#     ) Z\n",
    "# \"\"\")\n",
    "# print(null_count.count())\n",
    "#df.show(n=5, truncate=False) #default 20\n",
    "\n",
    "#parquet part\n",
    "# df_read_parquet = spark.read.parquet(outputFile, schema=df_dtype)\n",
    "# null_count_parquet = df_read_parquet.filter(col(\"Index\").isNotNull())\n",
    "# null_count_parquet.createOrReplaceTempView(\"my_parquet_table\")\n",
    "# #df_parquet_count = null_count_parquet.count()\n",
    "# #print(df_parquet_count)\n",
    "# null_count_parquet = spark.sql(\n",
    "#     \"\"\"\n",
    "#     SELECT country, cnt, sum(cnt) over () total_all\n",
    "#     FROM (\n",
    "#     SELECT Country, COUNT(1) AS cnt\n",
    "#         FROM my_parquet_table\n",
    "#         GROUP BY Country\n",
    "#     ) Z\n",
    "# \"\"\")\n",
    "# print(null_count_parquet.count())\n",
    "# null_count_parquet.show(n=5, truncate=False) #default 20\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of = []\n",
    "for i in range(240):\n",
    "    list_of.append(f\"Cloned_{i + 1}|Varchar(100)\")\n",
    "   \n",
    "df = pd.DataFrame(list_of, columns=[\"ColumnName\"])\n",
    "df.head(n=250)\n",
    "\n",
    "df.to_csv(\"/mnt/apps/Files/NewSChema/new_schema_cust.csv\",header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/mnt/apps/Files/Config/master_job.csv\"\n",
    "pathSchema = \"/mnt/apps/Files/Schema/\"\n",
    "\n",
    "df = pandas_read_csv(path,sep=\"|\")\n",
    "df = df.query(\"JobName == 'PAT'\")\n",
    "\n",
    "for row in df.itertuples():  # Collects all data to the driver - NOT recommended for large datasets\n",
    "    filePath = row.SourceDirectory + '/' + row.FileName + '.' + row.FileType\n",
    "    FullPathSchema = pathSchema + row.FileName + '.' + row.FileType\n",
    "    spark = SparkSession.builder.appName(f\"{row.JobName}\").getOrCreate()\n",
    "    df_dtype = construct_sql_schema(path=FullPathSchema, sep=\"|\")\n",
    "    df = spark_read_csv_from_os(spark, filePath, schema=df_dtype, quote='\"', sep=\"|\")\n",
    "    # Handle the error, e.g., skip the file, log the error, etc.\n",
    "    df.show()\n",
    "\"\"\"     result, dqc_msg, df_final = validateDecimal(dtypes=df_dtype, df_contents=df)\n",
    "    df_final.show()\n",
    "    if result:\n",
    "        print(dqc_msg)\n",
    "    else:\n",
    "        print(dqc_msg) \"\"\"\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index             int64\n",
      "User Id          object\n",
      "First Name       object\n",
      "Last Name        object\n",
      "Sex              object\n",
      "Email            object\n",
      "Phone            object\n",
      "Date of birth    object\n",
      "Job Title        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "path = \"/mnt/apps/Files/ETL4/PEOPLEPF/people.csv\"\n",
    "\n",
    "df = pandas_read_csv(path, sep=\",\")\n",
    "#df.head(1)\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "path = \"/mnt/apps/Files/Schema/etl4pat.csv\"\n",
    "fields = []\n",
    "type_mapping = {\n",
    "    \"varchar\": VarcharType,\n",
    "    \"nvarchar\": VarcharType,\n",
    "    \"int\": IntegerType(),\n",
    "    \"bigint\": LongType(),\n",
    "    \"date\": DateType(),\n",
    "    \"decimal\": DecimalType\n",
    "}\n",
    "\n",
    "df = pandas_read_csv(path,sep=\"|\")\n",
    "print(df)\n",
    "\n",
    "for row in df.itertuples():\n",
    "    name, data_type_str = row.DataType.split(\"(\", 1) if \"(\" in row.DataType else (row.DataType,\"\")\n",
    "    name = name.strip()\n",
    "    data_type_str = data_type_str[:-1].strip()\n",
    "    parts = data_type_str.split(\",\")\n",
    "    name_lower = name.lower()\n",
    "    print(data_type_str)\n",
    "\n",
    "    for keyword,spark_type in type_mapping.items():\n",
    "        if keyword in name_lower:\n",
    "            if spark_type == VarcharType:\n",
    "                data_type = VarcharType(4000) if data_type_str == \"MAX\" else VarcharType(int(data_type_str))\n",
    "                fields.append(StructField(row.ColumnName, data_type, True))\n",
    "            elif spark_type == DecimalType:\n",
    "                data_type = DecimalType() if not data_type_str else DecimalType(int(parts[0]),int(parts[1]))\n",
    "                fields.append(StructField(row.ColumnName, data_type, True))\n",
    "            else:\n",
    "                data_type = spark_type\n",
    "                fields.append(StructField(row.ColumnName, data_type, True))\n",
    "            break\n",
    "\n",
    "print(fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type_str = \"Decimal(2,0)\"\n",
    "data_type, *args = data_type_str.split(\")\")\n",
    "\n",
    "print(f\"data_type: {data_type}\")  # Output: data_type: Decimal(2,0\n",
    "print(f\"args: {args}\")          # Output: args: ['']\n",
    "\n",
    "data_type_str = \"VARCHAR\"\n",
    "data_type, *args = data_type_str.split(\")\")\n",
    "\n",
    "print(f\"data_type: {data_type}\")  # Output: data_type: VARCHAR\n",
    "print(f\"args: {args}\")          # Output: args: []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"A,2\"\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/mnt/apps/Files/Schema/etl4pat.csv\"\n",
    "test = construct_sql_schema(path=path,sep=\"|\")\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *  # Import data types for clarity\n",
    "\n",
    "# Create a SparkSession (if you don't have one already)\n",
    "spark = SparkSession.builder.appName(\"DataTypeExample\").getOrCreate()\n",
    "\n",
    "# Sample data (replace with your actual data)\n",
    "data = [(\"Alice\", 25, 2000.00), (\"Bob\", 30, 2000.00), (\"Charlie\", 22, 2000.00)]\n",
    "\n",
    "# Define the schema explicitly (best practice)\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"height\", DecimalType(2,0), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Display the schema\n",
    "df.printSchema()\n",
    "\n",
    "# Stop the SparkSession (good practice)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"/mnt/apps/gcs/ETL4/CONFIG/etl4pat*.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def writeOptions(df, path, **kwargs):\n",
    "    base_options = {  # Keep this separate\n",
    "        \"header\": \"true\",\n",
    "        \"delimiter\": \"|\",\n",
    "        \"quote\": '\"',\n",
    "        \"mode\":\"overwrite\",\n",
    "        \"format\":\"csv\"\n",
    "    }\n",
    "\n",
    "    # Correct way to merge options:\n",
    "    all_options = base_options.copy()  # Create a copy to avoid modifying base_options\n",
    "    all_options.update(kwargs)       # Add or overwrite kwargs\n",
    "    print(all_options)\n",
    "\n",
    "    df.write.options(**all_options).save(path)\n",
    "\n",
    "# Example usage (important: complete example):\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
    "\n",
    "dataMovement = \"path/to/save.csv\" # Or your actual path\n",
    "\n",
    "writeOptions(df, dataMovement)  # Now works correctly\n",
    "\n",
    "spark.stop()  # Don't forget to stop the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "path = \"/mnt/apps/Files/data-movement/ACMVPF/part*.csv.gz\"\n",
    "\n",
    "spark = SparkSession.builder.appName(\"READCSVGZ\").getOrCreate()\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "            CREATE EXTERNAL TABLE IF NOT EXISTS ACMVPF\n",
    "            USING CSV \n",
    "            OPTIONS (\n",
    "                    path '{path}',\n",
    "                    delimiter '|',\n",
    "                    header 'true',\n",
    "                    compression 'gzip'\n",
    "            )\n",
    "            \"\"\")\n",
    "\n",
    "spark.sql(\"DESCRIBE EXTENDED ACMVPF\").show(truncate=False)\n",
    "spark.sql(\"SELECT COUNT(*) FROM ACMVPF\").show()\n",
    "spark.sql(\"SELECT * FROM ACMVPF LIMIT 10\").show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /opt/bitnami/python/lib/python3.12/site-packages/pip-23.3.2-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mName: pyspark\n",
      "Version: 3.5.4\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: /opt/bitnami/spark/python\n",
      "Requires: py4j\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /opt/bitnami/python/lib/python3.12/site-packages/pip-23.3.2-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Package(s) not found: py4j\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
