{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+-------+\n",
      "|                 COA|AMT_RPT|         Description|AMT_ORG|\n",
      "+--------------------+-------+--------------------+-------+\n",
      "|11111111111111111...|2000.32|Does this type ne...| 123.00|\n",
      "|      11111111111111|   NULL|Does this type ne...|1111.00|\n",
      "|         12211111111|3000.00|Does this type ne...|   NULL|\n",
      "|11111111111111111...|2000.00|Does this type ne...|   NULL|\n",
      "+--------------------+-------+--------------------+-------+\n",
      "\n",
      "Invalid AMT_RPT values (containing only non-numeric characters) in /mnt/apps/Files/Schema/etl4pat.csv. Total count: 1\n",
      "Invalid AMT_ORG values (containing only non-numeric characters) in /mnt/apps/Files/Schema/etl4pat.csv. Total count: 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, length, regexp_replace\n",
    "\n",
    "def spark_read_csv_from_os(spark, file_path, schema, header=True, **options):\n",
    "    \"\"\"\n",
    "    Reads a CSV file from the operating system into a Spark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        spark: The SparkSession object.\n",
    "        file_path: The path to the CSV file.  Can be a local path or a path\n",
    "                   that your Spark environment can access (e.g., if you're\n",
    "                   using a distributed file system like HDFS).\n",
    "        header (bool, optional): Whether the CSV file has a header row. Defaults to True.\n",
    "        inferSchema (bool, optional): Whether to infer the schema from the data. Defaults to True.\n",
    "        **options: Additional options to pass to the Spark CSV reader.  See\n",
    "                   the Spark documentation for available options like `delimiter`,\n",
    "                   `quote`, `escape`, etc.\n",
    "\n",
    "    Returns:\n",
    "        A Spark DataFrame representing the CSV data, or None if there's an error.\n",
    "\n",
    "    Raises:\n",
    "       FileNotFoundError: If the file path doesn't exist.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.read.csv(file_path, header=header, inferSchema=False, schema=schema, **options)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:  # Catch other potential exceptions (e.g., parsing errors)\n",
    "        print(f\"An error occurred while reading the CSV: {e}\")\n",
    "        return None\n",
    "    \n",
    "def pandas_read_csv(file_path,**options):\n",
    "    \"\"\"\n",
    "        Read small volume of data only using read.csv\n",
    "        Args:\n",
    "            **Options ----> Any\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path,**options)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:  # Catch other potential exceptions (e.g., parsing errors)\n",
    "        print(f\"An error occurred while reading the CSV: {e}\")\n",
    "        return None\n",
    "    \n",
    "def construct_sql_schema(**kwargs):\n",
    "    \"\"\"\n",
    "        Args: kwargs path and sep -->>> Any\n",
    "        this function is best practice to compute large amount of data to not reading schema metadata\n",
    "        recommendation : \n",
    "    \"\"\"\n",
    "\n",
    "    fields = []\n",
    "    type_mapping = {\n",
    "        \"varchar\": StringType(),\n",
    "        \"nvarchar\": StringType(),\n",
    "        \"int\": IntegerType(),\n",
    "        \"bigint\": LongType(),\n",
    "        \"date\": DateType(),\n",
    "        \"decimal\": DecimalType\n",
    "    }\n",
    "\n",
    "    df = pandas_read_csv(kwargs[\"path\"],sep=kwargs[\"sep\"])\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        try:\n",
    "            name, data_type_str = row.DataType.split(\"(\", 1) if \"(\" in row.DataType else (row.DataType,\"\")\n",
    "            name = name.strip()\n",
    "            data_type_str = data_type_str[:-1].strip()\n",
    "            parts = data_type_str.split(\",\")\n",
    "            name_lower = name.lower()\n",
    "\n",
    "            for keyword,spark_type in type_mapping.items():\n",
    "                if keyword in name_lower:\n",
    "                    if spark_type == DecimalType:\n",
    "                        data_type = DecimalType() if not data_type_str else DecimalType(int(parts[0]),int(parts[1]))\n",
    "                        fields.append(StructField(row.ColumnName, data_type, True))\n",
    "                    else:\n",
    "                        data_type = spark_type\n",
    "                        fields.append(StructField(row.ColumnName, data_type, True))\n",
    "                    break\n",
    "        except Exception as e:  # Catch other potential errors\n",
    "            print(f\"Error processing file in construct schema {kwargs[\"path\"]}: {e}\")\n",
    "            return None\n",
    "    return StructType(fields)\n",
    "\n",
    "def validateDecimal(**kwargs):\n",
    "    df_contents = kwargs[\"df_contents\"]\n",
    "    is_valid = False\n",
    "    errors = []\n",
    "    for field in kwargs[\"dtypes\"]:\n",
    "        colName = field.name\n",
    "        dType = str(field.dataType)\n",
    "        if \"decimal\" in dType.lower():\n",
    "            #print(colName)\n",
    "            df_cleaned = df_contents.withColumn(\n",
    "                f\"{colName}_cleaned\",\n",
    "                regexp_replace(col(colName), \"[^0-9.]\", \"\")\n",
    "            )\n",
    "            df_empty = df_cleaned.filter((col(f\"{colName}_cleaned\")).isNull()) # is null due to schema defined as decimal if we use inferSchema its a string\n",
    "            empty_count = df_empty.count()\n",
    "\n",
    "            if empty_count > 0:\n",
    "                is_valid = True\n",
    "                sample_size = min(100, empty_count)\n",
    "                invalid_rows = df_empty.select(colName).take(sample_size)\n",
    "                error_msg = (f\"Invalid {colName} values (containing only non-numeric characters) in {FullPathSchema}. Total count: {empty_count}\")\n",
    "                errors.append(error_msg)\n",
    "            df_contents = df_contents.drop(f\"{colName}_cleaned\") \n",
    "    msg = \"\\n\".join(errors) if errors else \"True\"\n",
    "    return is_valid, msg, df_contents\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    path = \"/mnt/apps/Files/Config/master_job.csv\"\n",
    "    pathSchema = \"/mnt/apps/Files/Schema/\"\n",
    "\n",
    "    df = pandas_read_csv(path,sep=\"|\")\n",
    "    df = df.query(\"JobName == 'PAT'\")\n",
    "\n",
    "    for row in df.itertuples():  # Collects all data to the driver - NOT recommended for large datasets\n",
    "        filePath = row.SourceDirectory + '/' + row.FileName + '.' + row.FileType\n",
    "        FullPathSchema = pathSchema + row.FileName + '.' + row.FileType\n",
    "        spark = SparkSession.builder.appName(f\"{row.JobName}\").getOrCreate()\n",
    "        df_dtype = construct_sql_schema(path=FullPathSchema, sep=\"|\")\n",
    "        df = spark_read_csv_from_os(spark, filePath, schema=df_dtype, quote='\"', sep=\"|\")\n",
    "        result, dqc_msg, df_final = validateDecimal(dtypes=df_dtype, df_contents=df)\n",
    "        df_final.show()\n",
    "        if result:\n",
    "            print(dqc_msg)\n",
    "        else:\n",
    "            print(dqc_msg)\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- COA: string (nullable = true)\n",
      " |-- AMT_RPT: decimal(18,2) (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      "\n",
      "+--------------------+----------+--------------------+\n",
      "|                 COA|   AMT_RPT|         Description|\n",
      "+--------------------+----------+--------------------+\n",
      "|11111111111111111...|   2000.32|Does this type ne...|\n",
      "|      11111111111111|2000000.00|Does this type ne...|\n",
      "|         12211111111|   3000.00|Does this type ne...|\n",
      "|11111111111111111...|   2000.00|Does this type ne...|\n",
      "+--------------------+----------+--------------------+\n",
      "\n",
      "+--------------------+----------+--------------------+\n",
      "|                 COA|   AMT_RPT|         Description|\n",
      "+--------------------+----------+--------------------+\n",
      "|11111111111111111...|   2000.32|Does this type ne...|\n",
      "|      11111111111111|2000000.00|Does this type ne...|\n",
      "|         12211111111|   3000.00|Does this type ne...|\n",
      "|11111111111111111...|   2000.00|Does this type ne...|\n",
      "+--------------------+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/09 15:56:11 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 4, schema size: 3\n",
      "CSV file: file:///mnt/apps/Files/ETL4/etl4pat.csv\n",
      "25/02/09 15:56:11 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 4, schema size: 3\n",
      "CSV file: file:///mnt/apps/Files/ETL4/etl4pat.csv\n"
     ]
    }
   ],
   "source": [
    "path = \"/mnt/apps/Files/Config/master_job.csv\"\n",
    "pathSchema = \"/mnt/apps/Files/Schema/\"\n",
    "\n",
    "df = pandas_read_csv(path,sep=\"|\")\n",
    "df = df.query(\"JobName == 'PAT'\")\n",
    "\n",
    "for row in df.itertuples():  # Collects all data to the driver - NOT recommended for large datasets\n",
    "    filePath = row.SourceDirectory + '/' + row.FileName + '.' + row.FileType\n",
    "    FullPathSchema = pathSchema + row.FileName + '.' + row.FileType\n",
    "    spark = SparkSession.builder.appName(f\"{row.JobName}\").getOrCreate()\n",
    "    df_dtype = construct_sql_schema(path=FullPathSchema, sep=\"|\")\n",
    "    df = spark_read_csv_from_os(spark, filePath, schema=df_dtype, quote='\"', sep=\"|\")\n",
    "    # Handle the error, e.g., skip the file, log the error, etc.\n",
    "    df.show()\n",
    "\"\"\"     result, dqc_msg, df_final = validateDecimal(dtypes=df_dtype, df_contents=df)\n",
    "    df_final.show()\n",
    "    if result:\n",
    "        print(dqc_msg)\n",
    "    else:\n",
    "        print(dqc_msg) \"\"\"\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JobName</th>\n",
       "      <th>SourceDirectory</th>\n",
       "      <th>FileName</th>\n",
       "      <th>FileType</th>\n",
       "      <th>Flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Renova</td>\n",
       "      <td>/mnt/apps/Files/ETL4</td>\n",
       "      <td>customers</td>\n",
       "      <td>zip</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PAT</td>\n",
       "      <td>/mnt/apps/Files/ETL4</td>\n",
       "      <td>etl4pat</td>\n",
       "      <td>csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  JobName       SourceDirectory   FileName FileType  Flag\n",
       "0  Renova  /mnt/apps/Files/ETL4  customers      zip     1\n",
       "1     PAT  /mnt/apps/Files/ETL4    etl4pat      csv     1"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/mnt/apps/Files/Config/master_job.csv\"\n",
    "\n",
    "df = pandas_read_csv(path,sep=\"|\")\n",
    "df = df.query(\"Flag == 1\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ColumnName       DataType\n",
      "0          COA    Varchar(50)\n",
      "1      AMT_RPT  Decimal(18,2)\n",
      "2  Description   Varchar(MAX)\n",
      "50\n",
      "18,2\n",
      "MAX\n",
      "[StructField('COA', VarcharType(50), True), StructField('AMT_RPT', DecimalType(18,2), True), StructField('Description', VarcharType(4000), True)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "path = \"/mnt/apps/Files/Schema/etl4pat.csv\"\n",
    "fields = []\n",
    "type_mapping = {\n",
    "    \"varchar\": VarcharType,\n",
    "    \"nvarchar\": VarcharType,\n",
    "    \"int\": IntegerType(),\n",
    "    \"bigint\": LongType(),\n",
    "    \"date\": DateType(),\n",
    "    \"decimal\": DecimalType\n",
    "}\n",
    "\n",
    "df = pandas_read_csv(path,sep=\"|\")\n",
    "print(df)\n",
    "\n",
    "for row in df.itertuples():\n",
    "    name, data_type_str = row.DataType.split(\"(\", 1) if \"(\" in row.DataType else (row.DataType,\"\")\n",
    "    name = name.strip()\n",
    "    data_type_str = data_type_str[:-1].strip()\n",
    "    parts = data_type_str.split(\",\")\n",
    "    name_lower = name.lower()\n",
    "    print(data_type_str)\n",
    "\n",
    "    for keyword,spark_type in type_mapping.items():\n",
    "        if keyword in name_lower:\n",
    "            if spark_type == VarcharType:\n",
    "                data_type = VarcharType(4000) if data_type_str == \"MAX\" else VarcharType(int(data_type_str))\n",
    "                fields.append(StructField(row.ColumnName, data_type, True))\n",
    "            elif spark_type == DecimalType:\n",
    "                data_type = DecimalType() if not data_type_str else DecimalType(int(parts[0]),int(parts[1]))\n",
    "                fields.append(StructField(row.ColumnName, data_type, True))\n",
    "            else:\n",
    "                data_type = spark_type\n",
    "                fields.append(StructField(row.ColumnName, data_type, True))\n",
    "            break\n",
    "\n",
    "print(fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_type: Decimal(2,0\n",
      "args: ['']\n",
      "data_type: VARCHAR\n",
      "args: []\n"
     ]
    }
   ],
   "source": [
    "data_type_str = \"Decimal(2,0)\"\n",
    "data_type, *args = data_type_str.split(\")\")\n",
    "\n",
    "print(f\"data_type: {data_type}\")  # Output: data_type: Decimal(2,0\n",
    "print(f\"args: {args}\")          # Output: args: ['']\n",
    "\n",
    "data_type_str = \"VARCHAR\"\n",
    "data_type, *args = data_type_str.split(\")\")\n",
    "\n",
    "print(f\"data_type: {data_type}\")  # Output: data_type: VARCHAR\n",
    "print(f\"args: {args}\")          # Output: args: []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "data = \"A,2\"\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('COA', VarcharType(100), True), StructField('AMT_RPT', DecimalType(2,0), True), StructField('Description', VarcharType(4000), True)])\n"
     ]
    }
   ],
   "source": [
    "path = \"/mnt/apps/Files/Schema/etl4pat.csv\"\n",
    "test = construct_sql_schema(path=path,sep=\"|\")\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *  # Import data types for clarity\n",
    "\n",
    "# Create a SparkSession (if you don't have one already)\n",
    "spark = SparkSession.builder.appName(\"DataTypeExample\").getOrCreate()\n",
    "\n",
    "# Sample data (replace with your actual data)\n",
    "data = [(\"Alice\", 25, 2000.00), (\"Bob\", 30, 2000.00), (\"Charlie\", 22, 2000.00)]\n",
    "\n",
    "# Define the schema explicitly (best practice)\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"height\", DecimalType(2,0), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Display the schema\n",
    "df.printSchema()\n",
    "\n",
    "# Stop the SparkSession (good practice)\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
