{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+---------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+-----------------+--------------------+--------------------+\n",
      "|Index|    Customer Id|First Name|Last Name|             Company|            City|             Country|             Phone 1|             Phone 2|               Email|Subscription Date|             Website|       error_details|\n",
      "+-----+---------------+----------+---------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+-----------------+--------------------+--------------------+\n",
      "| NULL|4962fdbE6Bfee6D|       Pam|   Sparks|        Patel-Deleon|      Blakemouth|British Indian Oc...|    267-243-9490x035|    480-078-0535x889|nicolas00@faulkne...|       2020-11-29| https://nelson.com/|DDL Error please ...|\n",
      "|    2|9b12Ae76fdBc9bE|      Gina|    Rocha|Acosta, Paul and ...|East Lynnchester|          Costa Rica|        027.142.0940|+1-752-593-4777x0...|  yfarley@morgan.com|             NULL|https://pineda-ro...|DDL Error please ...|\n",
      "| NULL|Fc2c8D2BE1AEfDb|  Kristina|  Andrade|            Mann Ltd|    Port Taraton|    Pitcairn Islands| (640)067-7023x66846|001-367-405-8096x592|ivillarreal@fowle...|       2020-09-11|  https://foley.com/|DDL Error please ...|\n",
      "| NULL|9468BBc926AaAB3|       Zoe|   Hansen|          Tanner PLC|    Kimberlyfort|               Benin|   638-798-9796x0247|  (265)475-2386x9812|duransheena@hughe...|       2021-10-09|https://franco-ga...|DDL Error please ...|\n",
      "| NULL|A1505BF376CC5Ed|     Aimee|   Brooks|          Walker Ltd|    Mitchellview|            Malaysia|  112.920.9961x77753|  471.896.6847x82788|chambersdanielle@...|       2022-03-28|   http://solis.org/|DDL Error please ...|\n",
      "| NULL|a24eB840950dac7| Mackenzie|  Leonard|          Abbott Inc|       Bauerfort|             Ukraine|+1-010-716-9313x7...|        315.423.2995|bwheeler@hickman-...|       2022-05-02|http://www.pachec...|DDL Error please ...|\n",
      "+-----+---------------+----------+---------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+-----------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+---------+-----------------------+----------------+---------------------------------------------------+---------------------+---------------------+--------------------------------+-----------------+----------------------------+------------------------------------+\n",
      "|Index|Customer Id    |First Name|Last Name|Company                |City            |Country                                            |Phone 1              |Phone 2              |Email                           |Subscription Date|Website                     |error_details                       |\n",
      "+-----+---------------+----------+---------+-----------------------+----------------+---------------------------------------------------+---------------------+---------------------+--------------------------------+-----------------+----------------------------+------------------------------------+\n",
      "|NULL |4962fdbE6Bfee6D|Pam       |Sparks   |Patel-Deleon           |Blakemouth      |British Indian Ocean Territory (Chagos Archipelago)|267-243-9490x035     |480-078-0535x889     |nicolas00@faulkner-kramer.com   |2020-11-29       |https://nelson.com/         |DDL Error please check data contents|\n",
      "|2    |9b12Ae76fdBc9bE|Gina      |Rocha    |Acosta, Paul and Barber|East Lynnchester|Costa Rica                                         |027.142.0940         |+1-752-593-4777x07171|yfarley@morgan.com              |NULL             |https://pineda-rogers.biz/  |DDL Error please check data contents|\n",
      "|NULL |Fc2c8D2BE1AEfDb|Kristina  |Andrade  |Mann Ltd               |Port Taraton    |Pitcairn Islands                                   |(640)067-7023x66846  |001-367-405-8096x592 |ivillarreal@fowler.biz          |2020-09-11       |https://foley.com/          |DDL Error please check data contents|\n",
      "|NULL |9468BBc926AaAB3|Zoe       |Hansen   |Tanner PLC             |Kimberlyfort    |Benin                                              |638-798-9796x0247    |(265)475-2386x9812   |duransheena@hughes.com          |2021-10-09       |https://franco-galloway.com/|DDL Error please check data contents|\n",
      "|NULL |A1505BF376CC5Ed|Aimee     |Brooks   |Walker Ltd             |Mitchellview    |Malaysia                                           |112.920.9961x77753   |471.896.6847x82788   |chambersdanielle@good-cannon.com|2022-03-28       |http://solis.org/           |DDL Error please check data contents|\n",
      "|NULL |a24eB840950dac7|Mackenzie |Leonard  |Abbott Inc             |Bauerfort       |Ukraine                                            |+1-010-716-9313x74577|315.423.2995         |bwheeler@hickman-acevedo.com    |2022-05-02       |http://www.pacheco.net/     |DDL Error please check data contents|\n",
      "+-----+---------------+----------+---------+-----------------------+----------------+---------------------------------------------------+---------------------+---------------------+--------------------------------+-----------------+----------------------------+------------------------------------+\n",
      "\n",
      "+------------------------------------+-----+\n",
      "|error_details                       |total|\n",
      "+------------------------------------+-----+\n",
      "|DDL Error please check data contents|6    |\n",
      "+------------------------------------+-----+\n",
      "\n",
      "[{'JobName': 'RTRNPF', 'Path': '/mnt/apps/gcs/ETL4/RTRNPF', 'dqID': 'DQ000001', 'CountRecords': 6, 'Message': 'DDL Error please check data contents', 'Status': 'Failed'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/01 09:48:01 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/spark-a54f9980-7041-4384-92c9-2845a307b762/pyspark-fd0ef3d4-5c74-43dc-a464-829976b693ea. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/spark-a54f9980-7041-4384-92c9-2845a307b762/pyspark-fd0ef3d4-5c74-43dc-a464-829976b693ea\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, length, regexp_replace, lit, udf, count, trim\n",
    "from datetime import datetime\n",
    "\n",
    "def spark_read_csv_from_os(spark, file_path, schema, **kwargs):\n",
    "    \"\"\"\n",
    "    Reads a CSV file from the operating system into a Spark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        spark: The SparkSession object.\n",
    "        file_path: The path to the CSV file.  Can be a local path or a path\n",
    "                   that your Spark environment can access (e.g., if you're\n",
    "                   using a distributed file system like HDFS).\n",
    "        header (bool, optional): Whether the CSV file has a header row. Defaults to True.\n",
    "        inferSchema (bool, optional): Whether to infer the schema from the data. Defaults to True.\n",
    "        **options: Additional options to pass to the Spark CSV reader.  See\n",
    "                   the Spark documentation for available options like `delimiter`,\n",
    "                   `quote`, `escape`, etc.\n",
    "\n",
    "    Returns:\n",
    "        A Spark DataFrame representing the CSV data, or None if there's an error.\n",
    "\n",
    "    Raises:\n",
    "       FileNotFoundError: If the file path doesn't exist.\n",
    "    \"\"\"\n",
    "    base_options = {\n",
    "        \"inferSchema\": \"False\",\n",
    "        \"header\": \"True\",\n",
    "        \"quote\": '\"',\n",
    "        \"columnNameOfCorruptRecord\": \"rejected_records\",\n",
    "        \"mode\": \"PERMISSIVE\"\n",
    "    }\n",
    "    base_options.update(kwargs)\n",
    "\n",
    "    try:\n",
    "        schema = StructType(schema.fields + [StructField(\"rejected_records\", StringType(), True)])\n",
    "        df = spark.read.options(**base_options).schema(schema).csv(file_path)\n",
    "    \n",
    "        def parse_and_identify_errors(rejected_str, delimiter=\",\"):\n",
    "            if rejected_str:\n",
    "                try:\n",
    "                    parts = rejected_str.split(delimiter)\n",
    "                    error_columns = []\n",
    "                    error_reasons = []\n",
    "\n",
    "                    for i, field in enumerate(schema.fields):\n",
    "                        try:\n",
    "                            if field.dataType == IntegerType():\n",
    "                                int(parts[i])\n",
    "                            elif field.dataType == DateType():\n",
    "                                datetime.strptime(parts[i], \"%Y-%m-%d\").date() #change date format\n",
    "                            # Add more data type checks as needed\n",
    "                        except (ValueError, IndexError):\n",
    "                            error_columns.append(field.name)\n",
    "                            error_reasons.append(f\"Invalid {field.dataType.typeName()}\")\n",
    "\n",
    "                    if error_columns:\n",
    "                        error_detail_string = \", \".join([f\"{col}: {reason}\" for col, reason in zip(error_columns, error_reasons)])\n",
    "                        #print(f\"Rejected String: {rejected_str}, Error Details: {error_detail_string}\") #debugging print\n",
    "                        return error_detail_string\n",
    "                    else:\n",
    "                        print(f\"Error parsing record: {e}, No errors found\") #debugging print\n",
    "                        return None\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing record: {e}, Rejected String: {rejected_str}\") #debugging print\n",
    "                    return f\"Error parsing record: {e}\"\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        parse_errors_udf = udf(parse_and_identify_errors, StringType())\n",
    "\n",
    "        rejected_df = df.filter(col(\"rejected_records\").isNotNull()).withColumn(\n",
    "            \"error_details\", parse_errors_udf(col(\"rejected_records\"))\n",
    "        )\n",
    "\n",
    "        df = df.drop(\"rejected_records\")\n",
    "        rejected_df = rejected_df.drop(\"rejected_records\")\n",
    "\n",
    "        return df, rejected_df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:  # Catch other potential exceptions (e.g., parsing errors)\n",
    "        print(f\"An error occurred while reading the CSV: {e}\")\n",
    "        return None\n",
    "    \n",
    "def pandas_read_csv(file_path,**options):\n",
    "    \"\"\"\n",
    "        Read small volume of data only using read.csv\n",
    "        Args:\n",
    "            **Options ----> Any\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path,**options)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:  # Catch other potential exceptions (e.g., parsing errors)\n",
    "        print(f\"An error occurred while reading the CSV: {e}\")\n",
    "        return None\n",
    "    \n",
    "def construct_sql_schema(**kwargs):\n",
    "    \"\"\"\n",
    "        Args: kwargs path and sep -->>> Any\n",
    "        this function is best practice to compute large amount of data to not reading schema metadata\n",
    "        recommendation : \n",
    "    \"\"\"\n",
    "\n",
    "    fields = []\n",
    "    type_mapping = {\n",
    "        \"varchar\": StringType(),\n",
    "        \"nvarchar\": StringType(),\n",
    "        \"int\": IntegerType(),\n",
    "        \"bigint\": LongType(),\n",
    "        \"date\": DateType(),\n",
    "        \"decimal\": DecimalType\n",
    "    }\n",
    "\n",
    "    df = pandas_read_csv(kwargs[\"path\"],sep=kwargs[\"sep\"])\n",
    "    #print(df)\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        try:\n",
    "            name, data_type_str = row.DataType.split(\"(\", 1) if \"(\" in row.DataType else (row.DataType,\"\")\n",
    "            name = name.strip()\n",
    "            data_type_str = data_type_str[:-1].strip()\n",
    "            parts = data_type_str.split(\",\")\n",
    "            name_lower = name.lower()\n",
    "\n",
    "            for keyword,spark_type in type_mapping.items():\n",
    "                if keyword in name_lower:\n",
    "                    if spark_type == DecimalType:\n",
    "                        data_type = DecimalType() if not data_type_str else DecimalType(int(parts[0]),int(parts[1]))\n",
    "                        fields.append(StructField(row.ColumnName, data_type, True))\n",
    "                    else:\n",
    "                        data_type = spark_type\n",
    "                        fields.append(StructField(row.ColumnName, data_type, True))\n",
    "                    break\n",
    "        except Exception as e:  # Catch other potential errors\n",
    "            print(f\"Error processing file in construct schema {kwargs[\"path\"]}: {e}\")\n",
    "            return None\n",
    "    return StructType(fields)\n",
    "\n",
    "def validateDecimal(**kwargs):\n",
    "    df_contents = kwargs[\"df_contents\"]\n",
    "    is_valid = False\n",
    "    errors = []\n",
    "    dqcId = \"DQ000001\"\n",
    "    for field in kwargs[\"dtypes\"]:\n",
    "        colName = field.name\n",
    "        dType = str(field.dataType)\n",
    "        if \"decimal\" in dType.lower() or \"int\" in dType.lower():\n",
    "            #print(colName)\n",
    "            df_cleaned = df_contents.withColumn(\n",
    "                f\"{colName}_cleaned\",\n",
    "                regexp_replace(col(colName), \"[^0-9.]\", \"\")\n",
    "            )\n",
    "            df_empty = df_cleaned.filter((col(f\"{colName}_cleaned\")).isNull()) # is null due to schema defined as decimal if we use inferSchema its a string\n",
    "            #print(\"test disini\")\n",
    "            #df_empty.show()\n",
    "            empty_count = df_empty.count()\n",
    "\n",
    "            if empty_count > 0:\n",
    "                is_valid = True\n",
    "                error_msg = (f\"Invalid {colName} values (containing only non-numeric characters). Total count: {empty_count}\")\n",
    "                errors.append(error_msg)\n",
    "            df_contents = df_contents.drop(f\"{colName}_cleaned\") \n",
    "            \n",
    "    if is_valid == False:        \n",
    "        error_msg = \"DDL Decimal/Int Data Type Structure Checks Passed.\"\n",
    "        errors.append(error_msg)\n",
    "    msg = \"\\n\".join(errors) if errors else \"Data Quality Checks Passed.\" # this is for breakdown into rows from array\n",
    "    return is_valid, errors, df_contents, dqcId\n",
    "\n",
    "def writeOptions(df, dataMovement, **kwargs):\n",
    "    base_options = {  # Keep this separate\n",
    "        \"header\": \"true\",\n",
    "        \"delimiter\": \"|\",\n",
    "        \"quote\": '\"'\n",
    "    }\n",
    "    base_options.update(kwargs)\n",
    "    df.coalesce(1).write.format(\"csv\").mode(\"overwrite\").options(**base_options).save(dataMovement)\n",
    "\n",
    "def writeToParquet(df, path):\n",
    "    df = df.coalesce(50)\n",
    "    df.write.parquet(path, mode=\"overwrite\", compression=\"snappy\")\n",
    "\n",
    "def loadTable(**kwargs):\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS spark_catalog.default.{kwargs['tableName']}\") \n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE EXTERNAL TABLE {kwargs[\"tableName\"]}\n",
    "    USING CSV\n",
    "    OPTIONS (\n",
    "        header 'true',  -- If your CSV has a header row\n",
    "        inferSchema 'false', -- Important: Set to false since we provide schema\n",
    "        delimiter '|' -- Specify the delimiter if it's not a comma\n",
    "    )\n",
    "        LOCATION '{kwargs[\"path\"]}'\n",
    "    \"\"\")\n",
    "    df = spark.sql(f\"SELECT * FROM {kwargs[\"tableName\"]}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"/mnt/apps/Files/Config/master_job.csv\"\n",
    "    pathSchema = \"/mnt/apps/Files/Schema/\"\n",
    "    outputFile = \"/mnt/apps/Files/data-movement/\"\n",
    "    parquetOutput = \"/mnt/apps/Files/data-movement/Parquet/\"\n",
    "    dqcOutput = []\n",
    "    dqcId = \"DQ000001\"\n",
    "\n",
    "    spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        appName(\"Testing\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.ui.port\", \"4222\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    df = pandas_read_csv(path,sep=\"|\")\n",
    "    df = df.query(f\"JobName == 'RTRNPF' | JobName == 'ACMVPF'\")\n",
    "    #print(df)\n",
    "\n",
    "    for row in df.itertuples():  # Collects all data to the driver - NOT recommended for large datasets\n",
    "        filePath = row.SourceDirectory + '/' + row.FileName + '*.' + row.FileType\n",
    "        filePath = filePath.replace(\"/gcs\", \"/Files\")\n",
    "        dataMovement = outputFile + row.JobName + '/'\n",
    "        dataMovementParquet = parquetOutput + row.JobName\n",
    "        #print(row.JobName)\n",
    "        FullPathSchema = pathSchema + row.FileName + '.' + row.FileType\n",
    "        #print(FullPathSchema)\n",
    "        #spark.stop()\n",
    "        df_dtype = construct_sql_schema(path=FullPathSchema, sep=\"|\")\n",
    "        #print(df_dtype)\n",
    "        df, rejected_df = spark_read_csv_from_os(spark, filePath, schema=df_dtype, sep=row.Delimiter)\n",
    "        rejected_df.cache()\n",
    "        rejected_df.take(1)\n",
    "        is_empty = True if rejected_df.count() > 0 else False\n",
    "        if is_empty:\n",
    "            # need to catch malforme\n",
    "            # Group by error_details \n",
    "            rejected_df.show(truncate=False)\n",
    "            rejCnt = rejected_df.groupBy(\"error_details\").agg(count(\"*\").alias(\"total\"))\n",
    "            rejCnt.show(truncate=False)\n",
    "            collected_rejCnt = rejCnt.collect()\n",
    "            if collected_rejCnt:\n",
    "                err_msg = collected_rejCnt[0]\n",
    "                dqcOutput.append({\"JobName\":row.JobName, \"Path\":row.SourceDirectory, \"dqID\":dqcId, \"CountRecords\":err_msg.total, \"Message\":err_msg.error_details, \"Status\":\"Failed\"})\n",
    "            else:\n",
    "                print(\"No rejected errors found.\")\n",
    "            rejected_df.unpersist()\n",
    "        else:\n",
    "            df_count = df.count()\n",
    "            #writeOptions(df, dataMovement)\n",
    "            dqcOutput.append({\"JobName\":row.JobName, \"Path\":row.SourceDirectory, \"dqID\":dqcId, \"CountRecords\":df_count, \"Message\":\"DDL Data Quality Check Passed !!!!\", \"Status\":\"Successful\"})\n",
    "        rejected_df.unpersist()\n",
    "    print(dqcOutput)\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "#help(spark.read.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Index: int, Customer Id: string, First Name: string, Last Name: string, Company: string, City: string, Country: string, Phone 1: string, Phone 2: string, Email: string, Subscription Date: date, Website: string, error_details: string]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rejected_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(error_details='invalid', total=6)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark1 = SparkSession.builder.appName(\"CreateDataFrameExample\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"invalid\",6)\n",
    "]\n",
    "\n",
    "schema = [\"error_details\", \"total\"]\n",
    "\n",
    "df = spark1.createDataFrame(data, schema=schema)\n",
    "\n",
    "a = df.collect()[0]\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/01 05:42:49 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------+-----+\n",
      "|error_details                                          |count|\n",
      "+-------------------------------------------------------+-----+\n",
      "|Index: Invalid integer, Subscription Date: Invalid date|6    |\n",
      "+-------------------------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"GroupByErrorDetails\").getOrCreate()\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"Index\", StringType(), True),\n",
    "    StructField(\"Customer Id\", StringType(), True),\n",
    "    StructField(\"First Name\", StringType(), True),\n",
    "    StructField(\"Last Name\", StringType(), True),\n",
    "    StructField(\"Company\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "    StructField(\"Phone 1\", StringType(), True),\n",
    "    StructField(\"Phone 2\", StringType(), True),\n",
    "    StructField(\"Email\", StringType(), True),\n",
    "    StructField(\"Subscription Date\", StringType(), True),\n",
    "    StructField(\"Website\", StringType(), True),\n",
    "    StructField(\"error_details\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Sample data (replace with your actual data source)\n",
    "data = [\n",
    "    (None, \"4962fdbE6Bfee6D\", \"Pam\", \"Sparks\", \"Patel-Deleon\", \"Blakemouth\", \"British Indian Ocean Territory (Chagos Archipelago)\", \"267-243-9490x035\", \"480-078-0535x889\", \"nicolas00@faulkner-kramer.com\", \"2020-11-29\", \"https://nelson.com/\", \"Index: Invalid integer, Subscription Date: Invalid date\"),\n",
    "    (\"2\", \"9b12Ae76fdBc9bE\", \"Gina\", \"Rocha\", \"Acosta, Paul and Barber\", \"East Lynnchester\", \"Costa Rica\", \"027.142.0940\", \"+1-752-593-4777x07171\", \"yfarley@morgan.com\", None, \"https://pineda-rogers.biz/\", \"Index: Invalid integer, Subscription Date: Invalid date\"),\n",
    "    (None, \"Fc2c8D2BE1AEfDb\", \"Kristina\", \"Andrade\", \"Mann Ltd\", \"Port Taraton\", \"Pitcairn Islands\", \"(640)067-7023x66846\", \"001-367-405-8096x592\", \"ivillarreal@fowler.biz\", \"2020-09-11\", \"https://foley.com/\", \"Index: Invalid integer, Subscription Date: Invalid date\"),\n",
    "    (None, \"9468BBc926AaAB3\", \"Zoe\", \"Hansen\", \"Tanner PLC\", \"Kimberlyfort\", \"Benin\", \"638-798-9796x0247\", \"(265)475-2386x9812\", \"duransheena@hughes.com\", \"2021-10-09\", \"https://franco-galloway.com/\", \"Index: Invalid integer, Subscription Date: Invalid date\"),\n",
    "    (None, \"A1505BF376CC5Ed\", \"Aimee\", \"Brooks\", \"Walker Ltd\", \"Mitchellview\", \"Malaysia\", \"112.920.9961x77753\", \"471.896.6847x82788\", \"chambersdanielle@good-cannon.com\", \"2022-03-28\", \"http://solis.org/\", \"Index: Invalid integer, Subscription Date: Invalid date\"),\n",
    "    (None, \"a24eB840950dac7\", \"Mackenzie\", \"Leonard\", \"Abbott Inc\", \"Bauerfort\", \"Ukraine\", \"+1-010-716-9313x74577\", \"315.423.2995\", \"bwheeler@hickman-acevedo.com\", \"2022-05-02\", \"http://www.pacheco.net/\", \"Index: Invalid integer, Subscription Date: Invalid date\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Group by 'error_details' and count occurrences\n",
    "result_df = df.groupBy(\"error_details\").agg(F.count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Show the result\n",
    "result_df.show(truncate=False)\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       BatchName JobName            SourceDirectory FileName FileType  Flag  \\\n",
      "1  BATCH_ACT_VAL  ACMVPF  /mnt/apps/gcs/ETL4/ACMVPF   ACMVPF      csv     1   \n",
      "2  BATCH_ACT_VAL  RTRNPF  /mnt/apps/gcs/ETL4/RTRNPF   RTRNPF      csv     1   \n",
      "\n",
      "  Delimiter  \n",
      "1         |  \n",
      "2         |  \n",
      "['/mnt/apps/Files/ETL4/ACMVPF/ACMVPF.csv', '/mnt/apps/Files/ETL4/RTRNPF/RTRNPF.csv']\n"
     ]
    }
   ],
   "source": [
    "path = \"/mnt/apps/Files/Config/master_job.csv\"\n",
    "df = pandas_read_csv(path,sep=\"|\")\n",
    "df = df.query(f\"BatchName == 'BATCH_ACT_VAL'\")\n",
    "print(df)\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        appName(\"Thread\"). \\\n",
    "        getOrCreate()\n",
    "\n",
    "tables = []\n",
    "for row in df.itertuples():\n",
    "    filePath = row.SourceDirectory + '/' + row.FileName + '.' + row.FileType\n",
    "    filePath = filePath.replace(\"/gcs\", \"/Files\")\n",
    "    tables.append(filePath)\n",
    "\n",
    "def loadTable(path):\n",
    "    sc = path.split(\"/\")[5]\n",
    "    pathParquet = f\"/mnt/apps/Files/data-movement/Parquet/{sc}\"\n",
    "    print(pathParquet)\n",
    "    df_dtype = construct_sql_schema(path=FullPathSchema, sep=\"|\")\n",
    "    df = spark.read.csv(path, header=True, inferSchema=False, schema=df_dtype, sep=\"|\")\n",
    "    result, dqc_msg, df_final, dqcId = validateDecimal(dtypes=df_dtype, df_contents=df)\n",
    "    df_count = df_final.count()\n",
    "    if result:\n",
    "        print(dqc_msg)\n",
    "        dqcOutput.append({\"JobName\":row.JobName, \"Path\":row.SourceDirectory, \"dqID\":dqcId, \"CountRecords\":df_count, \"Message\":dqc_msg, \"Status\":\"Failed\"})\n",
    "    else:\n",
    "        writeToParquet(df_final, pathParquet)\n",
    "        print(dqc_msg)\n",
    "        dqcOutput.append({\"JobName\":row.JobName, \"Path\":row.SourceDirectory, \"dqID\":dqcId, \"CountRecords\":df_count, \"Message\":dqc_msg, \"Status\":\"Successful\"})\n",
    "    \n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from queue import Queue\n",
    "\n",
    "q = Queue()\n",
    "\n",
    "workerCount = 2\n",
    "\n",
    "def run_task(function, q):\n",
    "    while not q.empty():\n",
    "        value = q.get()\n",
    "        function(value)\n",
    "        q.task_done()\n",
    "\n",
    "for table in tables:\n",
    "    q.put(table)\n",
    "\n",
    "for i in range(workerCount):\n",
    "    t=Thread(target=run_task, args=(loadTable, q))\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "\n",
    "print(\"running load\")\n",
    "q.join()\n",
    "spark.stop()\n",
    "print(\"running completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name\n",
    "import dask.dataframe as dd\n",
    "\n",
    "#outputFile = \"/mnt/apps/Files/ETL4/LAS/customers.csv\"\n",
    "outputFile = \"/mnt/apps/Files/data-movement/Renova/part-00000*\"\n",
    "FullPathSchema = \"/mnt/apps/Files/Schema/customers.csv\"\n",
    "ParquetPath = \"/mnt/apps/Files/data-movement/Parquet/Renova/part-000*\"\n",
    "PdoutputFile = \"/mnt/apps/Files/ETL4/LAS/customers.csv\"\n",
    "\n",
    "#df_dtype = construct_sql_schema(path=FullPathSchema, sep=\"|\")\n",
    "#print(df_dtype)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TestReadCSV\").getOrCreate()\n",
    "\n",
    "#CSV reader\n",
    "#df = spark.read.csv(outputFile, sep=\"|\", header=True, schema=df_dtype, inferSchema=False).repartition(10)\n",
    "\n",
    "# df_with_filename = df.withColumn(\"filename\", input_file_name())\n",
    "# null_count = df_with_filename.filter(col(\"Index\").isNotNull())\n",
    "# df_count = null_count.count()\n",
    "# print(df_count)\n",
    "# #null_count.limit(10).show(truncate=False)\n",
    "# null_count.createOrReplaceTempView(\"readCSV\")\n",
    "# #df_parquet_count = null_count_parquet.count()\n",
    "# #print(df_parquet_count)\n",
    "# null_count = spark.sql(\n",
    "#     \"\"\"\n",
    "#     SELECT country, cnt, sum(cnt) over () total_all\n",
    "#     FROM (\n",
    "#     SELECT Country, COUNT(1) AS cnt\n",
    "#         FROM readCSV\n",
    "#         GROUP BY Country\n",
    "#     ) Z\n",
    "# \"\"\")\n",
    "# print(null_count.count())\n",
    "#df.show(n=5, truncate=False) #default 20\n",
    "\n",
    "#parquet part\n",
    "# df_read_parquet = spark.read.parquet(outputFile, schema=df_dtype)\n",
    "# null_count_parquet = df_read_parquet.filter(col(\"Index\").isNotNull())\n",
    "# null_count_parquet.createOrReplaceTempView(\"my_parquet_table\")\n",
    "# #df_parquet_count = null_count_parquet.count()\n",
    "# #print(df_parquet_count)\n",
    "# null_count_parquet = spark.sql(\n",
    "#     \"\"\"\n",
    "#     SELECT country, cnt, sum(cnt) over () total_all\n",
    "#     FROM (\n",
    "#     SELECT Country, COUNT(1) AS cnt\n",
    "#         FROM my_parquet_table\n",
    "#         GROUP BY Country\n",
    "#     ) Z\n",
    "# \"\"\")\n",
    "# print(null_count_parquet.count())\n",
    "# null_count_parquet.show(n=5, truncate=False) #default 20\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of = []\n",
    "for i in range(240):\n",
    "    list_of.append(f\"Cloned_{i + 1}|Varchar(100)\")\n",
    "   \n",
    "df = pd.DataFrame(list_of, columns=[\"ColumnName\"])\n",
    "df.head(n=250)\n",
    "\n",
    "df.to_csv(\"/mnt/apps/Files/NewSChema/new_schema_cust.csv\",header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/mnt/apps/Files/Config/master_job.csv\"\n",
    "pathSchema = \"/mnt/apps/Files/Schema/\"\n",
    "\n",
    "df = pandas_read_csv(path,sep=\"|\")\n",
    "df = df.query(\"JobName == 'PAT'\")\n",
    "\n",
    "for row in df.itertuples():  # Collects all data to the driver - NOT recommended for large datasets\n",
    "    filePath = row.SourceDirectory + '/' + row.FileName + '.' + row.FileType\n",
    "    FullPathSchema = pathSchema + row.FileName + '.' + row.FileType\n",
    "    spark = SparkSession.builder.appName(f\"{row.JobName}\").getOrCreate()\n",
    "    df_dtype = construct_sql_schema(path=FullPathSchema, sep=\"|\")\n",
    "    df = spark_read_csv_from_os(spark, filePath, schema=df_dtype, quote='\"', sep=\"|\")\n",
    "    # Handle the error, e.g., skip the file, log the error, etc.\n",
    "    df.show()\n",
    "\"\"\"     result, dqc_msg, df_final = validateDecimal(dtypes=df_dtype, df_contents=df)\n",
    "    df_final.show()\n",
    "    if result:\n",
    "        print(dqc_msg)\n",
    "    else:\n",
    "        print(dqc_msg) \"\"\"\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index             int64\n",
      "User Id          object\n",
      "First Name       object\n",
      "Last Name        object\n",
      "Sex              object\n",
      "Email            object\n",
      "Phone            object\n",
      "Date of birth    object\n",
      "Job Title        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "path = \"/mnt/apps/Files/ETL4/PEOPLEPF/people.csv\"\n",
    "\n",
    "df = pandas_read_csv(path, sep=\",\")\n",
    "#df.head(1)\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "path = \"/mnt/apps/Files/Schema/etl4pat.csv\"\n",
    "fields = []\n",
    "type_mapping = {\n",
    "    \"varchar\": VarcharType,\n",
    "    \"nvarchar\": VarcharType,\n",
    "    \"int\": IntegerType(),\n",
    "    \"bigint\": LongType(),\n",
    "    \"date\": DateType(),\n",
    "    \"decimal\": DecimalType\n",
    "}\n",
    "\n",
    "df = pandas_read_csv(path,sep=\"|\")\n",
    "print(df)\n",
    "\n",
    "for row in df.itertuples():\n",
    "    name, data_type_str = row.DataType.split(\"(\", 1) if \"(\" in row.DataType else (row.DataType,\"\")\n",
    "    name = name.strip()\n",
    "    data_type_str = data_type_str[:-1].strip()\n",
    "    parts = data_type_str.split(\",\")\n",
    "    name_lower = name.lower()\n",
    "    print(data_type_str)\n",
    "\n",
    "    for keyword,spark_type in type_mapping.items():\n",
    "        if keyword in name_lower:\n",
    "            if spark_type == VarcharType:\n",
    "                data_type = VarcharType(4000) if data_type_str == \"MAX\" else VarcharType(int(data_type_str))\n",
    "                fields.append(StructField(row.ColumnName, data_type, True))\n",
    "            elif spark_type == DecimalType:\n",
    "                data_type = DecimalType() if not data_type_str else DecimalType(int(parts[0]),int(parts[1]))\n",
    "                fields.append(StructField(row.ColumnName, data_type, True))\n",
    "            else:\n",
    "                data_type = spark_type\n",
    "                fields.append(StructField(row.ColumnName, data_type, True))\n",
    "            break\n",
    "\n",
    "print(fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type_str = \"Decimal(2,0)\"\n",
    "data_type, *args = data_type_str.split(\")\")\n",
    "\n",
    "print(f\"data_type: {data_type}\")  # Output: data_type: Decimal(2,0\n",
    "print(f\"args: {args}\")          # Output: args: ['']\n",
    "\n",
    "data_type_str = \"VARCHAR\"\n",
    "data_type, *args = data_type_str.split(\")\")\n",
    "\n",
    "print(f\"data_type: {data_type}\")  # Output: data_type: VARCHAR\n",
    "print(f\"args: {args}\")          # Output: args: []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"A,2\"\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/mnt/apps/Files/Schema/etl4pat.csv\"\n",
    "test = construct_sql_schema(path=path,sep=\"|\")\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *  # Import data types for clarity\n",
    "\n",
    "# Create a SparkSession (if you don't have one already)\n",
    "spark = SparkSession.builder.appName(\"DataTypeExample\").getOrCreate()\n",
    "\n",
    "# Sample data (replace with your actual data)\n",
    "data = [(\"Alice\", 25, 2000.00), (\"Bob\", 30, 2000.00), (\"Charlie\", 22, 2000.00)]\n",
    "\n",
    "# Define the schema explicitly (best practice)\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"height\", DecimalType(2,0), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Display the schema\n",
    "df.printSchema()\n",
    "\n",
    "# Stop the SparkSession (good practice)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"/mnt/apps/gcs/ETL4/CONFIG/etl4pat*.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def writeOptions(df, path, **kwargs):\n",
    "    base_options = {  # Keep this separate\n",
    "        \"header\": \"true\",\n",
    "        \"delimiter\": \"|\",\n",
    "        \"quote\": '\"',\n",
    "        \"mode\":\"overwrite\",\n",
    "        \"format\":\"csv\"\n",
    "    }\n",
    "\n",
    "    # Correct way to merge options:\n",
    "    all_options = base_options.copy()  # Create a copy to avoid modifying base_options\n",
    "    all_options.update(kwargs)       # Add or overwrite kwargs\n",
    "    print(all_options)\n",
    "\n",
    "    df.write.options(**all_options).save(path)\n",
    "\n",
    "# Example usage (important: complete example):\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
    "\n",
    "dataMovement = \"path/to/save.csv\" # Or your actual path\n",
    "\n",
    "writeOptions(df, dataMovement)  # Now works correctly\n",
    "\n",
    "spark.stop()  # Don't forget to stop the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------+-------+\n",
      "|col_name                    |data_type                   |comment|\n",
      "+----------------------------+----------------------------+-------+\n",
      "|Index                       |string                      |NULL   |\n",
      "|Customer Id                 |string                      |NULL   |\n",
      "|First Name                  |string                      |NULL   |\n",
      "|Last Name                   |string                      |NULL   |\n",
      "|Company                     |string                      |NULL   |\n",
      "|City                        |string                      |NULL   |\n",
      "|Country                     |string                      |NULL   |\n",
      "|Phone 1                     |string                      |NULL   |\n",
      "|Phone 2                     |string                      |NULL   |\n",
      "|Email                       |string                      |NULL   |\n",
      "|Subscription Date           |string                      |NULL   |\n",
      "|Website                     |string                      |NULL   |\n",
      "|                            |                            |       |\n",
      "|# Detailed Table Information|                            |       |\n",
      "|Catalog                     |spark_catalog               |       |\n",
      "|Database                    |default                     |       |\n",
      "|Table                       |acmvpf                      |       |\n",
      "|Created Time                |Sat Mar 15 04:20:48 UTC 2025|       |\n",
      "|Last Access                 |UNKNOWN                     |       |\n",
      "|Created By                  |Spark 3.5.4                 |       |\n",
      "+----------------------------+----------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|18000000|\n",
      "+--------+\n",
      "\n",
      "+-----+---------------+----------+---------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+-----------------+--------------------+\n",
      "|Index|    Customer Id|First Name|Last Name|             Company|             City|             Country|             Phone 1|             Phone 2|               Email|Subscription Date|             Website|\n",
      "+-----+---------------+----------+---------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+-----------------+--------------------+\n",
      "|    1|4962fdbE6Bfee6D|       Pam|   Sparks|        Patel-Deleon|       Blakemouth|British Indian Oc...|    267-243-9490x035|    480-078-0535x889|nicolas00@faulkne...|       2020-11-29| https://nelson.com/|\n",
      "|    2|9b12Ae76fdBc9bE|      Gina|    Rocha|Acosta, Paul and ...| East Lynnchester|          Costa Rica|        027.142.0940|+1-752-593-4777x0...|  yfarley@morgan.com|       2021-01-03|https://pineda-ro...|\n",
      "|    3|39edFd2F60C85BC|   Kristie|    Greer|           Ochoa PLC|      West Pamela|             Ecuador|+1-049-168-7497x5053|     +1-311-216-7855|jennyhayden@petty...|       2021-06-20|https://mckinney....|\n",
      "|    4|Fa42AE6a9aD39cE|    Arthur|   Fields|          Moyer-Wang|     East Belinda|         Afghanistan|001-653-754-7486x...|    521-630-3858x953|igrimes@ruiz-todd...|       2020-02-13|https://dominguez...|\n",
      "|    5|F5702Edae925F1D|  Michelle|  Blevins|       Shah and Sons|       West Jared|    Marshall Islands|          8735278329|   (633)283-6034x500|diamondcarter@jor...|       2020-10-20|http://murillo-ry...|\n",
      "|    6|9FBfbd34c3f30d9|      Greg|   Rivers|Hendrix, Arias an...|   New Pedrohaven|             Senegal|          2382697028|        822-449-7638|mannkiara@bowman.com|       2020-02-19|https://www.richa...|\n",
      "|    7|D31D38D576aB1Bf|     Brian|   Brandt|Reed, Wiley and O...|         Rayville|    Marshall Islands|    608.531.0005x630|   926-957-7052x6181|dlynn@palmer-morg...|       2020-03-31|https://www.washi...|\n",
      "|    8|3C3B8bee16BcC5F|    Dennis|  Pacheco|     Contreras-Brock|South Eugenehaven|           Sri Lanka|     +1-391-505-2358|        076.454.1128|    joy45@conner.org|       2021-03-20|http://www.armstr...|\n",
      "|    9|7C1C93522Bd5e4B|     Malik|   Rivers|Best, Washington ...|        Mejiastad|   Wallis and Futuna|001-366-285-9819x...|   852.896.9192x3681|stacydavis@spence...|       2020-01-16| https://watson.com/|\n",
      "|   10|bABC8cBffe68792|    Hayden|   Riddle|         Rogers-Dean|     Kaitlynmouth|            Cameroon|  951.359.7028x94651|        931.227.4541| danhouston@moon.net|       2022-05-18|https://www.farle...|\n",
      "+-----+---------------+----------+---------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "path = \"/mnt/apps/Files/data-movement/ACMVPF/part*.csv.gz\"\n",
    "\n",
    "spark = SparkSession.builder.appName(\"READCSVGZ\").getOrCreate()\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "            CREATE EXTERNAL TABLE IF NOT EXISTS ACMVPF\n",
    "            USING CSV \n",
    "            OPTIONS (\n",
    "                    path '{path}',\n",
    "                    delimiter '|',\n",
    "                    header 'true',\n",
    "                    compression 'gzip'\n",
    "            )\n",
    "            \"\"\")\n",
    "\n",
    "spark.sql(\"DESCRIBE EXTENDED ACMVPF\").show(truncate=False)\n",
    "spark.sql(\"SELECT COUNT(*) FROM ACMVPF\").show()\n",
    "spark.sql(\"SELECT * FROM ACMVPF LIMIT 10\").show()\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
