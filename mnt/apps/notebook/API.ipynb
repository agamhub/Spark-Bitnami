{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/apps/Files/data-movement/PAT/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/17 02:46:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Number of partitions: 16\n",
      "Invalid AMT_RPT values (containing only non-numeric characters) in /mnt/apps/Files/Schema/etl4pat.csv. Total count: 1\n",
      "Invalid AMT_ORG values (containing only non-numeric characters) in /mnt/apps/Files/Schema/etl4pat.csv. Total count: 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, length, regexp_replace\n",
    "\n",
    "def spark_read_csv_from_os(spark, file_path, schema, header=True, **options):\n",
    "    \"\"\"\n",
    "    Reads a CSV file from the operating system into a Spark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        spark: The SparkSession object.\n",
    "        file_path: The path to the CSV file.  Can be a local path or a path\n",
    "                   that your Spark environment can access (e.g., if you're\n",
    "                   using a distributed file system like HDFS).\n",
    "        header (bool, optional): Whether the CSV file has a header row. Defaults to True.\n",
    "        inferSchema (bool, optional): Whether to infer the schema from the data. Defaults to True.\n",
    "        **options: Additional options to pass to the Spark CSV reader.  See\n",
    "                   the Spark documentation for available options like `delimiter`,\n",
    "                   `quote`, `escape`, etc.\n",
    "\n",
    "    Returns:\n",
    "        A Spark DataFrame representing the CSV data, or None if there's an error.\n",
    "\n",
    "    Raises:\n",
    "       FileNotFoundError: If the file path doesn't exist.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.read.csv(file_path, header=header, inferSchema=False, schema=schema, **options)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:  # Catch other potential exceptions (e.g., parsing errors)\n",
    "        print(f\"An error occurred while reading the CSV: {e}\")\n",
    "        return None\n",
    "    \n",
    "def pandas_read_csv(file_path,**options):\n",
    "    \"\"\"\n",
    "        Read small volume of data only using read.csv\n",
    "        Args:\n",
    "            **Options ----> Any\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path,**options)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:  # Catch other potential exceptions (e.g., parsing errors)\n",
    "        print(f\"An error occurred while reading the CSV: {e}\")\n",
    "        return None\n",
    "    \n",
    "def construct_sql_schema(**kwargs):\n",
    "    \"\"\"\n",
    "        Args: kwargs path and sep -->>> Any\n",
    "        this function is best practice to compute large amount of data to not reading schema metadata\n",
    "        recommendation : \n",
    "    \"\"\"\n",
    "\n",
    "    fields = []\n",
    "    type_mapping = {\n",
    "        \"varchar\": StringType(),\n",
    "        \"nvarchar\": StringType(),\n",
    "        \"int\": IntegerType(),\n",
    "        \"bigint\": LongType(),\n",
    "        \"date\": DateType(),\n",
    "        \"decimal\": DecimalType\n",
    "    }\n",
    "\n",
    "    df = pandas_read_csv(kwargs[\"path\"],sep=kwargs[\"sep\"])\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        try:\n",
    "            name, data_type_str = row.DataType.split(\"(\", 1) if \"(\" in row.DataType else (row.DataType,\"\")\n",
    "            name = name.strip()\n",
    "            data_type_str = data_type_str[:-1].strip()\n",
    "            parts = data_type_str.split(\",\")\n",
    "            name_lower = name.lower()\n",
    "\n",
    "            for keyword,spark_type in type_mapping.items():\n",
    "                if keyword in name_lower:\n",
    "                    if spark_type == DecimalType:\n",
    "                        data_type = DecimalType() if not data_type_str else DecimalType(int(parts[0]),int(parts[1]))\n",
    "                        fields.append(StructField(row.ColumnName, data_type, True))\n",
    "                    else:\n",
    "                        data_type = spark_type\n",
    "                        fields.append(StructField(row.ColumnName, data_type, True))\n",
    "                    break\n",
    "        except Exception as e:  # Catch other potential errors\n",
    "            print(f\"Error processing file in construct schema {kwargs[\"path\"]}: {e}\")\n",
    "            return None\n",
    "    return StructType(fields)\n",
    "\n",
    "def validateDecimal(**kwargs):\n",
    "    df_contents = kwargs[\"df_contents\"]\n",
    "    is_valid = False\n",
    "    errors = []\n",
    "    for field in kwargs[\"dtypes\"]:\n",
    "        colName = field.name\n",
    "        dType = str(field.dataType)\n",
    "        if \"decimal\" in dType.lower():\n",
    "            #print(colName)\n",
    "            df_cleaned = df_contents.withColumn(\n",
    "                f\"{colName}_cleaned\",\n",
    "                regexp_replace(col(colName), \"[^0-9.]\", \"\")\n",
    "            )\n",
    "            df_empty = df_cleaned.filter((col(f\"{colName}_cleaned\")).isNull()) # is null due to schema defined as decimal if we use inferSchema its a string\n",
    "            empty_count = df_empty.count()\n",
    "\n",
    "            if empty_count > 0:\n",
    "                is_valid = True\n",
    "                sample_size = min(100, empty_count)\n",
    "                invalid_rows = df_empty.select(colName).take(sample_size)\n",
    "                error_msg = (f\"Invalid {colName} values (containing only non-numeric characters) in {FullPathSchema}. Total count: {empty_count}\")\n",
    "                errors.append(error_msg)\n",
    "            df_contents = df_contents.drop(f\"{colName}_cleaned\") \n",
    "    msg = \"\\n\".join(errors) if errors else \"Data Quality Checks Passed.\"\n",
    "    return is_valid, msg, df_contents\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"/mnt/apps/Files/Config/master_job.csv\"\n",
    "    pathSchema = \"/mnt/apps/Files/Schema/\"\n",
    "    outputFile = \"/mnt/apps/Files/data-movement/\"\n",
    "\n",
    "    df = pandas_read_csv(path,sep=\"|\")\n",
    "    df = df.query(\"JobName == 'PAT'\")\n",
    "\n",
    "    for row in df.itertuples():  # Collects all data to the driver - NOT recommended for large datasets\n",
    "        filePath = row.SourceDirectory + '/' + row.FileName + '*' + '.' + row.FileType\n",
    "        filePath = filePath.replace(\"/gcs\", \"/Files\")\n",
    "        dataMovement = outputFile + row.JobName + '/'\n",
    "        print(dataMovement)\n",
    "        FullPathSchema = pathSchema + row.FileName + '.' + row.FileType\n",
    "        #spark.stop()\n",
    "        spark = SparkSession. \\\n",
    "                builder. \\\n",
    "                appName(f\"{row.JobName}\").getOrCreate()\n",
    "        df_dtype = construct_sql_schema(path=FullPathSchema, sep=\"|\")\n",
    "        df = spark_read_csv_from_os(spark, filePath, schema=df_dtype, quote='\"', sep=row.Delimiter)\n",
    "        result, dqc_msg, df_final = validateDecimal(dtypes=df_dtype, df_contents=df)\n",
    "        print(df.rdd.getNumPartitions()) # rdd create while read csv\n",
    "        #df_count = df_final.count()\n",
    "        #print(df_count)\n",
    "        df.limit(10)\n",
    "        num_cores = spark.sparkContext.defaultParallelism\n",
    "        #print(f\"Type of num_cores: {type(num_cores)}\") # Add this line!\n",
    "        num_partitions = num_cores * 1\n",
    "        print(f\"Number of partitions: {num_partitions}\")\n",
    "        partitioned_df = df.repartition(num_partitions)\n",
    "        if result:\n",
    "            df.write.format(\"csv\"). \\\n",
    "                    mode(\"overwrite\"). \\\n",
    "                    option(\"header\", \"true\"). \\\n",
    "                    save(dataMovement)\n",
    "            print(dqc_msg)\n",
    "        else:\n",
    "            df.write.csv(\n",
    "            path=dataMovement,\n",
    "            mode=\"overwrite\",  # Overwrite existing files\n",
    "            header=True,       # Include header row (optional)\n",
    "            sep=row.Delimiter\n",
    "            # coalesce = True # DO NOT USE COALESCE for large datasets unless you *really* need one file.\n",
    "            )\n",
    "            print(dqc_msg)\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('Index', IntegerType(), True), StructField('Customer Id', StringType(), True), StructField('First Name', StringType(), True), StructField('Last Name', StringType(), True), StructField('Company', StringType(), True), StructField('City', StringType(), True), StructField('Country', StringType(), True), StructField('Phone 1', StringType(), True), StructField('Phone 2', StringType(), True), StructField('Email', StringType(), True), StructField('Subscription Date', DateType(), True), StructField('Website', StringType(), True)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/17 03:41:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/17 03:41:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/17 03:41:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/17 03:41:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/17 03:41:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/17 03:41:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/17 03:41:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+------+---------+\n",
      "|country                          |cnt   |total_all|\n",
      "+---------------------------------+------+---------+\n",
      "|Chad                             |284690|70000000 |\n",
      "|Paraguay                         |284550|70000000 |\n",
      "|Anguilla                         |284480|70000000 |\n",
      "|Macao                            |288645|70000000 |\n",
      "|Heard Island and McDonald Islands|282870|70000000 |\n",
      "+---------------------------------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "#outputFile = \"/mnt/apps/Files/ETL4/LAS/customers.csv\"\n",
    "outputFile = \"/mnt/apps/Files/data-movement/Renova/part-000*\"\n",
    "FullPathSchema = \"/mnt/apps/Files/Schema/customers.csv\"\n",
    "ParquetPath = \"/mnt/apps/Files/data-movement/Parquet/Renova/part-000*\"\n",
    "\n",
    "df_dtype = construct_sql_schema(path=FullPathSchema, sep=\"|\")\n",
    "print(df_dtype)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TestReadCSV\").getOrCreate()\n",
    "\n",
    "#CSV reader\n",
    "# df = spark.read.csv(outputFile, sep=\"|\", header=True, schema=df_dtype, inferSchema=False)\n",
    "\n",
    "# df_with_filename = df.withColumn(\"filename\", input_file_name())\n",
    "# null_count = df_with_filename.filter(col(\"Index\").isNotNull())\n",
    "# df_count = null_count.count()\n",
    "# print(df_count)\n",
    "# #null_count.limit(10).show(truncate=False)\n",
    "# null_count.createOrReplaceTempView(\"my_parquet_table\")\n",
    "# #df_parquet_count = null_count_parquet.count()\n",
    "# #print(df_parquet_count)\n",
    "# null_count = spark.sql(\n",
    "#     \"\"\"\n",
    "#     SELECT country, cnt, sum(cnt) over () total_all\n",
    "#     FROM (\n",
    "#     SELECT Country, COUNT(1) AS cnt\n",
    "#         FROM my_parquet_table\n",
    "#         GROUP BY Country\n",
    "#     ) Z\n",
    "# \"\"\")\n",
    "# print(null_count.count())\n",
    "# null_count.show(n=5, truncate=False) #default 20\n",
    "\n",
    "#parquet part\n",
    "df_read_parquet = spark.read.parquet(ParquetPath, schema=df_dtype)\n",
    "null_count_parquet = df_read_parquet.filter(col(\"Index\").isNotNull())\n",
    "null_count_parquet.createOrReplaceTempView(\"my_parquet_table\")\n",
    "#df_parquet_count = null_count_parquet.count()\n",
    "#print(df_parquet_count)\n",
    "null_count_parquet = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT country, cnt, sum(cnt) over () total_all\n",
    "    FROM (\n",
    "    SELECT Country, COUNT(1) AS cnt\n",
    "        FROM my_parquet_table\n",
    "        GROUP BY Country\n",
    "    ) Z\n",
    "\"\"\")\n",
    "print(null_count_parquet.count())\n",
    "null_count_parquet.show(n=5, truncate=False) #default 20\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/mnt/apps/Files/Config/master_job.csv\"\n",
    "pathSchema = \"/mnt/apps/Files/Schema/\"\n",
    "\n",
    "df = pandas_read_csv(path,sep=\"|\")\n",
    "df = df.query(\"JobName == 'PAT'\")\n",
    "\n",
    "for row in df.itertuples():  # Collects all data to the driver - NOT recommended for large datasets\n",
    "    filePath = row.SourceDirectory + '/' + row.FileName + '.' + row.FileType\n",
    "    FullPathSchema = pathSchema + row.FileName + '.' + row.FileType\n",
    "    spark = SparkSession.builder.appName(f\"{row.JobName}\").getOrCreate()\n",
    "    df_dtype = construct_sql_schema(path=FullPathSchema, sep=\"|\")\n",
    "    df = spark_read_csv_from_os(spark, filePath, schema=df_dtype, quote='\"', sep=\"|\")\n",
    "    # Handle the error, e.g., skip the file, log the error, etc.\n",
    "    df.show()\n",
    "\"\"\"     result, dqc_msg, df_final = validateDecimal(dtypes=df_dtype, df_contents=df)\n",
    "    df_final.show()\n",
    "    if result:\n",
    "        print(dqc_msg)\n",
    "    else:\n",
    "        print(dqc_msg) \"\"\"\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/mnt/apps/Files/Config/master_job.csv\"\n",
    "\n",
    "df = pandas_read_csv(path,sep=\"|\")\n",
    "df = df.query(\"Flag == 1\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pandas_read_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m      4\u001b[0m fields \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m type_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvarchar\u001b[39m\u001b[38;5;124m\"\u001b[39m: VarcharType,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnvarchar\u001b[39m\u001b[38;5;124m\"\u001b[39m: VarcharType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecimal\u001b[39m\u001b[38;5;124m\"\u001b[39m: DecimalType\n\u001b[1;32m     12\u001b[0m }\n\u001b[0;32m---> 14\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpandas_read_csv\u001b[49m(path,sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mitertuples():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pandas_read_csv' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "path = \"/mnt/apps/Files/Schema/etl4pat.csv\"\n",
    "fields = []\n",
    "type_mapping = {\n",
    "    \"varchar\": VarcharType,\n",
    "    \"nvarchar\": VarcharType,\n",
    "    \"int\": IntegerType(),\n",
    "    \"bigint\": LongType(),\n",
    "    \"date\": DateType(),\n",
    "    \"decimal\": DecimalType\n",
    "}\n",
    "\n",
    "df = pandas_read_csv(path,sep=\"|\")\n",
    "print(df)\n",
    "\n",
    "for row in df.itertuples():\n",
    "    name, data_type_str = row.DataType.split(\"(\", 1) if \"(\" in row.DataType else (row.DataType,\"\")\n",
    "    name = name.strip()\n",
    "    data_type_str = data_type_str[:-1].strip()\n",
    "    parts = data_type_str.split(\",\")\n",
    "    name_lower = name.lower()\n",
    "    print(data_type_str)\n",
    "\n",
    "    for keyword,spark_type in type_mapping.items():\n",
    "        if keyword in name_lower:\n",
    "            if spark_type == VarcharType:\n",
    "                data_type = VarcharType(4000) if data_type_str == \"MAX\" else VarcharType(int(data_type_str))\n",
    "                fields.append(StructField(row.ColumnName, data_type, True))\n",
    "            elif spark_type == DecimalType:\n",
    "                data_type = DecimalType() if not data_type_str else DecimalType(int(parts[0]),int(parts[1]))\n",
    "                fields.append(StructField(row.ColumnName, data_type, True))\n",
    "            else:\n",
    "                data_type = spark_type\n",
    "                fields.append(StructField(row.ColumnName, data_type, True))\n",
    "            break\n",
    "\n",
    "print(fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_type: Decimal(2,0\n",
      "args: ['']\n",
      "data_type: VARCHAR\n",
      "args: []\n"
     ]
    }
   ],
   "source": [
    "data_type_str = \"Decimal(2,0)\"\n",
    "data_type, *args = data_type_str.split(\")\")\n",
    "\n",
    "print(f\"data_type: {data_type}\")  # Output: data_type: Decimal(2,0\n",
    "print(f\"args: {args}\")          # Output: args: ['']\n",
    "\n",
    "data_type_str = \"VARCHAR\"\n",
    "data_type, *args = data_type_str.split(\")\")\n",
    "\n",
    "print(f\"data_type: {data_type}\")  # Output: data_type: VARCHAR\n",
    "print(f\"args: {args}\")          # Output: args: []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "data = \"A,2\"\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: File not found at path: /mnt/apps/Files/Schema/etl4pat.csv\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'itertuples'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/apps/Files/Schema/etl4pat.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_sql_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m|\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(test)\n",
      "Cell \u001b[0;32mIn[1], line 73\u001b[0m, in \u001b[0;36mconstruct_sql_schema\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m type_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvarchar\u001b[39m\u001b[38;5;124m\"\u001b[39m: StringType(),\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnvarchar\u001b[39m\u001b[38;5;124m\"\u001b[39m: StringType(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecimal\u001b[39m\u001b[38;5;124m\"\u001b[39m: DecimalType\n\u001b[1;32m     69\u001b[0m }\n\u001b[1;32m     71\u001b[0m df \u001b[38;5;241m=\u001b[39m pandas_read_csv(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m],sep\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msep\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitertuples\u001b[49m():\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         name, data_type_str \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mDataType\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m row\u001b[38;5;241m.\u001b[39mDataType \u001b[38;5;28;01melse\u001b[39;00m (row\u001b[38;5;241m.\u001b[39mDataType,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'itertuples'"
     ]
    }
   ],
   "source": [
    "path = \"/mnt/apps/Files/Schema/etl4pat.csv\"\n",
    "test = construct_sql_schema(path=path,sep=\"|\")\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *  # Import data types for clarity\n",
    "\n",
    "# Create a SparkSession (if you don't have one already)\n",
    "spark = SparkSession.builder.appName(\"DataTypeExample\").getOrCreate()\n",
    "\n",
    "# Sample data (replace with your actual data)\n",
    "data = [(\"Alice\", 25, 2000.00), (\"Bob\", 30, 2000.00), (\"Charlie\", 22, 2000.00)]\n",
    "\n",
    "# Define the schema explicitly (best practice)\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"height\", DecimalType(2,0), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Display the schema\n",
    "df.printSchema()\n",
    "\n",
    "# Stop the SparkSession (good practice)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"/mnt/apps/gcs/ETL4/CONFIG/etl4pat*.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'header': 'true', 'delimiter': '|', 'quote': '\"', 'mode': 'overwrite', 'format': 'csv'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def writeOptions(df, path, **kwargs):\n",
    "    base_options = {  # Keep this separate\n",
    "        \"header\": \"true\",\n",
    "        \"delimiter\": \"|\",\n",
    "        \"quote\": '\"',\n",
    "        \"mode\":\"overwrite\",\n",
    "        \"format\":\"csv\"\n",
    "    }\n",
    "\n",
    "    # Correct way to merge options:\n",
    "    all_options = base_options.copy()  # Create a copy to avoid modifying base_options\n",
    "    all_options.update(kwargs)       # Add or overwrite kwargs\n",
    "    print(all_options)\n",
    "\n",
    "    df.write.options(**all_options).save(path)\n",
    "\n",
    "# Example usage (important: complete example):\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
    "\n",
    "dataMovement = \"path/to/save.csv\" # Or your actual path\n",
    "\n",
    "writeOptions(df, dataMovement)  # Now works correctly\n",
    "\n",
    "spark.stop()  # Don't forget to stop the SparkSession"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
