+---+---------------+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------------+----+
|   |   BatchName   |  DqcId   |                                                                                                                       Scripts                                                                                                                       | Run |    RefTable     | SP |
+---+---------------+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------------+----+
| 0 | BATCH_ACT_VAL | DQ000003 |                                                                                    SELECT COUNT(1) CNT FROM PEOPLEPF UNION ALL SELECT COUNT(1) CNT FROM PEOPLEPF                                                                                    |  1  | [ACMVPF,GENLPF] | 0  |
| 2 | BATCH_ACT_VAL | DQ000005 | SELECT COUNT(1) CNT FROM PEOPLEPF WHERE YEAR(`Date of birth`) IN ('1992','1993','1995','2021') UNION ALL SELECT COUNT(1) CNT FROM PEOPLEPF WHERE YEAR(`Date of birth`) IN ('1992','1993','1995','2021') UNION ALL SELECT COUNT(1) CNT FROM PEOPLEPF |  1  |       nan       | 0  |
+---+---------------+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------------+----+
+---+---------------+-----------------+
|   |   BatchName   |     JobName     |
+---+---------------+-----------------+
| 0 | BATCH_ACT_VAL |     GENLPF      |
| 1 | BATCH_ACT_VAL | [ACMVPF,GENLPF] |
+---+---------------+-----------------+
25/05/17 07:54:48 INFO SparkContext: Running Spark version 3.5.4
25/05/17 07:54:48 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/05/17 07:54:48 INFO SparkContext: Java version 17.0.14
25/05/17 07:54:48 INFO ResourceUtils: ==============================================================
25/05/17 07:54:48 INFO ResourceUtils: No custom resources configured for spark.driver.
25/05/17 07:54:48 INFO ResourceUtils: ==============================================================
25/05/17 07:54:48 INFO SparkContext: Submitted application: BATCH_ACT_VAL_DQC
25/05/17 07:54:48 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 512, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/05/17 07:54:48 INFO ResourceProfile: Limiting resource is cpu
25/05/17 07:54:48 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/05/17 07:54:48 INFO SecurityManager: Changing view acls to: spark,root
25/05/17 07:54:48 INFO SecurityManager: Changing modify acls to: spark,root
25/05/17 07:54:48 INFO SecurityManager: Changing view acls groups to: 
25/05/17 07:54:48 INFO SecurityManager: Changing modify acls groups to: 
25/05/17 07:54:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark, root; groups with view permissions: EMPTY; users with modify permissions: spark, root; groups with modify permissions: EMPTY
25/05/17 07:54:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/05/17 07:54:48 INFO Utils: Successfully started service 'sparkDriver' on port 34121.
25/05/17 07:54:48 INFO SparkEnv: Registering MapOutputTracker
25/05/17 07:54:48 INFO SparkEnv: Registering BlockManagerMaster
25/05/17 07:54:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/05/17 07:54:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/05/17 07:54:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/05/17 07:54:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-94505d02-e4b9-47eb-abb6-0439fcf38001
25/05/17 07:54:48 INFO MemoryStore: MemoryStore started with capacity 1048.8 MiB
25/05/17 07:54:48 INFO SparkEnv: Registering OutputCommitCoordinator
25/05/17 07:54:48 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/05/17 07:54:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/05/17 07:54:49 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/05/17 07:54:49 INFO TransportClientFactory: Successfully created connection to spark-master/172.20.0.2:7077 after 23 ms (0 ms spent in bootstraps)
25/05/17 07:54:49 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250517075449-0241
25/05/17 07:54:49 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250517075449-0241/0 on worker-20250517024400-172.20.0.8-34111 (172.20.0.8:34111) with 1 core(s)
25/05/17 07:54:49 INFO StandaloneSchedulerBackend: Granted executor ID app-20250517075449-0241/0 on hostPort 172.20.0.8:34111 with 1 core(s), 512.0 MiB RAM
25/05/17 07:54:49 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250517075449-0241/1 on worker-20250517024400-172.20.0.4-42257 (172.20.0.4:42257) with 1 core(s)
25/05/17 07:54:49 INFO StandaloneSchedulerBackend: Granted executor ID app-20250517075449-0241/1 on hostPort 172.20.0.4:42257 with 1 core(s), 512.0 MiB RAM
25/05/17 07:54:49 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250517075449-0241/2 on worker-20250517024400-172.20.0.6-36935 (172.20.0.6:36935) with 1 core(s)
25/05/17 07:54:49 INFO StandaloneSchedulerBackend: Granted executor ID app-20250517075449-0241/2 on hostPort 172.20.0.6:36935 with 1 core(s), 512.0 MiB RAM
25/05/17 07:54:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34375.
25/05/17 07:54:49 INFO NettyBlockTransferService: Server created on ae6614f586c2:34375
25/05/17 07:54:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/05/17 07:54:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ae6614f586c2, 34375, None)
25/05/17 07:54:49 INFO BlockManagerMasterEndpoint: Registering block manager ae6614f586c2:34375 with 1048.8 MiB RAM, BlockManagerId(driver, ae6614f586c2, 34375, None)
25/05/17 07:54:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ae6614f586c2, 34375, None)
25/05/17 07:54:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ae6614f586c2, 34375, None)
25/05/17 07:54:49 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250517075449-0241/2 is now RUNNING
25/05/17 07:54:49 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250517075449-0241/0 is now RUNNING
25/05/17 07:54:49 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250517075449-0241/1 is now RUNNING
25/05/17 07:54:49 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/05/17 07:54:49 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
25/05/17 07:54:49 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
25/05/17 07:54:55 INFO InMemoryFileIndex: It took 93 ms to list leaf files for 4 paths.
25/05/17 07:54:55 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.0.6:34492) with ID 2,  ResourceProfileId 0
25/05/17 07:54:55 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.0.4:52608) with ID 1,  ResourceProfileId 0
25/05/17 07:54:55 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.0.8:44098) with ID 0,  ResourceProfileId 0
25/05/17 07:54:55 INFO BlockManagerMasterEndpoint: Registering block manager 172.20.0.6:42743 with 127.2 MiB RAM, BlockManagerId(2, 172.20.0.6, 42743, None)
25/05/17 07:54:55 INFO BlockManagerMasterEndpoint: Registering block manager 172.20.0.4:37509 with 127.2 MiB RAM, BlockManagerId(1, 172.20.0.4, 37509, None)
25/05/17 07:54:55 INFO BlockManagerMasterEndpoint: Registering block manager 172.20.0.8:46509 with 127.2 MiB RAM, BlockManagerId(0, 172.20.0.8, 46509, None)
25/05/17 07:54:56 INFO SparkContext: Starting job: sql at <unknown>:0
25/05/17 07:54:56 INFO DAGScheduler: Got job 0 (sql at <unknown>:0) with 1 output partitions
25/05/17 07:54:56 INFO DAGScheduler: Final stage: ResultStage 0 (sql at <unknown>:0)
25/05/17 07:54:56 INFO DAGScheduler: Parents of final stage: List()
25/05/17 07:54:56 INFO DAGScheduler: Missing parents: List()
25/05/17 07:54:56 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at sql at <unknown>:0), which has no missing parents
25/05/17 07:54:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 103.7 KiB, free 1048.7 MiB)
25/05/17 07:54:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.4 KiB, free 1048.7 MiB)
25/05/17 07:54:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ae6614f586c2:34375 (size: 37.4 KiB, free: 1048.8 MiB)
25/05/17 07:54:56 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
25/05/17 07:54:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at sql at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/05/17 07:54:56 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/05/17 07:54:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.20.0.6, executor 2, partition 0, PROCESS_LOCAL, 9213 bytes) 
25/05/17 07:54:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.20.0.6:42743 (size: 37.4 KiB, free: 127.2 MiB)
25/05/17 07:54:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 922 ms on 172.20.0.6 (executor 2) (1/1)
25/05/17 07:54:57 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/05/17 07:54:57 INFO DAGScheduler: ResultStage 0 (sql at <unknown>:0) finished in 1.116 s
25/05/17 07:54:57 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/05/17 07:54:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/05/17 07:54:57 INFO DAGScheduler: Job 0 finished: sql at <unknown>:0, took 1.177411 s
25/05/17 07:54:58 INFO CodeGenerator: Code generated in 191.47805 ms
25/05/17 07:54:58 INFO CodeGenerator: Code generated in 13.013394 ms
2025-05-17 07:54:58,739 - INFO - Closing down clientserver connection
25/05/17 07:54:58 INFO SparkContext: Invoking stop() from shutdown hook
25/05/17 07:54:58 INFO SparkContext: SparkContext is stopping with exitCode 0.
25/05/17 07:54:58 INFO SparkUI: Stopped Spark web UI at http://ae6614f586c2:4040
25/05/17 07:54:58 INFO StandaloneSchedulerBackend: Shutting down all executors
25/05/17 07:54:58 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
25/05/17 07:54:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/05/17 07:54:58 INFO MemoryStore: MemoryStore cleared
25/05/17 07:54:58 INFO BlockManager: BlockManager stopped
25/05/17 07:54:58 INFO BlockManagerMaster: BlockManagerMaster stopped
25/05/17 07:54:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/05/17 07:54:58 INFO SparkContext: Successfully stopped SparkContext
25/05/17 07:54:58 INFO ShutdownHookManager: Shutdown hook called
25/05/17 07:54:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-67eefc7d-4b25-4608-a384-b5d62c864d79
25/05/17 07:54:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-0a2f340b-0326-45c8-8a67-76ddcebf41c1/pyspark-6d47bd53-b5b5-4f47-a9b8-9b6d3e8dbe6b
25/05/17 07:54:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-0a2f340b-0326-45c8-8a67-76ddcebf41c1
