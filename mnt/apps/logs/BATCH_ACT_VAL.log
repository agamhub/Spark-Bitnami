2025-04-09 10:40:44,276 - INFO - +---+---------------+---------+---------------------------+----------+----------+------+-----------+
|   |   BatchName   | JobName |      SourceDirectory      | FileName | FileType | Flag | Delimiter |
+---+---------------+---------+---------------------------+----------+----------+------+-----------+
| 1 | BATCH_ACT_VAL | ACMVPF  | /mnt/apps/gcs/ETL4/ACMVPF |  ACMVPF  |   csv    |  1   |     |     |
+---+---------------+---------+---------------------------+----------+----------+------+-----------+
25/04/09 10:40:44 INFO SparkContext: Running Spark version 3.5.4
25/04/09 10:40:44 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/04/09 10:40:44 INFO SparkContext: Java version 17.0.14
25/04/09 10:40:44 INFO ResourceUtils: ==============================================================
25/04/09 10:40:44 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 10:40:44 INFO ResourceUtils: ==============================================================
25/04/09 10:40:44 INFO SparkContext: Submitted application: BATCH_ACT_VAL
25/04/09 10:40:44 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 10:40:44 INFO ResourceProfile: Limiting resource is cpu
25/04/09 10:40:44 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 10:40:44 INFO SecurityManager: Changing view acls to: spark,root
25/04/09 10:40:44 INFO SecurityManager: Changing modify acls to: spark,root
25/04/09 10:40:44 INFO SecurityManager: Changing view acls groups to: 
25/04/09 10:40:44 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 10:40:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark, root; groups with view permissions: EMPTY; users with modify permissions: spark, root; groups with modify permissions: EMPTY
25/04/09 10:40:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 10:40:44 INFO Utils: Successfully started service 'sparkDriver' on port 38861.
25/04/09 10:40:44 INFO SparkEnv: Registering MapOutputTracker
25/04/09 10:40:44 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 10:40:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 10:40:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 10:40:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 10:40:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6a2455a3-fdf6-4de4-b558-24e1cb72674e
25/04/09 10:40:44 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
25/04/09 10:40:44 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 10:40:45 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/04/09 10:40:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 10:40:45 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 10:40:45 INFO TransportClientFactory: Successfully created connection to spark-master/172.20.0.2:7077 after 19 ms (0 ms spent in bootstraps)
25/04/09 10:40:45 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409104045-0000
25/04/09 10:40:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42259.
25/04/09 10:40:45 INFO NettyBlockTransferService: Server created on 2bd8c065d526:42259
25/04/09 10:40:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 10:40:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 2bd8c065d526, 42259, None)
25/04/09 10:40:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409104045-0000/0 on worker-20250409103647-172.20.0.8-41381 (172.20.0.8:41381) with 2 core(s)
25/04/09 10:40:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409104045-0000/0 on hostPort 172.20.0.8:41381 with 2 core(s), 1024.0 MiB RAM
25/04/09 10:40:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409104045-0000/1 on worker-20250409103647-172.20.0.6-39527 (172.20.0.6:39527) with 1 core(s)
25/04/09 10:40:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409104045-0000/1 on hostPort 172.20.0.6:39527 with 1 core(s), 1024.0 MiB RAM
25/04/09 10:40:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409104045-0000/2 on worker-20250409103647-172.20.0.4-34093 (172.20.0.4:34093) with 1 core(s)
25/04/09 10:40:45 INFO BlockManagerMasterEndpoint: Registering block manager 2bd8c065d526:42259 with 434.4 MiB RAM, BlockManagerId(driver, 2bd8c065d526, 42259, None)
25/04/09 10:40:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409104045-0000/2 on hostPort 172.20.0.4:34093 with 1 core(s), 1024.0 MiB RAM
25/04/09 10:40:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 2bd8c065d526, 42259, None)
25/04/09 10:40:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 2bd8c065d526, 42259, None)
25/04/09 10:40:45 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409104045-0000/0 is now RUNNING
25/04/09 10:40:45 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409104045-0000/1 is now RUNNING
25/04/09 10:40:45 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409104045-0000/2 is now RUNNING
25/04/09 10:40:45 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
running load
25/04/09 10:40:46 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
25/04/09 10:40:46 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
25/04/09 10:40:48 INFO InMemoryFileIndex: It took 220 ms to list leaf files for 9 paths.
25/04/09 10:40:49 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.0.8:51682) with ID 0,  ResourceProfileId 0
25/04/09 10:40:49 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.0.6:34298) with ID 1,  ResourceProfileId 0
25/04/09 10:40:49 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.0.4:36100) with ID 2,  ResourceProfileId 0
25/04/09 10:40:49 INFO BlockManagerMasterEndpoint: Registering block manager 172.20.0.8:40329 with 434.4 MiB RAM, BlockManagerId(0, 172.20.0.8, 40329, None)
25/04/09 10:40:49 INFO BlockManagerMasterEndpoint: Registering block manager 172.20.0.6:34487 with 434.4 MiB RAM, BlockManagerId(1, 172.20.0.6, 34487, None)
25/04/09 10:40:49 INFO BlockManagerMasterEndpoint: Registering block manager 172.20.0.4:39777 with 434.4 MiB RAM, BlockManagerId(2, 172.20.0.4, 39777, None)
25/04/09 10:40:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(rejected_records)
25/04/09 10:40:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(rejected_records#12)
25/04/09 10:40:50 INFO CodeGenerator: Code generated in 196.185345 ms
25/04/09 10:40:50 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.2 KiB, free 434.2 MiB)
25/04/09 10:40:50 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 434.2 MiB)
25/04/09 10:40:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 2bd8c065d526:42259 (size: 34.7 KiB, free: 434.4 MiB)
25/04/09 10:40:50 INFO SparkContext: Created broadcast 0 from csv at <unknown>:0
25/04/09 10:40:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 10:40:51 INFO DAGScheduler: Registering RDD 3 (csv at <unknown>:0) as input to shuffle 0
25/04/09 10:40:51 INFO DAGScheduler: Got map stage job 0 (csv at <unknown>:0) with 27 output partitions
25/04/09 10:40:51 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (csv at <unknown>:0)
25/04/09 10:40:51 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:40:51 INFO DAGScheduler: Missing parents: List()
25/04/09 10:40:51 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at csv at <unknown>:0), which has no missing parents
25/04/09 10:40:51 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 23.6 KiB, free 434.1 MiB)
25/04/09 10:40:51 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.1 KiB, free 434.1 MiB)
25/04/09 10:40:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 2bd8c065d526:42259 (size: 10.1 KiB, free: 434.4 MiB)
25/04/09 10:40:51 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
25/04/09 10:40:51 INFO DAGScheduler: Submitting 27 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at csv at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
25/04/09 10:40:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 27 tasks resource profile 0
25/04/09 10:40:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.20.0.4, executor 2, partition 0, PROCESS_LOCAL, 9637 bytes) 
25/04/09 10:40:51 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.20.0.6, executor 1, partition 1, PROCESS_LOCAL, 9637 bytes) 
25/04/09 10:40:51 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (172.20.0.8, executor 0, partition 2, PROCESS_LOCAL, 9626 bytes) 
25/04/09 10:40:51 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (172.20.0.8, executor 0, partition 3, PROCESS_LOCAL, 9626 bytes) 
25/04/09 10:40:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.20.0.4:39777 (size: 10.1 KiB, free: 434.4 MiB)
25/04/09 10:40:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.20.0.8:40329 (size: 10.1 KiB, free: 434.4 MiB)
25/04/09 10:40:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.20.0.6:34487 (size: 10.1 KiB, free: 434.4 MiB)
25/04/09 10:40:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.20.0.8:40329 (size: 34.7 KiB, free: 434.4 MiB)
25/04/09 10:40:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.20.0.4:39777 (size: 34.7 KiB, free: 434.4 MiB)
25/04/09 10:40:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.20.0.6:34487 (size: 34.7 KiB, free: 434.4 MiB)
25/04/09 10:41:02 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (172.20.0.4, executor 2, partition 4, PROCESS_LOCAL, 9615 bytes) 
25/04/09 10:41:02 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (172.20.0.8, executor 0, partition 5, PROCESS_LOCAL, 9615 bytes) 
25/04/09 10:41:03 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 11875 ms on 172.20.0.4 (executor 2) (1/27)
25/04/09 10:41:03 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 11866 ms on 172.20.0.8 (executor 0) (2/27)
25/04/09 10:41:03 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (172.20.0.8, executor 0, partition 6, PROCESS_LOCAL, 9626 bytes) 
25/04/09 10:41:03 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 11963 ms on 172.20.0.8 (executor 0) (3/27)
25/04/09 10:41:03 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (172.20.0.6, executor 1, partition 7, PROCESS_LOCAL, 9626 bytes) 
25/04/09 10:41:03 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 12820 ms on 172.20.0.6 (executor 1) (4/27)
25/04/09 10:41:11 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (172.20.0.4, executor 2, partition 8, PROCESS_LOCAL, 9615 bytes) 
25/04/09 10:41:11 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 8696 ms on 172.20.0.4 (executor 2) (5/27)
25/04/09 10:41:11 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (172.20.0.8, executor 0, partition 9, PROCESS_LOCAL, 9615 bytes) 
25/04/09 10:41:11 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 8678 ms on 172.20.0.8 (executor 0) (6/27)
25/04/09 10:41:11 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (172.20.0.8, executor 0, partition 10, PROCESS_LOCAL, 9626 bytes) 
25/04/09 10:41:11 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 8712 ms on 172.20.0.8 (executor 0) (7/27)
25/04/09 10:41:12 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED
25/04/09 10:41:12 INFO SparkContext: SparkContext is stopping with exitCode 0.
25/04/09 10:41:12 INFO TaskSchedulerImpl: Cancelling stage 0
25/04/09 10:41:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled: Job aborted due to stage failure: Master removed our application: KILLED
25/04/09 10:41:12 INFO TaskSchedulerImpl: Stage 0 was cancelled
25/04/09 10:41:12 INFO DAGScheduler: ShuffleMapStage 0 (csv at <unknown>:0) failed in 20.986 s due to Job aborted due to stage failure: Master removed our application: KILLED
25/04/09 10:41:12 INFO SparkUI: Stopped Spark web UI at http://2bd8c065d526:4040
25/04/09 10:41:12 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 10:41:12 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
25/04/09 10:41:12 ERROR Inbox: Ignoring error
java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@35fb72e0[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@81aeeb4[Wrapped task = org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint$$anon$2@14359827]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cda5a5[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]
	at java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor.reject(Unknown Source)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(Unknown Source)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor.schedule(Unknown Source)
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint.$anonfun$onDisconnected$1(StandaloneSchedulerBackend.scala:346)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint.onDisconnected(StandaloneSchedulerBackend.scala:313)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:141)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)
25/04/09 10:41:12 ERROR Inbox: Ignoring error
java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@31b169a1[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@3e538a5e[Wrapped task = org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint$$anon$2@283d6ce7]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cda5a5[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]
	at java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor.reject(Unknown Source)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(Unknown Source)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor.schedule(Unknown Source)
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint.$anonfun$onDisconnected$1(StandaloneSchedulerBackend.scala:346)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint.onDisconnected(StandaloneSchedulerBackend.scala:313)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:141)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)
25/04/09 10:41:12 ERROR Utils: Uncaught exception in thread stop-spark-context
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.deploy.client.StandaloneAppClient.stop(StandaloneAppClient.scala:288)
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.org$apache$spark$scheduler$cluster$StandaloneSchedulerBackend$$stop(StandaloneSchedulerBackend.scala:275)
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.stop(StandaloneSchedulerBackend.scala:142)
	at org.apache.spark.scheduler.SchedulerBackend.stop(SchedulerBackend.scala:33)
	at org.apache.spark.scheduler.SchedulerBackend.stop$(SchedulerBackend.scala:33)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.stop(CoarseGrainedSchedulerBackend.scala:54)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$stop$2(TaskSchedulerImpl.scala:992)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)
	at org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:992)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$4(DAGScheduler.scala:2976)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2976)
	at org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2258)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2258)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2211)
	at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:2198)
Caused by: org.apache.spark.SparkException: Could not find AppClient.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:178)
	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)
	at org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)
	at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:72)
	... 17 more
25/04/09 10:41:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 10:41:12 INFO MemoryStore: MemoryStore cleared
25/04/09 10:41:12 INFO BlockManager: BlockManager stopped
25/04/09 10:41:12 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 10:41:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 10:41:12 INFO SparkContext: Successfully stopped SparkContext
An error occurred while reading the CSV: An error occurred while calling o62.csv.
: org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: KILLED
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

2025-04-09 10:41:12,195 - WARNING - Spark Session got killed or others issue encountered !!! 'NoneType' object has no attribute 'withColumn'
running completed
2025-04-09 10:41:12,196 - INFO - Closing down clientserver connection
2025-04-09 10:41:12,196 - INFO - 
                List Of Parameters
                -----------------------------------------------
                SparkName Mandatory = BATCH_ACT_VAL
                List Of DQC result = []
            
25/04/09 10:41:12 INFO ShutdownHookManager: Shutdown hook called
25/04/09 10:41:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-240beb9b-1bc7-4a4b-b4e2-eff4fd18c305
25/04/09 10:41:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-5c411a6f-bd06-4984-8572-835a6c9b34de
25/04/09 10:41:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-240beb9b-1bc7-4a4b-b4e2-eff4fd18c305/pyspark-92b59064-f614-47c6-a799-42eafd30407b
