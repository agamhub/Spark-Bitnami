25/02/11 20:10:36 INFO SparkContext: Running Spark version 3.5.4
25/02/11 20:10:36 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/02/11 20:10:36 INFO SparkContext: Java version 17.0.14
25/02/11 20:10:36 INFO ResourceUtils: ==============================================================
25/02/11 20:10:36 INFO ResourceUtils: No custom resources configured for spark.driver.
25/02/11 20:10:36 INFO ResourceUtils: ==============================================================
25/02/11 20:10:36 INFO SparkContext: Submitted application: Renova
25/02/11 20:10:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1524, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/02/11 20:10:36 INFO ResourceProfile: Limiting resource is cpu
25/02/11 20:10:36 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/02/11 20:10:36 INFO SecurityManager: Changing view acls to: spark
25/02/11 20:10:36 INFO SecurityManager: Changing modify acls to: spark
25/02/11 20:10:36 INFO SecurityManager: Changing view acls groups to: 
25/02/11 20:10:36 INFO SecurityManager: Changing modify acls groups to: 
25/02/11 20:10:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
25/02/11 20:10:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/02/11 20:10:37 INFO Utils: Successfully started service 'sparkDriver' on port 34273.
25/02/11 20:10:37 INFO SparkEnv: Registering MapOutputTracker
25/02/11 20:10:37 INFO SparkEnv: Registering BlockManagerMaster
25/02/11 20:10:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/02/11 20:10:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/02/11 20:10:37 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/02/11 20:10:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-afb63bfe-fd6a-46bd-9de0-b68e643342fe
25/02/11 20:10:37 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
25/02/11 20:10:37 INFO SparkEnv: Registering OutputCommitCoordinator
25/02/11 20:10:37 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/02/11 20:10:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/02/11 20:10:37 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/02/11 20:10:37 INFO TransportClientFactory: Successfully created connection to spark-master/172.19.0.2:7077 after 22 ms (0 ms spent in bootstraps)
25/02/11 20:10:37 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250211201037-0000
25/02/11 20:10:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36657.
25/02/11 20:10:37 INFO NettyBlockTransferService: Server created on 1916b67fa529:36657
25/02/11 20:10:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/02/11 20:10:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1916b67fa529, 36657, None)
25/02/11 20:10:37 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250211201037-0000/0 on worker-20250211200935-172.19.0.7-44285 (172.19.0.7:44285) with 2 core(s)
25/02/11 20:10:37 INFO StandaloneSchedulerBackend: Granted executor ID app-20250211201037-0000/0 on hostPort 172.19.0.7:44285 with 2 core(s), 1524.0 MiB RAM
25/02/11 20:10:37 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250211201037-0000/1 on worker-20250211200934-172.19.0.5-45445 (172.19.0.5:45445) with 1 core(s)
25/02/11 20:10:37 INFO StandaloneSchedulerBackend: Granted executor ID app-20250211201037-0000/1 on hostPort 172.19.0.5:45445 with 1 core(s), 1524.0 MiB RAM
25/02/11 20:10:37 INFO BlockManagerMasterEndpoint: Registering block manager 1916b67fa529:36657 with 434.4 MiB RAM, BlockManagerId(driver, 1916b67fa529, 36657, None)
25/02/11 20:10:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 1916b67fa529, 36657, None)
25/02/11 20:10:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 1916b67fa529, 36657, None)
25/02/11 20:10:37 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250211201037-0000/1 is now RUNNING
25/02/11 20:10:37 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250211201037-0000/0 is now RUNNING
25/02/11 20:10:37 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/02/11 20:10:38 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
25/02/11 20:10:38 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
25/02/11 20:10:39 INFO InMemoryFileIndex: It took 224 ms to list leaf files for 11 paths.
25/02/11 20:10:40 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.19.0.7:51288) with ID 0,  ResourceProfileId 0
25/02/11 20:10:40 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.19.0.5:42202) with ID 1,  ResourceProfileId 0
25/02/11 20:10:40 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.0.7:41457 with 734.4 MiB RAM, BlockManagerId(0, 172.19.0.7, 41457, None)
25/02/11 20:10:40 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.0.5:35935 with 734.4 MiB RAM, BlockManagerId(1, 172.19.0.5, 35935, None)
25/02/11 20:10:40 INFO FileSourceStrategy: Pushed Filters: 
25/02/11 20:10:40 INFO FileSourceStrategy: Post-Scan Filters: 
25/02/11 20:10:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/02/11 20:10:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/02/11 20:10:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
Traceback (most recent call last):
  File "/mnt/apps/jobs/Main.py", line 181, in <module>
    df.write.csv(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1864, in csv
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o47.csv.
: ExitCodeException exitCode=1: chmod: changing permissions of '/mnt/apps/gcs/data-movement/Renova': Operation not permitted

	at org.apache.hadoop.util.Shell.runCommand(Shell.java:1007)
	at org.apache.hadoop.util.Shell.run(Shell.java:900)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Unknown Source)

2025-02-11 20:10:40,812 - INFO - Closing down clientserver connection
25/02/11 20:10:40 INFO SparkContext: Invoking stop() from shutdown hook
25/02/11 20:10:40 INFO SparkContext: SparkContext is stopping with exitCode 0.
25/02/11 20:10:40 INFO SparkUI: Stopped Spark web UI at http://1916b67fa529:4040
25/02/11 20:10:40 INFO StandaloneSchedulerBackend: Shutting down all executors
25/02/11 20:10:40 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
25/02/11 20:10:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/02/11 20:10:40 INFO MemoryStore: MemoryStore cleared
25/02/11 20:10:40 INFO BlockManager: BlockManager stopped
25/02/11 20:10:40 INFO BlockManagerMaster: BlockManagerMaster stopped
25/02/11 20:10:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/02/11 20:10:40 INFO SparkContext: Successfully stopped SparkContext
25/02/11 20:10:40 INFO ShutdownHookManager: Shutdown hook called
25/02/11 20:10:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-fc04206d-591e-443d-9c2a-58a9417acc54
25/02/11 20:10:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-2b6a3ac8-02e3-4947-9d40-29a692967f96/pyspark-c77b5f47-35b4-4ffe-8ced-51f7b586ecfd
25/02/11 20:10:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-2b6a3ac8-02e3-4947-9d40-29a692967f96
Tue Feb 11 20:10:41 UTC 2025 - BASH - Spark job 'shtest1' completed
