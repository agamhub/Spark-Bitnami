x-spark-common: &spark-common
    #image: bitnami/spark:latest # build an image that referring to dockerfile instead
    build:
      context: ./spark
      dockerfile: Dockerfile.spark
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=3
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - PY4J_VERSION=0.10.9.7  # Add py4j version
      - SPARK_USER=root
    depends_on:
      - spark-master
    volumes:
      - ./mnt/apps:/mnt/apps
      - /mnt/e/database/data-movement:/mnt/apps/gcs/
      - /mnt/e/database:/mnt/apps/gcs
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - spark-network
    extra_hosts:
      - "host.docker.internal:host-gateway"
      
x-airflow-common: &airflow-common
  build:
      context: ./airflow
      dockerfile: Dockerfile.airflow
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    - AIRFLOW__WEBSERVER__SECRET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
    - AIRFLOW__CORE__LOAD_EXAMPLES=False
    - AIRFLOW__CORE__LOGGING_LEVEL=INFO
    - AIRFLOW__WEBSERVER_BASE_URL=http://localhost:8083
    - AIRFLOW__WEBSERVER__SHOW_TRIGGER_FORM_IF_NO_PARAMS=True #Trigger with parameter
    - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
  volumes:
    - ./mnt/apps/dags:/opt/airflow/dags
    - ./mnt/apps/airflow-data/logs:/opt/airflow/logs
    - ./mnt/apps/airflow-data/plugins:/opt/airflow/plugins
    - ./mnt/apps/jobs:/opt/airflow/jobs
    - /var/run/docker.sock:/var/run/docker.sock
  depends_on:
    - postgres
  networks:
    - db-network
    - spark-network
  extra_hosts:
    - "host.docker.internal:host-gateway"

services:
  spark-master:
    #image: bitnami/spark:latest # build an image that referring to dockerfile instead
    build:
      context: ./spark
      dockerfile: Dockerfile.spark
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - PY4J_VERSION=0.10.9.7  # Add py4j version
      - SPARK_USER=root
    ports:
      - "8080:8080"  # Spark Master Web UI
      - "7077:7077"  # Spark Master port
      - "4040-4045:4040-4045"  # Spark Application UI monitoring purpose
      - "18080:18080" # Spark Log UI #
      - "4040:4040"
    volumes:
      - ./mnt/apps:/mnt/apps
      - /mnt/e/database/data-movement:/mnt/apps/gcs/
      - /mnt/e/database:/mnt/apps/gcs
      - ./mnt/apps/logs:/opt/bitnami/spark/logs
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - spark-network
    extra_hosts:
      - "host.docker.internal:host-gateway"

  jupyterlab:
    build:
      context: ./spark
      dockerfile: Dockerfile.spark
    command: python -m jupyterlab --ip "0.0.0.0" --no-browser --NotebookApp.token=''
    container_name: jupyterlab
    environment:
      - JUPYTER_ENABLE_LAB=yes  # Enable JupyterLab
      - PYSPARK_PYTHON=/opt/bitnami/python/bin/python  # Use the same Python as Spark
      - IPYTHONDIR=/mnt/apps/.ipython
    ports:
      - "8888:8888"  # JupyterLab Web UI
      - "4222:4222"
    volumes:
      - ./mnt/apps/notebook:/mnt/apps/notebook  # Mount workspaces directory
      - /mnt/e/database:/mnt/apps/Files  # Temporary exclude from GIT only
    working_dir: /mnt/apps
    networks:
      - spark-network
    depends_on:
      - spark-master

  spark-worker:
    <<: *spark-common
    deploy:
      replicas: 3

  postgres:
    image: postgres:13-bullseye # Or your preferred Postgres version
    container_name: postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - "5432:5432"
    networks :
      - db-network

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8083:8083"
    depends_on:
      - airflow-scheduler

  airflow-scheduler:
    <<: *airflow-common
    command: bash -c "airflow db migrate && airflow users create --username admin --firstname Agam --lastname Adhinegara --role Admin --email agamadhinegara@yahoo.com --password admin && airflow scheduler"

networks:
  spark-network:
    driver: bridge
  db-network:
    driver: bridge