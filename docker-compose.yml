version: '3.8'

x-spark-common: &spark-common
    #image: bitnami/spark:latest # build an image that referring to dockerfile instead
    build:
      context: ./spark
      dockerfile: Dockerfile.spark
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=5G
      - SPARK_WORKER_CORES=5
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    depends_on:
      - spark-master
    volumes:
      - ./mnt/apps:/mnt/apps
    networks:
      - spark-network

x-airflow-common: &airflow-common
  build:
      context: ./airflow
      dockerfile: Dockerfile.airflow
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    - AIRFLOW__WEBSERVER__SECRET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
    - AIRFLOW__CORE__LOAD_EXAMPLES=False
    - AIRFLOW__CORE__LOGGING_LEVEL=INFO
    - AIRFLOW__WEBSERVER_BASE_URL=http://localhost:8080
    - AIRFLOW__WEBSERVER__SHOW_TRIGGER_FORM_IF_NO_PARAMS=True #Trigger with parameter
    - DOCKER_HOST=tcp://host.docker.internal:2375
    - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
  volumes:
    - ./mnt/apps/dags:/opt/airflow/dags
    - ./mnt/apps/airflow-data/logs:/opt/airflow/logs
    - ./mnt/apps/airflow-data/plugins:/opt/airflow/plugins
    - ./mnt/apps/jobs:/opt/airflow/jobs
  depends_on:
    - postgres
  networks:
    - db-network
    - spark-network

services:
  spark-master:
    #image: bitnami/spark:latest # build an image that referring to dockerfile instead
    build:
      context: ./spark
      dockerfile: Dockerfile.spark
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
      - SPARK_MASTER_MEMORY:12G
      - SPARK_MASTER_CORES:12
    ports:
      - "9090:8080"  # Spark Master Web UI
      - "7077:7077"  # Spark Master port
      - "4040:4040"  # Spark Application UI monitoring purpose
    volumes:
      - ./mnt/apps:/mnt/apps
      - ./mnt/apps/logs:/opt/bitnami/spark/logs
    networks:
      - spark-network

  spark-worker:
    <<: *spark-common
    deploy:
      replicas: 2
    ports:
      - "8081"

  jupyterlab:
    build:
      context: ./spark
      dockerfile: Dockerfile.spark
    command: python -m jupyterlab --ip "0.0.0.0" --no-browser --NotebookApp.token=''
    container_name: jupyterlab
    environment:
      - JUPYTER_ENABLE_LAB=yes  # Enable JupyterLab
      - PYSPARK_PYTHON=/opt/bitnami/python/bin/python  # Use the same Python as Spark
      - IPYTHONDIR=/mnt/apps/.ipython
    ports:
      - "8888:8888"  # JupyterLab Web UI
    volumes:
      - ./mnt/apps/notebook:/mnt/apps/notebook  # Mount workspaces directory
      - /mnt/d/database:/mnt/apps/Files  # Temporary exclude from GIT only
    working_dir: /mnt/apps
    networks:
      - spark-network
    depends_on:
      - spark-master
  
  postgres:
    image: postgres:13 # Or your preferred Postgres version
    container_name: postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - "5432:5432"
    networks :
      - db-network

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    depends_on:
      - airflow-scheduler

  airflow-scheduler:
    <<: *airflow-common
    command: bash -c "airflow db migrate && airflow users create --username admin --firstname Agam --lastname Adhinegara --role Admin --email agamadhinegara@yahoo.com --password admin && airflow scheduler"

networks:
  spark-network:
    driver: bridge
  db-network:
    driver: bridge